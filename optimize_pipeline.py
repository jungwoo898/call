#!/usr/bin/env python3
"""
Callytics íŒŒì´í”„ë¼ì¸ ì „ë©´ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
ì„±ëŠ¥, ì •í™•ë„, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
"""

import os
import sys
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
import json

class PipelineOptimizer:
    """íŒŒì´í”„ë¼ì¸ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.optimizations = []
        
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/optimization.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def optimize_audio_processing(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìµœì í™”"""
        self.logger.info("ğŸ”§ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìµœì í™” ì‹œì‘")
        
        # 1. IntegratedAudioProcessor ìµœì í™”
        self._optimize_integrated_audio_processor()
        
        # 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬
        self._optimize_memory_usage()
        
        # 3. GPU ë©”ëª¨ë¦¬ ìµœì í™”
        self._optimize_gpu_memory()
        
        self.logger.info("âœ… ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìµœì í™” ì™„ë£Œ")
    
    def _optimize_integrated_audio_processor(self):
        """IntegratedAudioProcessor ìµœì í™”"""
        processor_file = "src/audio/processing.py"
        
        # ëª¨ë¸ ìºì‹± ë° ì¬ì‚¬ìš© ìµœì í™”
        optimizations = [
            # ëª¨ë¸ ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©
            {
                'target': 'class IntegratedAudioProcessor:',
                'addition': '''
    # ëª¨ë¸ ìºì‹±ì„ ìœ„í•œ í´ë˜ìŠ¤ ë³€ìˆ˜
    _whisper_model_cache = {}
    _diarization_model_cache = None
    _punctuation_model_cache = None
    
    @classmethod
    def _get_cached_whisper_model(cls, model_name: str, device: str, compute_type: str):
        """Whisper ëª¨ë¸ ìºì‹±"""
        cache_key = f"{model_name}_{device}_{compute_type}"
        if cache_key not in cls._whisper_model_cache:
            cls._whisper_model_cache[cache_key] = faster_whisper.WhisperModel(
                model_name, device=device, compute_type=compute_type
            )
        return cls._whisper_model_cache[cache_key]
    
    @classmethod
    def _get_cached_diarization_model(cls, auth_token: str):
        """í™”ì ë¶„ë¦¬ ëª¨ë¸ ìºì‹±"""
        if cls._diarization_model_cache is None:
            try:
                from pyannote.audio import Pipeline
                cls._diarization_model_cache = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=auth_token
                )
            except Exception as e:
                print(f"âš ï¸ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                cls._diarization_model_cache = None
        return cls._diarization_model_cache
''',
                'description': 'ëª¨ë¸ ìºì‹± ì‹œìŠ¤í…œ ì¶”ê°€'
            },
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬
            {
                'target': 'def process_audio(self, audio_path: str) -> List[Dict[str, Any]]:',
                'replacement': '''def process_audio(self, audio_path: str) -> List[Dict[str, Any]]:
        """
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬
        """
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            processed_audio_path = self._preprocess_audio_efficient(audio_path)
            
            # í™”ì ë¶„ë¦¬ ì‹œë„
            if self.diarization_pipeline is not None:
                try:
                    return self._process_with_diarization_efficient(processed_audio_path)
                except Exception as e:
                    print(f"âš ï¸ í™”ì ë¶„ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì „í™˜: {e}")
                    return self._process_without_diarization_efficient(processed_audio_path)
            else:
                return self._process_without_diarization_efficient(processed_audio_path)
                
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()''',
                'description': 'ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¡œ ê°œì„ '
            }
        ]
        
        self._apply_optimizations(processor_file, optimizations)
    
    def _optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        # ì„ì‹œ íŒŒì¼ ê´€ë¦¬ ê°œì„ 
        temp_cleanup_script = '''
import atexit
import tempfile
import shutil

class TempFileManager:
    """ì„ì‹œ íŒŒì¼ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.temp_files = []
        atexit.register(self.cleanup)
    
    def create_temp_file(self, suffix='.wav'):
        """ì„ì‹œ íŒŒì¼ ìƒì„±"""
        temp_file = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)
        self.temp_files.append(temp_file.name)
        return temp_file.name
    
    def cleanup(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        for temp_file in self.temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ì„ì‹œ íŒŒì¼ ê´€ë¦¬ì
temp_manager = TempFileManager()
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/audio/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(temp_cleanup_script)
    
    def _optimize_gpu_memory(self):
        """GPU ë©”ëª¨ë¦¬ ìµœì í™”"""
        gpu_optimization = '''
# GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
import torch

def optimize_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ìµœì í™”"""
    if torch.cuda.is_available():
        # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        # ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        
        print(f"âœ… GPU ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {torch.cuda.get_device_name()}")

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/audio/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(gpu_optimization)
    
    def optimize_text_analysis(self):
        """í…ìŠ¤íŠ¸ ë¶„ì„ ìµœì í™”"""
        self.logger.info("ğŸ”§ í…ìŠ¤íŠ¸ ë¶„ì„ ìµœì í™” ì‹œì‘")
        
        # 1. ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ê¸° ìµœì í™”
        self._optimize_communication_quality_analyzer()
        
        # 2. ê°ì • ë¶„ì„ ì •í™•ë„ ê°œì„ 
        self._improve_sentiment_analysis()
        
        # 3. í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  ê°œì„ 
        self._improve_korean_sentence_splitting()
        
        self.logger.info("âœ… í…ìŠ¤íŠ¸ ë¶„ì„ ìµœì í™” ì™„ë£Œ")
    
    def _optimize_communication_quality_analyzer(self):
        """ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ê¸° ìµœì í™”"""
        analyzer_file = "src/text/communication_quality_analyzer.py"
        
        optimizations = [
            # íŒ¨í„´ ì»´íŒŒì¼ ìµœì í™”
            {
                'target': 'def _define_patterns(self):',
                'replacement': '''def _define_patterns(self):
        """ë¶„ì„ìš© íŒ¨í„´ ì •ì˜ (ì»´íŒŒì¼ëœ ì •ê·œì‹ìœ¼ë¡œ ìµœì í™”)"""
        
        # íŒ¨í„´ ì»´íŒŒì¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        import re
        
        # 1. ì¡´ëŒ“ë§ íŒ¨í„´ (Honorific Patterns)
        honorific_patterns = [
            r'ìŠµë‹ˆë‹¤$', r'ã…‚ë‹ˆë‹¤$', r'ã…‚ë‹ˆê¹Œ\?$', r'ì‹œì£ $', r'í•˜ì‹­ì‹œì˜¤$', r'í•´ì£¼ì‹­ì‹œì˜¤$',
            r'í•´ìš”$', r'ì„¸ìš”$', r'ì…”ìš”$', r'ë„¤ìš”$', r'ê±¸ìš”$', r'ì§€ìš”\?$', r'ê¹Œìš”\?$',
            r'ë“œë¦½ë‹ˆë‹¤$', r'ë“œë ¤ìš”$', r'í•´ë“œë¦´ê²Œìš”$', r'ë„ì™€ë“œë¦´ê¹Œìš”\?$',
            r'(ìœ¼)ì‹œê² ìŠµë‹ˆë‹¤$', r'(ìœ¼)ì…¨ìŠµë‹ˆë‹¤$', r'(ìœ¼)ì‹­ë‹ˆë‹¤$', r'(ìœ¼)ì‹œì£ $', r'(ìœ¼)ì‹œë„¤ìš”$', 
            r'(ìœ¼)ì‹œëŠ”êµ°ìš”$', r'ì´ì‹­ë‹ˆë‹¤$', r'í•˜ì‹œë©´$', r'í•˜ì‹œê³ $', r'ì´ì‹œê³ $'
        ]
        self.honorific_patterns = [re.compile(pattern) for pattern in honorific_patterns]
        
        # 2. ê¸ì • ë‹¨ì–´ íŒ¨í„´
        positive_patterns = [
            r'ì¢‹ë‹¤', r'ê°ì‚¬í•˜ë‹¤', r'ê¸°ì˜ë‹¤', r'ë‹¤í–‰ì´ë‹¤', r'ë§Œì¡±í•˜ë‹¤', 
            r'ì•ˆì‹¬ì´ë‹¤', r'ì¹œì ˆí•˜ë‹¤', r'í›Œë¥­í•˜ë‹¤', r'í–‰ë³µí•˜ë‹¤', r'ê³ ë§™ë‹¤',
            r'ë„ì›€', r'ì„±ê³µ', r'í•´ê²°', r'íš¨ê³¼', r'í¸ë¦¬í•˜ë‹¤', r'ë¹ ë¥´ë‹¤', r'ì‰½ë‹¤'
        ]
        self.positive_patterns = [re.compile(pattern) for pattern in positive_patterns]
        
        # 3. ë¶€ì • ë‹¨ì–´ íŒ¨í„´
        negative_patterns = [
            r'ë‚˜ì˜ë‹¤', r'ì‹«ë‹¤', r'ë¬¸ì œ', r'ì˜¤ë¥˜', r'ì–´ë µë‹¤', r'ëŠë¦¬ë‹¤', r'ë³µì¡í•˜ë‹¤',
            r'í˜ë“¤ë‹¤', r'ì•„ì‰½ë‹¤', r'ìœ ê°', r'ì‹¤ë§í•˜ë‹¤', r'í™”ë‚˜ë‹¤', r'ì§œì¦ë‚˜ë‹¤'
        ]
        self.negative_patterns = [re.compile(pattern) for pattern in negative_patterns]
        
        # 4. ì¿ ì…˜ì–´/ì™„ê³¡ í‘œí˜„
        euphonious_patterns = [
            r'ì‹¤ë¡€ì§€ë§Œ', r'ì£„ì†¡í•˜ì§€ë§Œ', r'ê´œì°®ìœ¼ì‹œë‹¤ë©´', r'í˜¹ì‹œë¼ë„', r'ë°”ì˜ì‹œê² ì§€ë§Œ',
            r'ë§Œì•½', r'ì˜ˆë¥¼ ë“¤ì–´', r'ì•„ì‰½ì§€ë§Œ', r'ìœ ê°ì´ì§€ë§Œ',
            r'ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤$', r'ã„¹ ê²ƒ ê°™ìŠµë‹ˆë‹¤$', r'ë“¯í•©ë‹ˆë‹¤$', r'ë¡œ ë³´ì…ë‹ˆë‹¤$'
        ]
        self.euphonious_patterns = [re.compile(pattern) for pattern in euphonious_patterns]
        
        # 5. ê³µê° í‘œí˜„
        empathy_patterns = [
            r'ì†ìƒí•˜ì…¨ê² ì–´ìš”', r'ë‹µë‹µí•˜ì…¨ê² ë„¤ìš”', r'ë§ì´ ë†€ë¼ì…¨ê² ì–´ìš”', r'ë¶ˆí¸í•˜ì…¨ê² ì–´ìš”',
            r'ì–´ë–¤ ë§ˆìŒì¸ì§€ ì•Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤', r'ì¶©ë¶„íˆ ì´í•´ë©ë‹ˆë‹¤', r'ê·¸ë ‡ê²Œ ìƒê°í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤'
        ]
        self.empathy_patterns = [re.compile(pattern) for pattern in empathy_patterns]
        
        # 6. ì‚¬ê³¼ í‘œí˜„
        apology_patterns = [
            r'ì£„ì†¡í•©ë‹ˆë‹¤', r'ë¯¸ì•ˆí•©ë‹ˆë‹¤', r'ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤', r'ì–‘í•´í•´ì£¼ì„¸ìš”', r'ë¶ˆí¸ì„ ë¼ì³ì„œ ì£„ì†¡í•©ë‹ˆë‹¤'
        ]
        self.apology_patterns = [re.compile(pattern) for pattern in apology_patterns]''',
                'description': 'ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ë¡œ ì„±ëŠ¥ í–¥ìƒ'
            },
            
            # ìºì‹± ì‹œìŠ¤í…œ ì¶”ê°€
            {
                'target': 'def __init__(self):',
                'addition': '''
        # ë¶„ì„ ê²°ê³¼ ìºì‹±
        self._analysis_cache = {}
        self._sentiment_cache = {}
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì¥ ìˆ˜''',
                'description': 'ë¶„ì„ ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ ì¶”ê°€'
            }
        ]
        
        self._apply_optimizations(analyzer_file, optimizations)
    
    def _improve_sentiment_analysis(self):
        """ê°ì • ë¶„ì„ ì •í™•ë„ ê°œì„ """
        # í–¥ìƒëœ ê°ì„± ì‚¬ì „ ìƒì„±
        enhanced_sentiment_dict = {
            # ê³ ê° ì„œë¹„ìŠ¤ íŠ¹í™” ê¸ì • ë‹¨ì–´
            "í•´ê²°": 2, "ë„ì›€": 2, "ê°ì‚¬": 2, "ì¹œì ˆ": 2, "ë¹ ë¥´ë‹¤": 1, "ì •í™•í•˜ë‹¤": 1,
            "ë§Œì¡±": 2, "í¸ë¦¬í•˜ë‹¤": 1, "íš¨ê³¼ì ": 1, "ì „ë¬¸ì ": 1, "ì‹ ì†í•˜ë‹¤": 1,
            "ê¼¼ê¼¼í•˜ë‹¤": 1, "ìƒì„¸í•˜ë‹¤": 1, "ì´í•´í•˜ê¸° ì‰½ë‹¤": 1, "ë„ì›€ì´ ë˜ë‹¤": 2,
            
            # ê³ ê° ì„œë¹„ìŠ¤ íŠ¹í™” ë¶€ì • ë‹¨ì–´
            "ë¶ˆë§Œ": -2, "ì‹¤ë§": -2, "ë‹µë‹µí•˜ë‹¤": -2, "ì§œì¦ë‚˜ë‹¤": -2, "í™”ë‚˜ë‹¤": -2,
            "ë¶ˆí¸í•˜ë‹¤": -1, "ì–´ë µë‹¤": -1, "ë³µì¡í•˜ë‹¤": -1, "ëŠë¦¬ë‹¤": -1, "ì˜¤ë¥˜": -2,
            "ë¬¸ì œ": -1, "ì‹¤íŒ¨": -2, "ì§€ì—°": -1, "ëˆ„ë½": -2, "ì˜¤ì‘ë™": -2,
            "ë¶ˆì¹œì ˆí•˜ë‹¤": -2, "ë¬´ì‹œí•˜ë‹¤": -2, "ê±°ë¶€í•˜ë‹¤": -2, "ê±°ì ˆí•˜ë‹¤": -1
        }
        
        # ê°ì„± ì‚¬ì „ íŒŒì¼ ì—…ë°ì´íŠ¸
        sentiment_file = "data/enhanced_sentiment_dict.json"
        os.makedirs(os.path.dirname(sentiment_file), exist_ok=True)
        
        with open(sentiment_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_sentiment_dict, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… í–¥ìƒëœ ê°ì„± ì‚¬ì „ ìƒì„±: {len(enhanced_sentiment_dict)}ê°œ ë‹¨ì–´")
    
    def _improve_korean_sentence_splitting(self):
        """í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  ê°œì„ """
        korean_sentence_splitter = '''
import re
from typing import List

class KoreanSentenceSplitter:
    """í•œêµ­ì–´ íŠ¹í™” ë¬¸ì¥ ë¶„í• ê¸°"""
    
    def __init__(self):
        # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
        self.sentence_end_patterns = [
            r'[.!?]+$',  # ì¼ë°˜ì ì¸ ì¢…ê²° ë¶€í˜¸
            r'[ê°€-í£]+[ë‹¤ìš”ë„¤ì£ êµ°ìš”ìŠµë‹ˆë‹¤ë‹ˆë‹¤ê¹Œìš”ì£ ìš”ë„¤ìš”ê±¸ìš”ì§€ìš”ê¹Œìš”]+$',  # í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸
            r'[ê°€-í£]+[ê² ìŠµë‹ˆë‹¤ê² ì–´ìš”ê² ë„¤ìš”ê² ì£ ]+$',  # ë¯¸ë˜/ì¶”ì¸¡ ì¢…ê²°
            r'[ê°€-í£]+[ì—ˆìŠµë‹ˆë‹¤ì—ˆì–´ìš”ì—ˆë„¤ìš”ì—ˆì£ ]+$',  # ê³¼ê±° ì¢…ê²°
            r'[ê°€-í£]+[ê³ ìˆìŠµë‹ˆë‹¤ê³ ìˆì–´ìš”]+$',  # ì§„í–‰í˜• ì¢…ê²°
        ]
        self.compiled_patterns = [re.compile(pattern) for pattern in self.sentence_end_patterns]
    
    def split_sentences(self, text: str) -> List[str]:
        """í•œêµ­ì–´ ë¬¸ì¥ ë¶„í• """
        if not text.strip():
            return []
        
        # ê¸°ë³¸ ë¶„í•  (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
        sentences = re.split(r'[.!?]+', text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ê¸°ë°˜ ì¶”ê°€ ë¶„í• 
        refined_sentences = []
        for sentence in sentences:
            if len(sentence) > 50:  # ê¸´ ë¬¸ì¥ë§Œ ì¶”ê°€ ë¶„í• 
                refined_sentences.extend(self._split_long_sentence(sentence))
            else:
                refined_sentences.append(sentence)
        
        return refined_sentences
    
    def _split_long_sentence(self, sentence: str) -> List[str]:
        """ê¸´ ë¬¸ì¥ì„ í•œêµ­ì–´ íŒ¨í„´ì— ë”°ë¼ ë¶„í• """
        # ì—°ê²°ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        split_patterns = [
            r'[ê°€-í£]+[ê³ ì„œë©°ëŠ”ë°]+',  # ì—°ê²°ì–´ë¯¸
            r'[ê°€-í£]+[ì§€ë§Œê·¸ëŸ°ë°í•˜ì§€ë§Œ]+',  # ëŒ€ì¡° ì—°ê²°ì–´
            r'[ê°€-í£]+[ê·¸ë¦¬ê³ ë˜í•œë˜í•œ]+',  # ì¶”ê°€ ì—°ê²°ì–´
        ]
        
        for pattern in split_patterns:
            if re.search(pattern, sentence):
                parts = re.split(pattern, sentence)
                if len(parts) > 1:
                    return [part.strip() for part in parts if part.strip()]
        
        return [sentence]

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
korean_sentence_splitter = KoreanSentenceSplitter()
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/text/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(korean_sentence_splitter)
    
    def optimize_database_operations(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ìµœì í™”"""
        self.logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ìµœì í™” ì‹œì‘")
        
        # 1. ì—°ê²° í’€ë§
        self._optimize_database_connections()
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬
        self._optimize_batch_operations()
        
        # 3. ì¸ë±ìŠ¤ ìµœì í™”
        self._optimize_database_indexes()
        
        self.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ìµœì í™” ì™„ë£Œ")
    
    def _optimize_database_connections(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì í™”"""
        db_manager_file = "src/db/manager.py"
        
        connection_pooling = '''
import sqlite3
from contextlib import contextmanager
from typing import Optional

class DatabaseConnectionPool:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€"""
    
    def __init__(self, db_path: str, max_connections: int = 5):
        self.db_path = db_path
        self.max_connections = max_connections
        self._connections = []
        self._lock = threading.Lock()
    
    @contextmanager
    def get_connection(self):
        """ì—°ê²° í’€ì—ì„œ ì—°ê²° ê°€ì ¸ì˜¤ê¸°"""
        connection = None
        try:
            with self._lock:
                if self._connections:
                    connection = self._connections.pop()
                else:
                    connection = sqlite3.connect(self.db_path)
                    connection.row_factory = sqlite3.Row
            
            yield connection
        finally:
            if connection:
                with self._lock:
                    if len(self._connections) < self.max_connections:
                        self._connections.append(connection)
                    else:
                        connection.close()

# ì „ì—­ ì—°ê²° í’€
_connection_pool = None

def get_connection_pool(db_path: str) -> DatabaseConnectionPool:
    """ì—°ê²° í’€ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _connection_pool
    if _connection_pool is None:
        _connection_pool = DatabaseConnectionPool(db_path)
    return _connection_pool
'''
        
        # db/manager.pyì— ì¶”ê°€
        with open(db_manager_file, 'a', encoding='utf-8') as f:
            f.write(connection_pooling)
    
    def _optimize_batch_operations(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        batch_operations = '''
def batch_insert_consultation_analysis(self, analyses: List[Dict[str, Any]]) -> bool:
    """ìƒë‹´ ë¶„ì„ ê²°ê³¼ ë°°ì¹˜ ì‚½ì…"""
    try:
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # íŠ¸ëœì­ì…˜ ì‹œì‘
            cursor.execute("BEGIN TRANSACTION")
            
            for analysis in analyses:
                cursor.execute("""
                    INSERT INTO consultation_analysis (
                        consultation_id, business_type, classification_type,
                        detailed_classification, consultation_result, summary,
                        customer_request, solution, additional_info, confidence,
                        created_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    analysis['consultation_id'],
                    analysis['business_type'],
                    analysis['classification_type'],
                    analysis['detailed_classification'],
                    analysis['consultation_result'],
                    analysis['summary'],
                    analysis['customer_request'],
                    analysis['solution'],
                    analysis['additional_info'],
                    analysis['confidence'],
                    datetime.now()
                ))
            
            # íŠ¸ëœì­ì…˜ ì»¤ë°‹
            conn.commit()
            return True
            
    except Exception as e:
        self.logger.error(f"ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: {e}")
        return False
'''
        
        # db/manager.pyì— ì¶”ê°€
        with open(db_manager_file, 'a', encoding='utf-8') as f:
            f.write(batch_operations)
    
    def _optimize_database_indexes(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”"""
        index_optimization = '''
def optimize_database_indexes(self):
    """ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”"""
    try:
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # ìƒë‹´ ë¶„ì„ í…Œì´ë¸” ì¸ë±ìŠ¤
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_consultation_analysis_id 
                ON consultation_analysis(consultation_id)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_consultation_analysis_type 
                ON consultation_analysis(business_type, classification_type)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_consultation_analysis_date 
                ON consultation_analysis(created_at)
            """)
            
            # ì˜¤ë””ì˜¤ ì†ì„± í…Œì´ë¸” ì¸ë±ìŠ¤
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_audio_properties_file 
                ON audio_properties(file_path)
            """)
            
            conn.commit()
            self.logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")
            
    except Exception as e:
        self.logger.error(f"ì¸ë±ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
'''
        
        # db/manager.pyì— ì¶”ê°€
        with open(db_manager_file, 'a', encoding='utf-8') as f:
            f.write(index_optimization)
    
    def optimize_error_handling(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ìµœì í™”"""
        self.logger.info("ğŸ”§ ì˜¤ë¥˜ ì²˜ë¦¬ ìµœì í™” ì‹œì‘")
        
        # 1. ì¤‘ì•™í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬
        self._create_centralized_error_handler()
        
        # 2. ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        self._implement_retry_mechanism()
        
        # 3. ì˜¤ë¥˜ ë¡œê¹… ê°œì„ 
        self._improve_error_logging()
        
        self.logger.info("âœ… ì˜¤ë¥˜ ì²˜ë¦¬ ìµœì í™” ì™„ë£Œ")
    
    def _create_centralized_error_handler(self):
        """ì¤‘ì•™í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ê¸° ìƒì„±"""
        error_handler = '''
import functools
import traceback
from typing import Callable, Any, Optional

class ErrorHandler:
    """ì¤‘ì•™í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, logger):
        self.logger = logger
    
    def handle_errors(self, func: Callable) -> Callable:
        """ì˜¤ë¥˜ ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                self.logger.error(f"í•¨ìˆ˜ {func.__name__} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                self.logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                raise
        return wrapper
    
    def safe_execute(self, func: Callable, *args, **kwargs) -> Optional[Any]:
        """ì•ˆì „í•œ í•¨ìˆ˜ ì‹¤í–‰"""
        try:
            return func(*args, **kwargs)
        except Exception as e:
            self.logger.error(f"ì•ˆì „í•œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ì˜¤ë¥˜ ì²˜ë¦¬ê¸°
error_handler = ErrorHandler(logging.getLogger(__name__))
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/utils/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(error_handler)
    
    def _implement_retry_mechanism(self):
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„"""
        retry_mechanism = '''
import time
from functools import wraps

def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                    else:
                        raise last_exception
            
            return None
        return wrapper
    return decorator

@retry_on_failure(max_retries=3, delay=1.0)
def safe_api_call(func, *args, **kwargs):
    """ì•ˆì „í•œ API í˜¸ì¶œ"""
    return func(*args, **kwargs)
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/utils/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(retry_mechanism)
    
    def _improve_error_logging(self):
        """ì˜¤ë¥˜ ë¡œê¹… ê°œì„ """
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('logs', exist_ok=True)
        
        # í–¥ìƒëœ ë¡œê¹… ì„¤ì •
        enhanced_logging = '''
import logging.handlers
import os

def setup_enhanced_logging():
    """í–¥ìƒëœ ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('logs', exist_ok=True)
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger('callytics')
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì¼ë³„ ë¡œí…Œì´ì…˜)
    file_handler = logging.handlers.TimedRotatingFileHandler(
        'logs/callytics.log',
        when='midnight',
        interval=1,
        backupCount=30,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„°
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ì „ì—­ ë¡œê±°
logger = setup_enhanced_logging()
'''
        
        # utils.pyì— ì¶”ê°€
        utils_file = "src/utils/utils.py"
        with open(utils_file, 'a', encoding='utf-8') as f:
            f.write(enhanced_logging)
    
    def _apply_optimizations(self, file_path: str, optimizations: List[Dict]):
        """ìµœì í™” ì ìš©"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for opt in optimizations:
                if 'target' in opt and 'replacement' in opt:
                    # ê¸°ì¡´ ì½”ë“œ êµì²´
                    if opt['target'] in content:
                        content = content.replace(opt['target'], opt['replacement'])
                        self.logger.info(f"âœ… {opt['description']} ì ìš©")
                    else:
                        self.logger.warning(f"âš ï¸ ëŒ€ìƒ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {opt['target']}")
                
                elif 'target' in opt and 'addition' in opt:
                    # ì½”ë“œ ì¶”ê°€
                    if opt['target'] in content:
                        content = content.replace(opt['target'], opt['target'] + opt['addition'])
                        self.logger.info(f"âœ… {opt['description']} ì ìš©")
                    else:
                        self.logger.warning(f"âš ï¸ ëŒ€ìƒ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {opt['target']}")
            
            # íŒŒì¼ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
        except Exception as e:
            self.logger.error(f"ìµœì í™” ì ìš© ì‹¤íŒ¨ ({file_path}): {e}")
    
    def run_full_optimization(self):
        """ì „ì²´ ìµœì í™” ì‹¤í–‰"""
        self.logger.info("ğŸš€ Callytics íŒŒì´í”„ë¼ì¸ ì „ë©´ ìµœì í™” ì‹œì‘")
        
        try:
            # 1. ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìµœì í™”
            self.optimize_audio_processing()
            
            # 2. í…ìŠ¤íŠ¸ ë¶„ì„ ìµœì í™”
            self.optimize_text_analysis()
            
            # 3. ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ìµœì í™”
            self.optimize_database_operations()
            
            # 4. ì˜¤ë¥˜ ì²˜ë¦¬ ìµœì í™”
            self.optimize_error_handling()
            
            # 5. ì„ì‹œ íŒŒì¼ ì •ë¦¬
            self._cleanup_temp_files()
            
            self.logger.info("ğŸ‰ ì „ì²´ ìµœì í™” ì™„ë£Œ!")
            self._print_optimization_summary()
            
        except Exception as e:
            self.logger.error(f"ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _cleanup_temp_files(self):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        temp_dirs = ['.temp', 'temp', 'logs']
        
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                    os.makedirs(temp_dir, exist_ok=True)
                    self.logger.info(f"âœ… {temp_dir} ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {temp_dir} ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _print_optimization_summary(self):
        """ìµœì í™” ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ ìµœì í™” ì™„ë£Œ ìš”ì•½")
        print("="*60)
        print("âœ… ì„±ëŠ¥ ê°œì„ :")
        print("   â€¢ ëª¨ë¸ ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„")
        print("   â€¢ GPU ë©”ëª¨ë¦¬ ìµœì í™”")
        print("   â€¢ ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼")
        print("   â€¢ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ")
        print()
        print("âœ… ì •í™•ë„ ê°œì„ :")
        print("   â€¢ í–¥ìƒëœ ê°ì„± ì‚¬ì „")
        print("   â€¢ í•œêµ­ì–´ íŠ¹í™” ë¬¸ì¥ ë¶„í• ")
        print("   â€¢ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¶„ì„")
        print()
        print("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:")
        print("   â€¢ ì„ì‹œ íŒŒì¼ ìë™ ì •ë¦¬")
        print("   â€¢ ì—°ê²° í’€ë§")
        print("   â€¢ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€")
        print()
        print("âœ… ì•ˆì •ì„± ê°œì„ :")
        print("   â€¢ ì¤‘ì•™í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬")
        print("   â€¢ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜")
        print("   â€¢ í–¥ìƒëœ ë¡œê¹…")
        print("="*60)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    optimizer = PipelineOptimizer()
    optimizer.run_full_optimization()

if __name__ == "__main__":
    main() 