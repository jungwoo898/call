#!/usr/bin/env python3
"""
ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢… ê°„ë‹¨ í…ŒìŠ¤íŠ¸
ì˜ì¡´ì„± ìµœì†Œí™” ë²„ì „
"""

import re
from typing import Dict, List, Any, Optional, Tuple


class SimpleQuantitativeAnalyzer:
    """ê°„ë‹¨í•œ ì •ëŸ‰ ë¶„ì„ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        pass
    
    def _map_sentiment_to_score(self, sentiment_text: str) -> Optional[float]:
        """sentiment í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ì ìˆ˜ë¡œ ë§¤í•‘"""
        sentiment_mapping = {
            # ê¸°ë³¸ ë§¤í•‘
            'positive': 1.0,
            'neutral': 0.0,
            'negative': -1.0,
            
            # í™•ì¥ ë§¤í•‘ (5ì  ì²™ë„)
            'very positive': 2.0,
            'very_positive': 2.0,
            'very negative': -2.0,
            'very_negative': -2.0,
            
            # í•œêµ­ì–´ ë§¤í•‘
            'ê¸ì •': 1.0,
            'ë¶€ì •': -1.0,
            'ì¤‘ë¦½': 0.0,
            'ë§¤ìš°ê¸ì •': 2.0,
            'ë§¤ìš°ë¶€ì •': -2.0,
            
            # ìˆ«ì ë¬¸ìì—´ ì§ì ‘ ë§¤í•‘
            '1': 1.0,
            '0': 0.0,
            '-1': -1.0,
            '2': 2.0,
            '-2': -2.0
        }
        
        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘ ì‹œë„
        normalized_text = sentiment_text.strip().lower().replace(' ', '_')
        
        if normalized_text in sentiment_mapping:
            return sentiment_mapping[normalized_text]
        
        # ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜ ì‹œë„
        try:
            score = float(sentiment_text)
            return max(-2.0, min(2.0, score))  # -2.0 ~ 2.0 ë²”ìœ„ë¡œ ì œí•œ
        except (ValueError, TypeError):
            pass
        
        return None
    
    def calculate_customer_sentiment_trend(self, utterances_data: List[Dict[str, Any]]) -> Tuple[Optional[float], Optional[float], Optional[float]]:
        """
        ê³ ê° ê°ì • ì¶”ì„¸ ë¶„ì„ (ì§€í‘œ 1, 2, 3)
        
        Returns
        -------
        tuple: (customer_sentiment_early, customer_sentiment_late, customer_sentiment_trend)
        """
        try:
            # 1. ê³ ê° ë°œí™”ë§Œ í•„í„°ë§
            customer_utterances = []
            for utterance in utterances_data:
                speaker = utterance.get('speaker', '').lower()
                if any(keyword in speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']):
                    customer_utterances.append(utterance)
            
            if len(customer_utterances) < 3:  # ìµœì†Œ 3ê°œ ë°œí™” í•„ìš”
                return None, None, None
            
            # 2. sentiment í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë§¤í•‘
            sentiment_scores = []
            for utterance in customer_utterances:
                sentiment_text = utterance.get('sentiment', '').lower()
                score = self._map_sentiment_to_score(sentiment_text)
                if score is not None:
                    sentiment_scores.append(score)
            
            if len(sentiment_scores) < 3:
                return None, None, None
            
            # 3. ì´ˆë°˜ë¶€(ì²˜ìŒ 33%)ì™€ í›„ë°˜ë¶€(ë 33%) êµ¬ë¶„
            total_count = len(sentiment_scores)
            early_count = max(1, int(total_count * 0.33))
            late_count = max(1, int(total_count * 0.33))
            
            early_scores = sentiment_scores[:early_count]
            late_scores = sentiment_scores[-late_count:]
            
            # 4. ê° êµ¬ê°„ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
            customer_sentiment_early = round(sum(early_scores) / len(early_scores), 3)
            customer_sentiment_late = round(sum(late_scores) / len(late_scores), 3)
            
            # 5. ê°ì • ë³€í™” ì¶”ì„¸ ê³„ì‚° (í›„ë°˜ë¶€ - ì´ˆë°˜ë¶€)
            customer_sentiment_trend = round(customer_sentiment_late - customer_sentiment_early, 3)
            
            return customer_sentiment_early, customer_sentiment_late, customer_sentiment_trend
            
        except Exception as e:
            print(f"âš ï¸ ê³ ê° ê°ì • ì¶”ì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def calculate_avg_response_latency(self, utterances_data: List[Dict[str, Any]]) -> Optional[float]:
        """
        í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„ ê³„ì‚° (ì§€í‘œ 4)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            ë°œí™” ë°ì´í„° (start_time, end_time, speaker í¬í•¨)
            
        Returns
        -------
        Optional[float]
            í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„(ì´ˆ) ë˜ëŠ” None
        """
        try:
            # 1. ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_utterances = sorted(utterances_data, key=lambda x: x.get('start_time', 0))
            
            latencies = []
            prev_utterance = None
            
            # 2. ê³ ê° -> ìƒë‹´ì‚¬ ì „í™˜ ì§€ì  ì°¾ê¸°
            for utterance in sorted_utterances:
                current_speaker = utterance.get('speaker', '').lower()
                current_start_time = utterance.get('start_time')
                
                if prev_utterance is not None:
                    prev_speaker = prev_utterance.get('speaker', '').lower()
                    prev_end_time = prev_utterance.get('end_time')
                    
                    # ê³ ê° -> ìƒë‹´ì‚¬ ì „í™˜ í™•ì¸
                    is_customer_to_counselor = (
                        any(keyword in prev_speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']) and
                        any(keyword in current_speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff'])
                    )
                    
                    if is_customer_to_counselor and prev_end_time is not None and current_start_time is not None:
                        # 3. ì§€ì—°ì‹œê°„ = ìƒë‹´ì‚¬ ë°œí™” ì‹œì‘ - ê³ ê° ë°œí™” ì¢…ë£Œ
                        latency = current_start_time - prev_end_time
                        if latency >= 0:  # ìŒìˆ˜ ì§€ì—°ì‹œê°„ ì œì™¸
                            latencies.append(latency)
                
                prev_utterance = utterance
            
            # 4. í‰ê·  ê³„ì‚°
            if latencies:
                avg_latency = sum(latencies) / len(latencies)
                return round(avg_latency, 3)
            else:
                return None
                
        except Exception as e:
            print(f"âš ï¸ í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def calculate_task_ratio(self, utterances_data: List[Dict[str, Any]]) -> Optional[float]:
        """
        ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ ê³„ì‚° (ì§€í‘œ 5)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            ë°œí™” ë°ì´í„° (start_time, end_time, speaker í¬í•¨)
            
        Returns
        -------
        Optional[float]
            ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ (ê³ ê° ë°œí™” ì‹œê°„ / ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„) ë˜ëŠ” None
        """
        try:
            customer_total_time = 0.0
            counselor_total_time = 0.0
            
            # 1. ê° í™”ìë³„ ì´ ë°œí™” ì‹œê°„ ê³„ì‚°
            for utterance in utterances_data:
                speaker = utterance.get('speaker', '').lower()
                start_time = utterance.get('start_time')
                end_time = utterance.get('end_time')
                
                if start_time is not None and end_time is not None and end_time > start_time:
                    duration = end_time - start_time
                    
                    # ê³ ê° ë°œí™” ì‹œê°„
                    if any(keyword in speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']):
                        customer_total_time += duration
                    
                    # ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„
                    elif any(keyword in speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff']):
                        counselor_total_time += duration
            
            # 2. ë¹„ìœ¨ ê³„ì‚°
            if counselor_total_time > 0:
                task_ratio = customer_total_time / counselor_total_time
                return round(task_ratio, 3)
            else:
                return None  # ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„ì´ 0ì¸ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                
        except Exception as e:
            print(f"âš ï¸ ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None


def test_quantitative_metrics():
    """ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢… í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢… í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*70)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê³ ê° ê°ì •ì´ ê°œì„ ë˜ëŠ” ìƒë‹´
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê³ ê° ê°ì • ê°œì„  ìƒë‹´")
    print("-"*50)
    
    improving_case = [
        # ì´ˆë°˜ë¶€ - ë¶€ì •ì  ê°ì •
        {"speaker": "ê³ ê°", "text": "ì •ë§ í™”ê°€ ë‚˜ìš”! ì„œë¹„ìŠ¤ê°€ ë„ˆë¬´ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”!", 
         "start_time": 0.0, "end_time": 3.5, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤. ë¶ˆí¸ì„ ë“œë ¤ ëŒ€ë‹¨íˆ ì£„ì†¡í•©ë‹ˆë‹¤. ì–´ë–¤ ë¬¸ì œì¸ì§€ ìì„¸íˆ ë§ì”€í•´ì£¼ì„¸ìš”.", 
         "start_time": 5.0, "end_time": 9.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ì¸í„°ë„·ì´ ê³„ì† ëŠì–´ì ¸ìš”. ì§œì¦ë‚˜ì„œ ë¯¸ì¹˜ê² ì–´ìš”!", 
         "start_time": 10.0, "end_time": 13.0, "sentiment": "very negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì†ìƒí•˜ì…¨ê² ì–´ìš”. ì¦‰ì‹œ ê¸°ìˆ ì§„ì—ê²Œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.", 
         "start_time": 14.5, "end_time": 18.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ë¹¨ë¦¬ í•´ê²°í•´ì£¼ì„¸ìš”. ì—…ë¬´ì— ì§€ì¥ì´ ìˆì–´ìš”.", 
         "start_time": 19.0, "end_time": 22.0, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë„¤, ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì›ê²©ìœ¼ë¡œ ì ê²€í•´ë³´ê² ìŠµë‹ˆë‹¤.", 
         "start_time": 23.5, "end_time": 27.0, "sentiment": "positive"},
        
        # ì¤‘ë°˜ë¶€ - ì¤‘ë¦½ì  ê°ì •
        {"speaker": "ê³ ê°", "text": "ì§€ê¸ˆ ì–´ë–»ê²Œ ë˜ê³  ìˆëŠ” ê±´ê°€ìš”?", 
         "start_time": 28.0, "end_time": 30.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë¼ìš°í„° ì„¤ì •ì— ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ìˆ˜ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.", 
         "start_time": 31.0, "end_time": 34.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ì–¼ë§ˆë‚˜ ë” ê±¸ë¦´ê¹Œìš”?", 
         "start_time": 35.0, "end_time": 37.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "5ë¶„ ì •ë„ë©´ ì™„ë£Œë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.", 
         "start_time": 38.0, "end_time": 41.0, "sentiment": "positive"},
        
        # í›„ë°˜ë¶€ - ê¸ì •ì  ê°ì •
        {"speaker": "ê³ ê°", "text": "ì˜¤, ì´ì œ ì˜ ë˜ëŠ” ê²ƒ ê°™ë„¤ìš”!", 
         "start_time": 42.0, "end_time": 44.0, "sentiment": "positive"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë‹¤í–‰ì…ë‹ˆë‹¤! ì´ì œ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•  ê±°ì˜ˆìš”. ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”.", 
         "start_time": 45.0, "end_time": 49.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ê°ì‚¬í•©ë‹ˆë‹¤. ë¹ ë¥´ê²Œ í•´ê²°í•´ì£¼ì…”ì„œ ì •ë§ ê³ ë§ˆì›Œìš”.", 
         "start_time": 50.0, "end_time": 53.0, "sentiment": "positive"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì²œë§Œì—ìš”. ì•ìœ¼ë¡œë„ ë¶ˆí¸í•œ ì  ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.", 
         "start_time": 54.0, "end_time": 57.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ë„¤, ì •ë§ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„œë¹„ìŠ¤ì˜€ì–´ìš”. í›Œë¥­í•©ë‹ˆë‹¤!", 
         "start_time": 58.0, "end_time": 61.0, "sentiment": "very positive"}
    ]
    
    analyzer = SimpleQuantitativeAnalyzer()
    
    # ì§€í‘œ ê³„ì‚°
    early, late, trend = analyzer.calculate_customer_sentiment_trend(improving_case)
    latency = analyzer.calculate_avg_response_latency(improving_case)
    task_ratio = analyzer.calculate_task_ratio(improving_case)
    
    print("ğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
    print(f"  ê³ ê° ê°ì • ì´ˆë°˜ë¶€: {early}")
    print(f"  ê³ ê° ê°ì • í›„ë°˜ë¶€: {late}")
    print(f"  ê°ì • ë³€í™” ì¶”ì„¸: {trend} (ì–‘ìˆ˜ë©´ ê°œì„ )")
    print(f"  í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„: {latency}ì´ˆ")
    print(f"  ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨: {task_ratio} (ê³ ê°/ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„ ë¹„ìœ¨)")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ê³ ê° ê°ì •ì´ ì•…í™”ë˜ëŠ” ìƒë‹´
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ê³ ê° ê°ì • ì•…í™” ìƒë‹´")
    print("-"*50)
    
    worsening_case = [
        # ì´ˆë°˜ë¶€ - ì¤‘ë¦½ì  ê°ì •
        {"speaker": "ê³ ê°", "text": "ì•ˆë…•í•˜ì„¸ìš”. ìš”ê¸ˆ ë¬¸ì˜ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤.", 
         "start_time": 0.0, "end_time": 3.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë„¤, ìš”ê¸ˆ ê´€ë ¨ ë¬¸ì˜ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.", 
         "start_time": 5.0, "end_time": 7.0, "sentiment": "positive"},
        
        {"speaker": "ê³ ê°", "text": "ì´ë²ˆ ë‹¬ ìš”ê¸ˆì´ ì¢€ ë†’ê²Œ ë‚˜ì˜¨ ê²ƒ ê°™ì•„ì„œìš”.", 
         "start_time": 8.0, "end_time": 11.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.", 
         "start_time": 13.0, "end_time": 15.0, "sentiment": "neutral"},
        
        # ì¤‘ë°˜ë¶€ - ì•½ê°„ ë¶€ì •ì 
        {"speaker": "ê³ ê°", "text": "ì™œ ì´ë ‡ê²Œ ì˜¤ë˜ ê±¸ë¦¬ë‚˜ìš”?", 
         "start_time": 16.0, "end_time": 18.0, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì‹œìŠ¤í…œì´ ì¢€ ëŠë ¤ì„œ ê·¸ë ‡ìŠµë‹ˆë‹¤.", 
         "start_time": 20.0, "end_time": 22.0, "sentiment": "neutral"},
        
        {"speaker": "ê³ ê°", "text": "ë§¤ë²ˆ ì´ëŸ° ì‹ì´ë„¤ìš”. ë¶ˆí¸í•´ìš”.", 
         "start_time": 23.0, "end_time": 25.0, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ë” ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.", 
         "start_time": 27.0, "end_time": 29.0, "sentiment": "neutral"},
        
        # í›„ë°˜ë¶€ - ë§¤ìš° ë¶€ì •ì 
        {"speaker": "ê³ ê°", "text": "ì •ë§ ì§œì¦ë‚˜ë„¤ìš”! ì‹œê°„ ë‚­ë¹„ì˜ˆìš”!", 
         "start_time": 30.0, "end_time": 33.0, "sentiment": "very negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ìŒì— ë” ë¹¨ë¦¬ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.", 
         "start_time": 35.0, "end_time": 37.0, "sentiment": "negative"},
        
        {"speaker": "ê³ ê°", "text": "ì´ëŸ° ì„œë¹„ìŠ¤ë¡œëŠ” ì•ˆ ë˜ê² ì–´ìš”. ì •ë§ ì‹¤ë§ì´ì—ìš”!", 
         "start_time": 38.0, "end_time": 41.0, "sentiment": "very negative"}
    ]
    
    early2, late2, trend2 = analyzer.calculate_customer_sentiment_trend(worsening_case)
    latency2 = analyzer.calculate_avg_response_latency(worsening_case)
    task_ratio2 = analyzer.calculate_task_ratio(worsening_case)
    
    print("ğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
    print(f"  ê³ ê° ê°ì • ì´ˆë°˜ë¶€: {early2}")
    print(f"  ê³ ê° ê°ì • í›„ë°˜ë¶€: {late2}")
    print(f"  ê°ì • ë³€í™” ì¶”ì„¸: {trend2} (ìŒìˆ˜ë©´ ì•…í™”)")
    print(f"  í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„: {latency2}ì´ˆ")
    print(f"  ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨: {task_ratio2}")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ì‘ë‹µ ì§€ì—°ì´ ë§ì€ ìƒë‹´
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ì‘ë‹µ ì§€ì—° ìƒë‹´")
    print("-"*50)
    
    delayed_response_case = [
        {"speaker": "ê³ ê°", "text": "ë¬¸ì˜ê°€ ìˆìŠµë‹ˆë‹¤.", 
         "start_time": 0.0, "end_time": 2.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë„¤, ë§ì”€í•˜ì„¸ìš”.", 
         "start_time": 8.0, "end_time": 10.0, "sentiment": "positive"},  # 6ì´ˆ ì§€ì—°
        
        {"speaker": "ê³ ê°", "text": "ì„œë¹„ìŠ¤ í•´ì§€í•˜ê³  ì‹¶ì–´ìš”.", 
         "start_time": 11.0, "end_time": 13.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "í•´ì§€ ì‚¬ìœ ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.", 
         "start_time": 18.0, "end_time": 20.0, "sentiment": "neutral"},  # 5ì´ˆ ì§€ì—°
        
        {"speaker": "ê³ ê°", "text": "ìš”ê¸ˆì´ ë„ˆë¬´ ë¹„ì‹¸ìš”.", 
         "start_time": 21.0, "end_time": 23.0, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë” ì €ë ´í•œ ìš”ê¸ˆì œë¥¼ ì¶”ì²œí•´ë“œë¦´ê¹Œìš”?", 
         "start_time": 30.0, "end_time": 33.0, "sentiment": "positive"}  # 7ì´ˆ ì§€ì—°
    ]
    
    early3, late3, trend3 = analyzer.calculate_customer_sentiment_trend(delayed_response_case)
    latency3 = analyzer.calculate_avg_response_latency(delayed_response_case)
    task_ratio3 = analyzer.calculate_task_ratio(delayed_response_case)
    
    print("ğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
    print(f"  í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„: {latency3}ì´ˆ (ë†’ì€ ì§€ì—°)")
    print(f"  ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨: {task_ratio3}")
    
    # ì¢…í•© ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì¢…í•© ë¹„êµ ê²°ê³¼")
    print("="*70)
    print(f"{'í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤':<15} {'ê°ì •ì´ˆë°˜ë¶€':<10} {'ê°ì •í›„ë°˜ë¶€':<10} {'ê°ì •ì¶”ì„¸':<8} {'ì‘ë‹µì§€ì—°(ì´ˆ)':<12} {'ì—…ë¬´ë¹„ìœ¨':<8}")
    print("-"*70)
    print(f"{'ê°ì • ê°œì„  ìƒë‹´':<15} {early:<10} {late:<10} {trend:<8} {latency:<12} {task_ratio:<8}")
    print(f"{'ê°ì • ì•…í™” ìƒë‹´':<15} {early2:<10} {late2:<10} {trend2:<8} {latency2:<12} {task_ratio2:<8}")
    print(f"{'ì‘ë‹µ ì§€ì—° ìƒë‹´':<15} {early3 or 'N/A':<10} {late3 or 'N/A':<10} {trend3 or 'N/A':<8} {latency3:<12} {task_ratio3:<8}")
    
    # ì§€í‘œ í•´ì„ ê°€ì´ë“œ
    print("\nğŸ’¡ ì§€í‘œ í•´ì„ ê°€ì´ë“œ")
    print("="*70)
    print("ğŸ“ˆ ê³ ê° ê°ì • ë³€í™” ì¶”ì„¸:")
    print("  - ì–‘ìˆ˜(+): ìƒë‹´ ê³¼ì •ì—ì„œ ê³ ê° ê°ì •ì´ ê°œì„ ë¨")
    print("  - ìŒìˆ˜(-): ìƒë‹´ ê³¼ì •ì—ì„œ ê³ ê° ê°ì •ì´ ì•…í™”ë¨")
    print("  - 0ì— ê°€ê¹Œì›€: ê°ì • ë³€í™”ê°€ ê±°ì˜ ì—†ìŒ")
    
    print("\nâ±ï¸ í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„:")
    print("  - 0~2ì´ˆ: ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ")
    print("  - 2~5ì´ˆ: ì ì ˆí•œ ì‘ë‹µ ì†ë„")
    print("  - 5ì´ˆ ì´ìƒ: ì‘ë‹µ ì§€ì—°, ê°œì„  í•„ìš”")
    
    print("\nâš–ï¸ ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨:")
    print("  - 1.0 ì´ˆê³¼: ê³ ê°ì´ ìƒë‹´ì‚¬ë³´ë‹¤ ë§ì´ ë§í•¨ (ì„¤ëª…/ë¬¸ì˜ê°€ ë§ìŒ)")
    print("  - 1.0 ë¯¸ë§Œ: ìƒë‹´ì‚¬ê°€ ê³ ê°ë³´ë‹¤ ë§ì´ ë§í•¨ (ì•ˆë‚´/ì„¤ëª… ì œê³µ)")
    print("  - 1.0ì— ê°€ê¹Œì›€: ê· í˜•ì ì¸ ëŒ€í™”")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ“ ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢…ì´ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    test_quantitative_metrics() 