#!/usr/bin/env python3
"""
GPU ëª¨ë¸ Pre-loading ë° ìºì‹± ì‹œìŠ¤í…œ
ë¶„ì„ ìš”ì²­ ì‹œ ì¦‰ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë”©
"""

import os
import time
import logging
import asyncio
from typing import Dict, Any, Optional
from pathlib import Path
import torch
from transformers import pipeline
import whisper

logger = logging.getLogger(__name__)

class ModelPreloader:
    """GPU ëª¨ë¸ ì‚¬ì „ ë¡œë”© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.models = {}
        self.loading_status = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.preload_complete = False
        
        logger.info(f"ModelPreloader ì´ˆê¸°í™” - ë””ë°”ì´ìŠ¤: {self.device}")
    
    async def preload_all_models(self):
        """ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‚¬ì „ ë¡œë”©"""
        start_time = time.time()
        logger.info("ğŸš€ GPU ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì‹œì‘...")
        
        # ë³‘ë ¬ ë¡œë”© íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            asyncio.create_task(self._load_whisper_model()),
            asyncio.create_task(self._load_diarization_model()),
            asyncio.create_task(self._load_sentiment_model()),
            asyncio.create_task(self._load_punctuation_model()),
        ]
        
        # ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ í™•ì¸
        success_count = sum(1 for result in results if not isinstance(result, Exception))
        total_time = time.time() - start_time
        
        self.preload_complete = success_count >= 3  # ìµœì†Œ 3ê°œ ëª¨ë¸ ì„±ê³µ
        
        if self.preload_complete:
            logger.info(f"âœ… ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì™„ë£Œ! ({success_count}/4ê°œ ì„±ê³µ, {total_time:.1f}ì´ˆ)")
        else:
            logger.warning(f"âš ï¸ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ë¶€ë¶„ ì‹¤íŒ¨ ({success_count}/4ê°œ ì„±ê³µ)")
        
        return self.preload_complete
    
    async def _load_whisper_model(self):
        """Whisper ìŒì„± ì¸ì‹ ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ“¥ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.loading_status['whisper'] = 'loading'
            
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            # Whisper ëª¨ë¸ ë¡œë”© (í•œêµ­ì–´ ìµœì í™”)
            model = whisper.load_model("medium", device=self.device)
            
            # ëª¨ë¸ warm-up (ë”ë¯¸ ë°ì´í„°ë¡œ ì²« ì¶”ë¡ )
            dummy_audio = torch.zeros(16000, device=self.device)  # 1ì´ˆ ë¬´ìŒ
            _ = model.transcribe(dummy_audio.cpu().numpy())
            
            self.models['whisper'] = model
            self.loading_status['whisper'] = 'ready'
            logger.info("âœ… Whisper ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Whisper ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.loading_status['whisper'] = 'failed'
            raise
    
    async def _load_diarization_model(self):
        """í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ“¥ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.loading_status['diarization'] = 'loading'
            
            from pyannote.audio import Pipeline
            
            # ì‚¬ì „ í›ˆë ¨ëœ íŒŒì´í”„ë¼ì¸ ë¡œë”©
            pipeline_model = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=os.getenv("HUGGINGFACE_TOKEN")
            )
            
            if self.device == "cuda":
                pipeline_model = pipeline_model.to(torch.device("cuda"))
            
            # ëª¨ë¸ warm-up
            dummy_audio = {"waveform": torch.zeros(1, 16000), "sample_rate": 16000}
            if self.device == "cuda":
                dummy_audio["waveform"] = dummy_audio["waveform"].cuda()
            
            _ = pipeline_model(dummy_audio)
            
            self.models['diarization'] = pipeline_model
            self.loading_status['diarization'] = 'ready'
            logger.info("âœ… í™”ì ë¶„ë¦¬ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.loading_status['diarization'] = 'failed'
            raise
    
    async def _load_sentiment_model(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ“¥ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.loading_status['sentiment'] = 'loading'
            
            # í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸
            sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="beomi/KcELECTRA-base-v2022",
                device=0 if self.device == "cuda" else -1
            )
            
            # ëª¨ë¸ warm-up
            _ = sentiment_pipeline("ì•ˆë…•í•˜ì„¸ìš”")
            
            self.models['sentiment'] = sentiment_pipeline
            self.loading_status['sentiment'] = 'ready'
            logger.info("âœ… ê°ì • ë¶„ì„ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.loading_status['sentiment'] = 'failed'
            raise
    
    async def _load_punctuation_model(self):
        """ë¬¸ì¥ ë¶€í˜¸ ë³µì› ëª¨ë¸ ë¡œë”©"""
        try:
            logger.info("ğŸ“¥ ë¬¸ì¥ ë¶€í˜¸ ë³µì› ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.loading_status['punctuation'] = 'loading'
            
            # í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ë³µì› ëª¨ë¸
            punctuation_pipeline = pipeline(
                "text2text-generation",
                model="paust/pko-t5-base",
                device=0 if self.device == "cuda" else -1
            )
            
            # ëª¨ë¸ warm-up
            _ = punctuation_pipeline("ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”")
            
            self.models['punctuation'] = punctuation_pipeline
            self.loading_status['punctuation'] = 'ready'
            logger.info("âœ… ë¬¸ì¥ ë¶€í˜¸ ë³µì› ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì¥ ë¶€í˜¸ ë³µì› ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.loading_status['punctuation'] = 'failed'
            raise
    
    def text_get_model(self, model_name: str):
        """ë¡œë”©ëœ ëª¨ë¸ ë°˜í™˜"""
        if model_name not in self.models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return self.models[model_name]
    
    def text_is_ready(self, model_name: Optional[str] = None) -> bool:
        """ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ í™•ì¸"""
        if model_name:
            return self.loading_status.get(model_name) == 'ready'
        return self.preload_complete
    
    def text_get_status(self) -> Dict[str, Any]:
        """ì „ì²´ ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        total_models = len(self.loading_status)
        ready_models = sum(1 for status in self.loading_status.values() if status == 'ready')
        
        return {
            "preload_complete": self.preload_complete,
            "ready_models": ready_models,
            "total_models": total_models,
            "device": self.device,
            "models_status": self.loading_status.copy(),
            "memory_usage": self._get_memory_usage()
        }
    
    def _get_memory_usage(self) -> Dict[str, Any]:
        """GPU/CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        memory_info = {"cpu_memory": "N/A", "gpu_memory": "N/A"}
        
        try:
            import psutil
            memory_info["cpu_memory"] = f"{psutil.virtual_memory().percent:.1f}%"
        except:
            pass
        
        if torch.cuda.is_available():
            try:
                gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_info["gpu_memory"] = f"{gpu_memory:.1f}GB / {gpu_total:.1f}GB"
            except:
                pass
        
        return memory_info
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        for model_name in list(self.models.keys()):
            del self.models[model_name]
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.models.clear()
        self.loading_status.clear()
        self.preload_complete = False
        
        logger.info("âœ… ëª¨ë¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
model_preloader = ModelPreloader() 