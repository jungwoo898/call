# Standard library imports
import os
import asyncio
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from contextlib import asynccontextmanager

# Related third-party imports
from omegaconf import OmegaConf

# NeMo ì•ˆì „ import
NeuralDiarizer = None
try:
    from nemo.collections.asr.models.msdd_models import NeuralDiarizer
    print("âœ… NeMo NeuralDiarizer imported successfully")
except ImportError as e:
    print(f"âš ï¸ NeMo import failed: {e}")
    print("ğŸ”„ Diarization will run in fallback mode")
except Exception as e:
    print(f"âš ï¸ NeMo error: {e}")
    print("ğŸ”„ Diarization will run in fallback mode")

# Local imports
from src.audio.utils import Formatter
from src.audio.metrics import SilenceStats
from src.audio.error import DialogueDetecting
from src.audio.alignment import ForcedAligner
from src.audio.effect import DemucsVocalSeparator
from src.audio.preprocessing import SpeechEnhancement
from src.audio.io import SpeakerTimestampReader, TranscriptWriter
from src.audio.analysis import WordSpeakerMapper, SentenceSpeakerMapper, Audio
from src.audio.processing import AudioProcessor, Transcriber, PunctuationRestorer
from src.text.utils import Annotator
from src.text.llm import LLMOrchestrator, LLMResultHandler
from src.utils.utils import Cleaner, Watcher
from src.db.manager import DatabaseManager
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer
from src.text.korean_models import KoreanModels

# FastAPI ì•± ìƒì„±
app = FastAPI(title="Callytics API", version="1.0.0")

# ì „ì—­ ë³€ìˆ˜ë¡œ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬
processing_status = {
    "is_processing": False,
    "current_file": None,
    "total_processed": 0,
    "errors": []
}

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    print("ğŸš€ Callytics API ì„œë²„ ì‹œì‘")
    yield
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    print("ğŸ›‘ Callytics API ì„œë²„ ì¢…ë£Œ")

app = FastAPI(title="Callytics API", version="1.0.0", lifespan=lifespan)

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ê¸°ë³¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        import torch
        cuda_available = torch.cuda.is_available()
        
        # API í‚¤ í™•ì¸
        api_keys = {
            "openai": bool(os.getenv("OPENAI_API_KEY")),
            "azure": bool(os.getenv("AZURE_OPENAI_API_KEY") and os.getenv("AZURE_OPENAI_ENDPOINT")),
            "huggingface": bool(os.getenv("HUGGINGFACE_API_TOKEN"))
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        db_path = "/app/Callytics_new.sqlite"
        db_accessible = os.path.exists(db_path)
        
        # ì˜¤ë””ì˜¤ ë””ë ‰í† ë¦¬ í™•ì¸
        audio_dir = "/app/audio"
        audio_accessible = os.path.exists(audio_dir)
        
        status = "healthy" if any(api_keys.values()) and db_accessible else "degraded"
        
        return JSONResponse({
            "status": status,
            "timestamp": asyncio.get_event_loop().time(),
            "cuda_available": cuda_available,
            "api_keys_configured": api_keys,
            "database_accessible": db_accessible,
            "audio_directory_accessible": audio_accessible,
            "processing_status": processing_status
        })
    except Exception as e:
        return JSONResponse({
            "status": "unhealthy",
            "error": str(e)
        }, status_code=500)

@app.get("/metrics")
async def get_metrics():
    """ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ (Prometheusìš©)"""
    try:
        import psutil
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # GPU ë©”íŠ¸ë¦­ (ê°€ëŠ¥í•œ ê²½ìš°)
        gpu_metrics = {}
        try:
            import torch
            if torch.cuda.is_available():
                gpu_metrics = {
                    "gpu_count": torch.cuda.device_count(),
                    "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**3,  # GB
                    "gpu_memory_reserved": torch.cuda.memory_reserved() / 1024**3,    # GB
                }
        except:
            pass
        
        return JSONResponse({
            "cpu_percent": cpu_percent,
            "memory_percent": memory.percent,
            "memory_available_gb": memory.available / 1024**3,
            "gpu_metrics": gpu_metrics,
            "processing_status": processing_status
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Callytics API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "metrics": "/metrics",
            "docs": "/docs"
        }
    }

async def main(audio_file_path: str):
    """
    Process an audio file to perform diarization, transcription, punctuation restoration,
    and speaker role classification.

    Parameters
    ----------
    audio_file_path : str
        The path to the input audio file to be processed.

    Returns
    -------
    None
    """
    # Paths
    config_nemo = "config/nemo/diar_infer_telephonic.yaml"
    manifest_path = "/app/.temp/manifest.json"
    temp_dir = "/app/.temp"
    rttm_file_path = os.path.join(temp_dir, "pred_rttms", "mono_file.rttm")
    transcript_output_path = "/app/.temp/output.txt"
    srt_output_path = "/app/.temp/output.srt"
    config_path = "config/config.yaml"
    prompt_path = "config/prompt.yaml"
    db_path = "/app/Callytics_new.sqlite"
    db_topic_fetch_path = "src/db/sql/TopicFetch.sql"
    db_topic_insert_path = "src/db/sql/TopicInsert.sql"
    db_audio_properties_insert_path = "src/db/sql/AudioPropertiesInsert.sql"
    db_utterance_insert_path = "src/db/sql/UtteranceInsert.sql"

    # Configuration
    config = OmegaConf.load(config_path)
    device = config.runtime.device
    compute_type = config.runtime.compute_type
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = config.runtime.cuda_alloc_conf

    # Initialize Classes
    dialogue_detector = DialogueDetecting(delete_original=True)
    enhancer = SpeechEnhancement(config_path=config_path, output_dir=temp_dir)
    separator = DemucsVocalSeparator()
    processor = AudioProcessor(audio_path=audio_file_path, temp_dir=temp_dir)
    transcriber = Transcriber(device=device, compute_type=compute_type)
    aligner = ForcedAligner(device=device)
    llm_handler = LLMOrchestrator(config_path=config_path, prompt_config_path=prompt_path, model_id="openai")
    llm_result_handler = LLMResultHandler()
    cleaner = Cleaner()
    formatter = Formatter()
    db = DatabaseManager(config_path)
    audio_feature_extractor = Audio(audio_file_path)

    # Step 1: Detect Dialogue
    has_dialogue = dialogue_detector.process(audio_file_path)
    if not has_dialogue:
        return

    # Step 2: Speech Enhancement
    audio_path = enhancer.enhance_audio(
        input_path=audio_file_path,
        output_path=os.path.join(temp_dir, "enhanced.wav"),
        noise_threshold=0.0001,
        verbose=True
    )

    # Step 3: Vocal Separation
    vocal_path = separator.separate_vocals(audio_file=audio_path, output_dir=temp_dir)

    # Step 4: Transcription
    transcript, info = transcriber.transcribe(audio_path=vocal_path)
    detected_language = info["language"]

    # Step 5: Forced Alignment
    word_timestamps = aligner.align(
        audio_path=vocal_path,
        transcript=transcript,
        language=detected_language
    )

    # Step 6: Diarization
    processor.audio_path = vocal_path
    mono_audio_path = processor.convert_to_mono()
    processor.audio_path = mono_audio_path
    processor.create_manifest(manifest_path)
    
    # NeuralDiarizerë¥¼ ì‚¬ìš©í•œ í™”ì ë¶„ë¦¬ (fallback ëª¨ë“œ ì§€ì›)
    if NeuralDiarizer is None:
        print("Warning: NeuralDiarizer is not available. Using fallback diarization.")
        # ë”ë¯¸ RTTM íŒŒì¼ ìƒì„± (ë‹¨ì¼ í™”ìë¡œ ê°€ì •)
        os.makedirs(os.path.join(temp_dir, "pred_rttms"), exist_ok=True)
        with open(rttm_file_path, 'w') as f:
            f.write(f"SPEAKER mono_file 1 0.0 {processor.duration} <NA> <NA> SPEAKER_00 <NA> <NA>\n")
        print(f"Created fallback RTTM file: {rttm_file_path}")
    else:
        try:
            cfg = OmegaConf.load(config_nemo)
            cfg.diarizer.manifest_filepath = manifest_path
            cfg.diarizer.out_dir = temp_dir
            msdd_model = NeuralDiarizer(cfg=cfg)
            msdd_model.diarize()
        except Exception as e:
            print(f"Warning: NeuralDiarizer failed: {e}")
            # ë”ë¯¸ RTTM íŒŒì¼ ìƒì„±
            os.makedirs(os.path.join(temp_dir, "pred_rttms"), exist_ok=True)
            with open(rttm_file_path, 'w') as f:
                f.write(f"SPEAKER mono_file 1 0.0 {processor.duration} <NA> <NA> SPEAKER_00 <NA> <NA>\n")
            print(f"Created fallback RTTM file: {rttm_file_path}")

    # Step 7: Processing Transcript
    # Step 7.1: Speaker Timestamps
    speaker_reader = SpeakerTimestampReader(rttm_path=rttm_file_path)
    speaker_ts = speaker_reader.read_speaker_timestamps()

    # Step 7.2: Mapping Words
    word_speaker_mapper = WordSpeakerMapper(word_timestamps, speaker_ts)
    wsm = word_speaker_mapper.get_words_speaker_mapping()

    # Step 7.3: Punctuation Restoration
    punct_restorer = PunctuationRestorer(language=detected_language)
    wsm = punct_restorer.restore_punctuation(wsm)
    word_speaker_mapper.word_speaker_mapping = wsm
    word_speaker_mapper.realign_with_punctuation()
    wsm = word_speaker_mapper.word_speaker_mapping

    # Step 7.4: Mapping Sentences
    sentence_mapper = SentenceSpeakerMapper()
    ssm = sentence_mapper.get_sentences_speaker_mapping(wsm)

    # Step 7.5: ì–¸ì–´ ê°ì§€ ë° ê²€ì¦
    # API ê¸°ë°˜ í•œêµ­ì–´ ëª¨ë¸ ì´ˆê¸°í™”
    korean_models = KoreanModels(device=device)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ì–¸ì–´ ê°ì§€
    full_text = " ".join([utterance["text"] for utterance in ssm])
    
    if not korean_models.is_korean_content(full_text):
        print("âš ï¸ ê²½ê³ : í•œêµ­ì–´ê°€ ì•„ë‹Œ ì˜¤ë””ì˜¤ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“ ê°ì§€ëœ ì–¸ì–´ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ í•œêµ­ì–´ ì˜¤ë””ì˜¤ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        # í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš°ì—ë„ ê³„ì† ì§„í–‰í•˜ë˜ ê²½ê³  í‘œì‹œ
        language_warning = True
    else:
        print("âœ… í•œêµ­ì–´ ì˜¤ë””ì˜¤ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        language_warning = False

    # Step 8 (Optional): Write Transcript and SRT Files
    writer = TranscriptWriter()
    writer.write_transcript(ssm, transcript_output_path)
    writer.write_srt(ssm, srt_output_path)

    # Step 9: Classify Speaker Roles
    speaker_roles = await llm_handler.generate("Classification", ssm)

    # Step 9.1: LLM results validate and fallback
    ssm = llm_result_handler.validate_and_fallback(speaker_roles, ssm)
    llm_result_handler.log_result(ssm, speaker_roles)

    # Step 10: Sentiment Analysis (API ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬)
    ssm_with_indices = formatter.add_indices_to_ssm(ssm)
    annotator = Annotator(ssm_with_indices)
    
    # APIë¥¼ í†µí•´ ë°°ì¹˜ë¡œ ê°ì • ë¶„ì„ ìˆ˜í–‰
    texts_for_sentiment = [utterance["text"] for utterance in ssm]
    sentiment_results = await korean_models.analyze_sentiment_batch(texts_for_sentiment)
    
    # ê²°ê³¼ë¥¼ SSMì— ì ìš©
    for i, utterance in enumerate(ssm):
        utterance["sentiment"] = sentiment_results[i]
    
    # Step 11: Profanity Word Detection (API ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬)
    profanity_results = await korean_models.detect_profanity_batch(texts_for_sentiment)
    
    # ê²°ê³¼ë¥¼ SSMì— ì ìš©
    for i, utterance in enumerate(ssm):
        utterance["profane"] = profanity_results[i]

    # Step 12: Summary
    summary_result = await llm_handler.generate("Summary", user_input=ssm)
    annotator.add_summary(summary_result)

    # Step 13: Conflict Detection
    conflict_result = await llm_handler.generate("ConflictDetection", user_input=ssm)
    annotator.add_conflict(conflict_result)

    # Step 14: Topic Detection
    topics = db.fetch(db_topic_fetch_path)
    topic_result = await llm_handler.generate(
        "TopicDetection",
        user_input=ssm,
        system_input=topics
    )
    annotator.add_topic(topic_result)

    # Step 15: Complaint Analysis
    complaint_result = await llm_handler.generate("ComplaintAnalysis", user_input=ssm)
    annotator.add_complaint(complaint_result)

    # Step 16: Action Items
    action_result = await llm_handler.generate("ActionItems", user_input=ssm)
    annotator.add_action_items(action_result)

    # Step 17: Quality Assessment
    quality_result = await llm_handler.generate("QualityAssessment", user_input=ssm)
    annotator.add_quality_assessment(quality_result)

    # Step 18: Extract Audio Properties
    audio_properties = audio_feature_extractor.extract_properties()

    # Step 19: Database Operations
    # Step 19.1: Insert Audio Properties (ê¸°ì¡´ SQL íŒŒì¼ ì‚¬ìš©)
    try:
        # ê¸°ì¡´ AudioPropertiesInsert.sqlì— ë§ëŠ” ë°ì´í„° êµ¬ì¡°ë¡œ ë³€í™˜
        (
            name,
            extension,
            path,
            sample_rate,
            min_freq,
            max_freq,
            bit_depth,
            channels,
            duration,
            rms_loudness,
            final_features
        ) = audio_feature_extractor.properties()
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        summary = "ì˜¤ë””ì˜¤ ë¶„ì„ ì™„ë£Œ"
        conflict_flag = 0
        silence_value = 0.0
        detected_topic = "ì¼ë°˜"
        
        # Topic ID ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ 1 ì‚¬ìš©)
        topic_id = 1
        
        # AudioPropertiesInsert.sqlì— ë§ëŠ” íŒŒë¼ë¯¸í„° êµ¬ì„±
        params = (
            path,           # file_path
            name,           # file_name
            duration,       # duration
            sample_rate,    # sample_rate
            channels,       # channels
            topic_id,       # topic_id
            extension,      # extension
            min_freq,       # min_frequency
            max_freq,       # max_frequency
            bit_depth,      # bit_depth
            rms_loudness,   # rms_loudness
            final_features.get("ZeroCrossingRate", 0.0),  # zero_crossing_rate
            final_features.get("SpectralCentroid", 0.0),  # spectral_centroid
            final_features.get("EQ_20_250_Hz", 0.0),      # eq_20_250_hz
            final_features.get("EQ_250_2000_Hz", 0.0),    # eq_250_2000_hz
            final_features.get("EQ_2000_6000_Hz", 0.0),   # eq_2000_6000_hz
            final_features.get("EQ_6000_20000_Hz", 0.0),  # eq_6000_20000_hz
            final_features.get("MFCC_1", 0.0),            # mfcc_1
            final_features.get("MFCC_2", 0.0),            # mfcc_2
            final_features.get("MFCC_3", 0.0),            # mfcc_3
            final_features.get("MFCC_4", 0.0),            # mfcc_4
            final_features.get("MFCC_5", 0.0),            # mfcc_5
            final_features.get("MFCC_6", 0.0),            # mfcc_6
            final_features.get("MFCC_7", 0.0),            # mfcc_7
            final_features.get("MFCC_8", 0.0),            # mfcc_8
            final_features.get("MFCC_9", 0.0),            # mfcc_9
            final_features.get("MFCC_10", 0.0),           # mfcc_10
            final_features.get("MFCC_11", 0.0),           # mfcc_11
            final_features.get("MFCC_12", 0.0),           # mfcc_12
            final_features.get("MFCC_13", 0.0),           # mfcc_13
            summary,        # summary
            conflict_flag,  # conflict_flag
            silence_value   # silence_value
        )
        
        last_id = db.insert(db_audio_properties_insert_path, params)
        print(f"âœ… ì˜¤ë””ì˜¤ ì†ì„± DB ì €ì¥ ì™„ë£Œ: ID {last_id}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ì†ì„± DB ì €ì¥ ì‹¤íŒ¨: {e}")
        last_id = None

    # Step 19.2: Insert Utterances (ê¸°ì¡´ SQL íŒŒì¼ ì‚¬ìš©)
    if last_id is not None:
        try:
            for i, utterance in enumerate(ssm):
                # UtteranceInsert.sqlì— ë§ëŠ” íŒŒë¼ë¯¸í„° êµ¬ì„±
                utterance_params = (
                    last_id,  # audio_properties_id
                    utterance["speaker"],  # speaker
                    utterance["start"] / 1000.0,  # start_time (ì´ˆ ë‹¨ìœ„)
                    utterance["end"] / 1000.0,  # end_time (ì´ˆ ë‹¨ìœ„)
                    utterance["text"],  # text
                    1.0,  # confidence (ê¸°ë³¸ê°’)
                    i + 1,  # sequence
                    utterance.get("sentiment", "ì¤‘ë¦½"),  # sentiment
                    1 if utterance.get("profane", False) else 0  # profane (0/1)
                )
                db.insert(db_utterance_insert_path, utterance_params)
            
            print(f"âœ… ë°œí™” ë°ì´í„° DB ì €ì¥ ì™„ë£Œ: {len(ssm)}ê°œ ë°œí™”")
            
        except Exception as e:
            print(f"âŒ ë°œí™” ë°ì´í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ File IDê°€ ì—†ì–´ ë°œí™” ë°ì´í„° ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    # Step 20: Cleanup
    cleaner.cleanup_temp_files(temp_dir)

    print(f"âœ… ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {audio_file_path}")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°œí™” ìˆ˜: {len(ssm)}")
    print(f"ğŸ—£ï¸ ê°ì§€ëœ í™”ì ìˆ˜: {len(set(utterance['speaker'] for utterance in ssm))}")
    print(f"ğŸ“ ê°ì§€ëœ ì–¸ì–´: {detected_language}")
    if language_warning:
        print("âš ï¸ ì–¸ì–´ ê²½ê³ : í•œêµ­ì–´ê°€ ì•„ë‹Œ ì½˜í…ì¸ ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")


async def process(path: str):
    """
    Process a single audio file asynchronously.

    Parameters
    ----------
    path : str
        Path to the audio file to process.

    Returns
    -------
    None
    """
    try:
        # ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        processing_status["is_processing"] = True
        processing_status["current_file"] = path
        processing_status["errors"] = []
        
        print(f"ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {path}")
        await main(path)
        print(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {path}")
        
        # ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        processing_status["total_processed"] += 1
        processing_status["current_file"] = None
        
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {path}")
        print(f"ì˜¤ë¥˜: {e}")
        
        # ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        processing_status["errors"].append({
            "file": path,
            "error": str(e),
            "timestamp": asyncio.get_event_loop().time()
        })
        
    finally:
        # ì²˜ë¦¬ ì™„ë£Œ
        processing_status["is_processing"] = False


def watch_and_process():
    """
    Watch for new audio files and process them automatically.
    """
    print("ğŸ” ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì‹œ ì‹œì‘...")
    
    # ê°ì‹œí•  ë””ë ‰í† ë¦¬ ì„¤ì •
    watch_directories = [
        "/app/audio",
        "/app/temp"
    ]
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜
    class FileHandler(FileSystemEventHandler):
        def __init__(self, callback):
            self.callback = callback
            self.processing_files = set()
        
        def on_created(self, event):
            if not event.is_directory:
                self._handle_file(event.src_path)
        
        def on_moved(self, event):
            if not event.is_directory:
                self._handle_file(event.dest_path)
        
        def _handle_file(self, file_path):
            # ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ì¸ì§€ í™•ì¸
            if file_path in self.processing_files:
                return
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¥ì í™•ì¸
            audio_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg'}
            if any(file_path.lower().endswith(ext) for ext in audio_extensions):
                print(f"ğŸµ ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€: {file_path}")
                self.processing_files.add(file_path)
                
                # ë¹„ë™ê¸°ë¡œ íŒŒì¼ ì²˜ë¦¬
                asyncio.create_task(self._process_file(file_path))
        
        async def _process_file(self, file_path):
            try:
                await self.callback(file_path)
            finally:
                # ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ëª©ë¡ì—ì„œ ì œê±°
                self.processing_files.discard(file_path)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ìƒì„±
    handler = FileHandler(process)
    
    # Observer ì„¤ì • ë° ì‹œì‘
    observer = Observer()
    for directory in watch_directories:
        if os.path.exists(directory):
            observer.schedule(handler, directory, recursive=False)
            print(f"ğŸ“ ë””ë ‰í† ë¦¬ ê°ì‹œ ì‹œì‘: {directory}")
        else:
            print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
    
    observer.start()
    
    try:
        print("ğŸ”„ íŒŒì¼ ê°ì‹œ ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")
        while True:
            import time
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ íŒŒì¼ ê°ì‹œ ì¤‘ì§€...")
        observer.stop()
    
    observer.join()
    print("âœ… íŒŒì¼ ê°ì‹œ ì¢…ë£Œ")


if __name__ == "__main__":
    import sys
    import threading
    
    if len(sys.argv) > 1:
        # íŒŒì¼ ê²½ë¡œê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ íŒŒì¼ë§Œ ì²˜ë¦¬
        file_path = sys.argv[1]
        asyncio.run(process(file_path))
    else:
        # FastAPI ì„œë²„ì™€ íŒŒì¼ ê°ì‹œë¥¼ ë™ì‹œì— ì‹¤í–‰
        def run_file_watcher():
            """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ íŒŒì¼ ê°ì‹œ ì‹¤í–‰"""
            watch_and_process()
        
        # íŒŒì¼ ê°ì‹œ ìŠ¤ë ˆë“œ ì‹œì‘
        watcher_thread = threading.Thread(target=run_file_watcher, daemon=True)
        watcher_thread.start()
        
        # FastAPI ì„œë²„ ì‹œì‘
        print("ğŸš€ Callytics API ì„œë²„ ì‹œì‘...")
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            log_level="info",
            access_log=True
        )