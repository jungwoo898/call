# Standard library imports
import os
import json
import asyncio
import time
import threading
from typing import Dict, List, Any, Optional
from pathlib import Path

# Local imports
from src.audio.preprocessing import AudioPreprocessor
from src.audio.error import AdvancedDialogueDetecting
from src.audio.effect import AdvancedDemucsVocalSeparator
from src.audio.advanced_processing import AdvancedTranscriber
from src.audio.advanced_punctuation import AdvancedPunctuationRestorer
from src.text.advanced_analysis import AdvancedAnalysisManager, analyze_communication_quality_with_trend
from src.db.advanced_manager import AdvancedDatabaseManager


class AdvancedIntegratedAnalyzer:
    """
    ê³ ì„±ëŠ¥ í†µí•© ë¶„ì„ê¸°
    ëª¨ë“  ê°œì„ ì‚¬í•­ì„ í†µí•©í•œ ìµœì¢… íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, 
                 config_path: str = "config/config.yaml",
                 enable_cache: bool = True,
                 enable_parallel: bool = True,
                 enable_async: bool = True,
                 max_workers: int = 4):
        """
        AdvancedIntegratedAnalyzer ì´ˆê¸°í™”
        
        Parameters
        ----------
        config_path : str
            ì„¤ì • íŒŒì¼ ê²½ë¡œ
        enable_cache : bool
            ìºì‹œ í™œì„±í™” ì—¬ë¶€
        enable_parallel : bool
            ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        enable_async : bool
            ë¹„ë™ê¸° ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        max_workers : int
            ìµœëŒ€ ì›Œì»¤ ìˆ˜
        """
        self.config_path = config_path
        self.enable_cache = enable_cache
        self.enable_parallel = enable_parallel
        self.enable_async = enable_async
        self.max_workers = max_workers
        
        # ê³ ì„±ëŠ¥ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.audio_preprocessor = AudioPreprocessor(
            max_workers=max_workers,
            cache_dir="/app/.cache/audio"
        )
        
        self.dialogue_detector = AdvancedDialogueDetecting(
            max_workers=max_workers,
            enable_parallel=enable_parallel
        )
        
        self.vocal_separator = AdvancedDemucsVocalSeparator(
            enable_cache=enable_cache,
            voice_detection_threshold=0.3
        )
        
        self.transcriber = AdvancedTranscriber(
            cache_dir="/app/.cache/stt",
            max_workers=max_workers,
            enable_cache=enable_cache,
            retry_attempts=3
        )
        
        self.punctuation_restorer = AdvancedPunctuationRestorer(
            cache_dir="/app/.cache/punctuation",
            enable_cache=enable_cache,
            max_batch_size=100
        )
        
        self.analysis_manager = AdvancedAnalysisManager(
            config_path=config_path,
            cache_dir="/app/.cache/analysis",
            max_workers=max_workers,
            enable_cache=enable_cache,
            enable_parallel=enable_parallel
        )
        
        self.db_manager = AdvancedDatabaseManager(
            config_path=config_path,
            max_workers=max_workers,
            batch_size=100,
            enable_async=enable_async,
            enable_recovery=True
        )
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "avg_processing_time": 0.0,
            "stage_times": {
                "preprocessing": 0.0,
                "dialogue_detection": 0.0,
                "vocal_separation": 0.0,
                "transcription": 0.0,
                "punctuation": 0.0,
                "analysis": 0.0,
                "database": 0.0
            }
        }
        
        print(f"âœ… AdvancedIntegratedAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def analyze_audio_comprehensive(self, audio_file: str) -> Dict[str, Any]:
        """
        ì¢…í•© ì˜¤ë””ì˜¤ ë¶„ì„
        
        Parameters
        ----------
        audio_file : str
            ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns
        -------
        Dict[str, Any]
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        try:
            print(f"ğŸ¯ ì¢…í•© ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œì‘: {audio_file}")
            overall_start_time = time.time()
            
            # 1. ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ (ffmpeg ë³‘ë ¬ ì²˜ë¦¬, ì„ì‹œíŒŒì¼ ìë™ ì •ë¦¬)
            print("ğŸ“Š 1ë‹¨ê³„: ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬")
            stage_start = time.time()
            
            preprocessed_files = await self.audio_preprocessor.normalize_audio_parallel(
                [audio_file], "/app/temp/preprocessed"
            )
            
            if not preprocessed_files or not preprocessed_files[0]:
                raise Exception("ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì‹¤íŒ¨")
            
            preprocessed_file = preprocessed_files[0]
            self.performance_stats["stage_times"]["preprocessing"] = time.time() - stage_start
            
            # 2. í™”ì ë¶„ë¦¬ (ê¸´ ì˜¤ë””ì˜¤ chunk ë¶„í• , ë³‘ë ¬ diarization, graceful degradation)
            print("ğŸ¤ 2ë‹¨ê³„: í™”ì ë¶„ë¦¬")
            stage_start = time.time()
            
            dialogue_result = self.dialogue_detector.process(preprocessed_file)
            speakers = dialogue_result.get("speakers", set())
            segments = dialogue_result.get("segments", [])
            
            self.performance_stats["stage_times"]["dialogue_detection"] = time.time() - stage_start
            
            # 3. ë³´ì»¬ ë¶„ë¦¬ (ë¶„ë¦¬ ê²°ê³¼ ìºì‹±, ë¶ˆí•„ìš”í•œ ë¶„ë¦¬ ìƒëµ)
            print("ğŸµ 3ë‹¨ê³„: ë³´ì»¬ ë¶„ë¦¬")
            stage_start = time.time()
            
            vocal_file = self.vocal_separator.separate_vocals_advanced(
                preprocessed_file, "/app/temp/vocals"
            )
            
            self.performance_stats["stage_times"]["vocal_separation"] = time.time() - stage_start
            
            # 4. STT ì²˜ë¦¬ (STT ê²°ê³¼ ìºì‹±, ì‹¤íŒ¨ êµ¬ê°„ ì¬ì‹œë„, ê¸´ ì˜¤ë””ì˜¤ ë¶„í•  ìµœì í™”)
            print("ğŸ¤ 4ë‹¨ê³„: ìŒì„± ì¸ì‹")
            stage_start = time.time()
            
            stt_result = self.transcriber.transcribe_advanced(vocal_file)
            transcription_segments = stt_result.get("segments", [])
            
            self.performance_stats["stage_times"]["transcription"] = time.time() - stage_start
            
            # 5. ë¬¸ì¥ ë¶€í˜¸ ë³µì› (batch í¬ê¸° ìë™ ì¡°ì •, fallback)
            print("ğŸ”¤ 5ë‹¨ê³„: ë¬¸ì¥ ë¶€í˜¸ ë³µì›")
            stage_start = time.time()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = []
            for segment in transcription_segments:
                if segment.get("text"):
                    texts.append(segment["text"])
            
            if texts:
                restored_texts = self.punctuation_restorer.restore_punctuation_advanced(texts)
                
                # ë³µì›ëœ í…ìŠ¤íŠ¸ë¥¼ ì„¸ê·¸ë¨¼íŠ¸ì— ì ìš©
                text_index = 0
                for segment in transcription_segments:
                    if segment.get("text") and text_index < len(restored_texts):
                        segment["text"] = restored_texts[text_index]
                        text_index += 1
            
            self.performance_stats["stage_times"]["punctuation"] = time.time() - stage_start
            
            # 6. í…ìŠ¤íŠ¸ ë¶„ì„ (ë¶„ì„ ê²°ê³¼ ìºì‹±, LLM í˜¸ì¶œ ë³‘ë ¬í™”, í’ˆì§ˆì§€í‘œ ì„¸ë¶„í™”)
            print("ğŸ” 6ë‹¨ê³„: í…ìŠ¤íŠ¸ ë¶„ì„")
            stage_start = time.time()
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
            full_text = " ".join([segment.get("text", "") for segment in transcription_segments])
            
            # ë°œí™” ë°ì´í„° êµ¬ì„± (ê°ì • ì¶”ì„¸ ë¶„ì„ìš©)
            utterances_data = []
            for segment in transcription_segments:
                if segment.get("text"):
                    # í™”ì ì‹ë³„ (ê°„ë‹¨í•œ ê·œì¹™)
                    speaker = "ê³ ê°"  # ê¸°ë³¸ê°’
                    if any(keyword in segment.get("text", "").lower() for keyword in ["ì£„ì†¡í•©ë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤", "ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤"]):
                        speaker = "ìƒë‹´ì‚¬"
                    
                    utterances_data.append({
                        "speaker": speaker,
                        "text": segment.get("text", ""),
                        "start_time": segment.get("start", 0),
                        "end_time": segment.get("end", 0),
                        "sentiment": "ì¤‘ë¦½"  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ê°ì • ë¶„ì„ ê²°ê³¼ ì‚¬ìš©
                    })
            
            if full_text.strip():
                # ê¸°ì¡´ í…ìŠ¤íŠ¸ ë¶„ì„
                analysis_result = await self.analysis_manager.analyze_text_comprehensive(full_text)
                
                # ê°ì • ì¶”ì„¸ ë¶„ì„ ì¶”ê°€
                trend_analysis = analyze_communication_quality_with_trend(utterances_data)
                
                # ê²°ê³¼ ë³‘í•©
                analysis_result.update(trend_analysis)
            else:
                analysis_result = {
                    "sentiment_analysis": {"sentiment": "ì¤‘ë¦½", "confidence": 0.5},
                    "profanity_detection": {"has_profanity": False, "confidence": 0.5},
                    "speaker_classification": {"speaker_type": "ê³ ê°", "confidence": 0.5},
                    "communication_quality": {
                        "overall_score": 0.5,
                        "category_scores": {
                            "politeness": 0.5,
                            "negative_expression": 0.5,
                            "empathy": 0.5,
                            "expertise": 0.5,
                            "specific_info": 0.5,
                            "punctuation": 0.5,
                            "sentiment": 0.5
                        },
                        "detailed_analysis": {
                            "politeness": {"score": 0.5, "details": {}, "examples": []},
                            "negative_expression": {"score": 0.5, "details": {}, "examples": []},
                            "empathy": {"score": 0.5, "details": {}, "examples": []},
                            "expertise": {"score": 0.5, "details": {}, "examples": []},
                            "specific_info": {"score": 0.5, "details": {}, "examples": []},
                            "punctuation": {"score": 0.5, "details": {}, "examples": []},
                            "sentiment": {
                                "score": 0.5, 
                                "details": {
                                    "positive_ratio": 0,
                                    "negative_ratio": 0,
                                    "positive_intensity": 0,
                                    "negative_intensity": 0
                                }, 
                                "examples": []
                            }
                        },
                        "recommendations": ["í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
                    },
                    "customer_sentiment_early": None,
                    "customer_sentiment_late": None,
                    "customer_sentiment_trend": None
                }
            
            self.performance_stats["stage_times"]["analysis"] = time.time() - stage_start
            
            # 7. DB ì €ì¥ (bulk insert, ë¹„ë™ê¸° ì €ì¥, ì¥ì•  ë³µêµ¬ ë¡œì§)
            print("ğŸ’¾ 7ë‹¨ê³„: DB ì €ì¥")
            stage_start = time.time()
            
            # ì˜¤ë””ì˜¤ ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            audio_analysis_data = {
                "file_path": audio_file,
                "duration": stt_result.get("end_time", 0),
                "sample_rate": 16000,
                "channels": 1,
                "transcription": full_text,
                "language": stt_result.get("language", "ko"),
                "confidence_score": stt_result.get("language_probability", 0.0)
            }
            
            # í’ˆì§ˆ ë¶„ì„ ë°ì´í„° ì¤€ë¹„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)
            communication_quality = analysis_result.get("communication_quality", {})
            category_scores = communication_quality.get("category_scores", {})
            detailed_analysis = communication_quality.get("detailed_analysis", {})
            
            # KNU ê°ì„± ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            sentiment_analysis = detailed_analysis.get("sentiment", {})
            sentiment_details = sentiment_analysis.get("details", {})
            
            # ìƒˆë¡œ ì¶”ê°€í•œ ì»¬ëŸ¼ë“¤ ì¶”ì¶œ
            honorific_ratio = analysis_result.get("honorific_ratio", 0.0)
            positive_word_ratio = analysis_result.get("positive_word_ratio", 0.0)
            negative_word_ratio = analysis_result.get("negative_word_ratio", 0.0)
            euphonious_word_ratio = analysis_result.get("euphonious_word_ratio", 0.0)
            empathy_ratio = analysis_result.get("empathy_ratio", 0.0)
            apology_ratio = analysis_result.get("apology_ratio", 0.0)
            avg_response_latency = analysis_result.get("avg_response_latency", 0.0)
            interruption_count = analysis_result.get("interruption_count", 0)
            silence_ratio = analysis_result.get("silence_ratio", 0.0)
            talk_ratio = analysis_result.get("talk_ratio", 0.0)
            
            quality_analysis_data = {
                "audio_analysis_id": 0,  # ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì˜¨ ID
                "clarity_score": category_scores.get("expertise", 0.5) * 0.6 + category_scores.get("specific_info", 0.5) * 0.4,
                "politeness_score": category_scores.get("politeness", 0.5) * 0.7 + category_scores.get("negative_expression", 0.5) * 0.3,
                "empathy_score": category_scores.get("empathy", 0.5),
                "professionalism_score": category_scores.get("expertise", 0.5),
                "response_quality_score": communication_quality.get("overall_score", 0.5),
                "overall_score": communication_quality.get("overall_score", 0.5),
                "sentiment_analysis": analysis_result.get("sentiment_analysis", {}),
                "profanity_detected": analysis_result.get("profanity_detection", {}).get("has_profanity", False),
                "speaker_classification": analysis_result.get("speaker_classification", {}).get("speaker_type", "ê³ ê°"),
                "detailed_quality_analysis": detailed_analysis,
                "recommendations": communication_quality.get("recommendations", []),
                # KNU ê°ì„± ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                "knu_sentiment_score": category_scores.get("sentiment", 0.5),
                "positive_word_ratio": sentiment_details.get("positive_ratio", 0),
                "negative_word_ratio": sentiment_details.get("negative_ratio", 0),
                "positive_intensity": sentiment_details.get("positive_intensity", 0),
                "negative_intensity": sentiment_details.get("negative_intensity", 0),
                "sentiment_examples": sentiment_analysis.get("examples", []),
                # ìƒˆë¡œ ì¶”ê°€í•œ ì»¬ëŸ¼ë“¤
                "honorific_ratio": honorific_ratio,
                "euphonious_word_ratio": euphonious_word_ratio,
                "empathy_ratio": empathy_ratio,
                "apology_ratio": apology_ratio,
                "avg_response_latency": avg_response_latency,
                "interruption_count": interruption_count,
                "silence_ratio": silence_ratio,
                "talk_ratio": talk_ratio
            }
            
            # LLM ë¶„ì„ ë°ì´í„° ì¤€ë¹„
            llm_analysis_data = {
                "audio_analysis_id": 0,  # ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì˜¨ ID
                "analysis_type": "comprehensive",
                "analysis_result": analysis_result,
                "confidence_score": 0.8,
                "model_used": "advanced_analysis_manager",
                "processing_time": self.performance_stats["stage_times"]["analysis"]
            }
            
            # ë¹„ë™ê¸° ì €ì¥
            if self.enable_async:
                await asyncio.gather(
                    self.db_manager.save_audio_analysis_async(audio_analysis_data),
                    self.db_manager.save_quality_analysis_async(quality_analysis_data),
                    self.db_manager.save_llm_analysis_async(llm_analysis_data)
                )
            else:
                # ì¦‰ì‹œ ì €ì¥
                self.db_manager.bulk_save_audio_analysis([audio_analysis_data])
                self.db_manager.bulk_save_quality_analysis([quality_analysis_data])
                self.db_manager.bulk_save_llm_analysis([llm_analysis_data])
            
            self.performance_stats["stage_times"]["database"] = time.time() - stage_start
            
            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            overall_processing_time = time.time() - overall_start_time
            
            final_result = {
                "audio_file": audio_file,
                "processing_status": "success",
                "processing_time": overall_processing_time,
                "speakers_detected": len(speakers),
                "speaker_segments": segments,
                "transcription": {
                    "full_text": full_text,
                    "segments": transcription_segments,
                    "language": stt_result.get("language", "ko"),
                    "confidence": stt_result.get("language_probability", 0.0)
                },
                "analysis": analysis_result,
                "quality_metrics": {
                    "clarity": category_scores.get("expertise", 0.5) * 0.6 + category_scores.get("specific_info", 0.5) * 0.4,
                    "politeness": category_scores.get("politeness", 0.5) * 0.7 + category_scores.get("negative_expression", 0.5) * 0.3,
                    "empathy": category_scores.get("empathy", 0.5),
                    "professionalism": category_scores.get("expertise", 0.5),
                    "response_quality": communication_quality.get("overall_score", 0.5),
                    "overall_score": communication_quality.get("overall_score", 0.5),
                    "detailed_analysis": detailed_analysis,
                    "recommendations": communication_quality.get("recommendations", []),
                    # KNU ê°ì„± ë¶„ì„ ê²°ê³¼
                    "knu_sentiment": {
                        "score": category_scores.get("sentiment", 0.5),
                        "positive_ratio": sentiment_details.get("positive_ratio", 0),
                        "negative_ratio": sentiment_details.get("negative_ratio", 0),
                        "positive_intensity": sentiment_details.get("positive_intensity", 0),
                        "negative_intensity": sentiment_details.get("negative_intensity", 0),
                        "examples": sentiment_analysis.get("examples", [])
                    },
                    # ê³ ê° ê°ì • ì¶”ì„¸ ë¶„ì„ ê²°ê³¼ (50% êµ¬ë¶„ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ)
                    "customer_sentiment_trend": {
                        "early_sentiment": analysis_result.get("customer_sentiment_early"),
                        "late_sentiment": analysis_result.get("customer_sentiment_late"),
                        "trend": analysis_result.get("customer_sentiment_trend"),
                        "trend_description": "ê°œì„ ë¨" if analysis_result.get("customer_sentiment_trend", 0) > 0 else "ì•…í™”ë¨" if analysis_result.get("customer_sentiment_trend", 0) < 0 else "ë³€í™”ì—†ìŒ",
                        "analysis_method": "50% êµ¬ë¶„ (ì•ˆì •ì„± í–¥ìƒ)"
                    }
                },
                "performance_stats": {
                    "stage_times": self.performance_stats["stage_times"].copy(),
                    "cache_hits": {
                        "audio_preprocessing": 0,  # ì‹¤ì œë¡œëŠ” ê° ì»´í¬ë„ŒíŠ¸ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
                        "vocal_separation": 0,
                        "stt_processing": 0,
                        "punctuation_restoration": 0,
                        "text_analysis": 0
                    }
                }
            }
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats["total_analyses"] += 1
            self.performance_stats["successful_analyses"] += 1
            self.performance_stats["avg_processing_time"] = (
                (self.performance_stats["avg_processing_time"] * (self.performance_stats["total_analyses"] - 1) + overall_processing_time) 
                / self.performance_stats["total_analyses"]
            )
            
            print(f"âœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ: {overall_processing_time:.2f}ì´ˆ")
            print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {final_result['quality_metrics']['overall_score']:.2f}")
            
            return final_result
            
        except Exception as e:
            print(f"âš ï¸ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats["total_analyses"] += 1
            self.performance_stats["failed_analyses"] += 1
            
            return {
                "audio_file": audio_file,
                "processing_status": "failed",
                "error": str(e),
                "processing_time": time.time() - overall_start_time if 'overall_start_time' in locals() else 0
            }
    
    async def analyze_batch_parallel(self, audio_files: List[str]) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ë³‘ë ¬ ë¶„ì„
        
        Parameters
        ----------
        audio_files : List[str]
            ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            
        Returns
        -------
        List[Dict[str, Any]]
            ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸš€ ë°°ì¹˜ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {len(audio_files)}ê°œ íŒŒì¼")
            start_time = time.time()
            
            # ë³‘ë ¬ íƒœìŠ¤í¬ ìƒì„±
            tasks = [self.analyze_audio_comprehensive(audio_file) for audio_file in audio_files]
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì •ë¦¬
            final_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"âš ï¸ íŒŒì¼ {i} ë¶„ì„ ì‹¤íŒ¨: {result}")
                    final_results.append({
                        "audio_file": audio_files[i],
                        "processing_status": "failed",
                        "error": str(result),
                        "processing_time": 0
                    })
                else:
                    final_results.append(result)
            
            processing_time = time.time() - start_time
            print(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            
            return final_results
            
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return [{"error": str(e)} for _ in audio_files]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.performance_stats.copy()
    
    def get_component_stats(self) -> Dict[str, Any]:
        """ê° ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            "audio_preprocessor": self.audio_preprocessor.performance_stats if hasattr(self.audio_preprocessor, 'performance_stats') else {},
            "dialogue_detector": {},  # AdvancedDialogueDetectingì—ëŠ” ì„±ëŠ¥ í†µê³„ê°€ ì—†ìŒ
            "vocal_separator": {},    # AdvancedDemucsVocalSeparatorì—ëŠ” ì„±ëŠ¥ í†µê³„ê°€ ì—†ìŒ
            "transcriber": self.transcriber.performance_stats if hasattr(self.transcriber, 'performance_stats') else {},
            "punctuation_restorer": self.punctuation_restorer.get_performance_stats(),
            "analysis_manager": self.analysis_manager.get_performance_stats(),
            "db_manager": self.db_manager.get_performance_stats()
        }
    
    def cleanup_all_caches(self, max_age_hours: int = 24):
        """ëª¨ë“  ìºì‹œ ì •ë¦¬"""
        try:
            print("ğŸ§¹ ëª¨ë“  ìºì‹œ ì •ë¦¬ ì‹œì‘")
            
            # ê° ì»´í¬ë„ŒíŠ¸ì˜ ìºì‹œ ì •ë¦¬
            if hasattr(self.audio_preprocessor, 'cleanup_cache'):
                self.audio_preprocessor.cleanup_cache(max_age_hours)
            
            if hasattr(self.vocal_separator, 'cleanup_cache'):
                self.vocal_separator.cleanup_cache(max_age_hours)
            
            if hasattr(self.transcriber, 'cleanup_cache'):
                self.transcriber.cleanup_cache(max_age_hours)
            
            self.punctuation_restorer.cleanup_cache(max_age_hours)
            self.analysis_manager.cleanup_cache(max_age_hours)
            
            print("âœ… ëª¨ë“  ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            print("ğŸ§¹ AdvancedIntegratedAnalyzer ì •ë¦¬ ì‹œì‘")
            
            # ê° ì»´í¬ë„ŒíŠ¸ ì •ë¦¬
            self.audio_preprocessor.cleanup()
            self.dialogue_detector.cleanup()
            self.transcriber.cleanup()
            self.analysis_manager.cleanup()
            self.db_manager.cleanup()
            
            print("âœ… AdvancedIntegratedAnalyzer ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = AdvancedIntegratedAnalyzer(
        enable_cache=True,
        enable_parallel=True,
        enable_async=True,
        max_workers=4
    )
    
    try:
        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        result = await analyzer.analyze_audio_comprehensive("audio/sample.wav")
        print(f"ë¶„ì„ ê²°ê³¼: {result['processing_status']}")
        
        # ë°°ì¹˜ ë¶„ì„
        audio_files = ["audio/file1.wav", "audio/file2.wav", "audio/file3.wav"]
        results = await analyzer.analyze_batch_parallel(audio_files)
        
        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        stats = analyzer.get_performance_stats()
        print(f"ì„±ëŠ¥ í†µê³„: {stats}")
        
    finally:
        analyzer.cleanup()


if __name__ == "__main__":
    asyncio.run(main()) 