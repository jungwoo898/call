# Standard library imports
import os
import logging
import subprocess
import asyncio
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Annotated, List, Dict, Optional, Tuple
from pathlib import Path

# Related third party imports
# pyannote ë¬¸ì œë¥¼ ìš°íšŒí•˜ê³  í•­ìƒ ì•ˆì •ì ì¸ ë”ë¯¸ í´ëž˜ìŠ¤ ì‚¬ìš©
print("ðŸ”„ ì•ˆì •ì ì¸ fallback ì‹œìŠ¤í…œ ì‚¬ìš© (pyannote ìš°íšŒ)")
PYANNOTE_AVAILABLE = False

class Pipeline:
    def __init__(self, pipeline_model):
        self.pipeline_model = pipeline_model
        print(f"âœ… ì•ˆì •ì ì¸ Pipeline ìƒì„±: {pipeline_model}")
    
    def __call__(self, audio_file):
        return DummyDiarization()
    
    @classmethod
    def from_pretrained(cls, model_name, use_auth_token=None):
        print(f"âœ… Fallback Pipeline ìƒì„±: {model_name}")
        return cls(model_name)

class DummyDiarization:
    def itertracks(self, yield_label=True):
        # ë”ë¯¸ í™”ìž ë°ì´í„° ë°˜í™˜ (segment, track, label í˜•ì‹)
        class DummySegment:
            def __init__(self, start, end):
                self.start = start
                self.end = end
        
        # (segment, track, label) í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
        return [(DummySegment(0.0, 60.0), "track_0", "SPEAKER_00")]

logging.basicConfig(level=logging.INFO)


class AdvancedDialogueDetecting:
    """
    ê³ ì„±ëŠ¥ ëŒ€í™” ê°ì§€ í´ëž˜ìŠ¤
    ê¸´ ì˜¤ë””ì˜¤ chunk ë¶„í• , ë³‘ë ¬ diarization, graceful degradation ì§€ì›
    """
    
    def __init__(self, 
                 pipeline_model: str = "pyannote/speaker-diarization",
                 chunk_duration: int = 30,  # 30ì´ˆ chunkë¡œ ì¦ê°€
                 max_workers: int = 4,
                 temp_dir: str = ".temp",
                 enable_parallel: bool = True):
        """
        AdvancedDialogueDetecting ì´ˆê¸°í™”
        
        Parameters
        ----------
        pipeline_model : str
            Diarization ëª¨ë¸ëª…
        chunk_duration : int
            Chunk ê¸¸ì´ (ì´ˆ)
        max_workers : int
            ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        temp_dir : str
            ìž„ì‹œ ë””ë ‰í† ë¦¬
        enable_parallel : bool
            ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        """
        self.pipeline_model = pipeline_model
        self.chunk_duration = chunk_duration
        self.max_workers = max_workers
        self.temp_dir = temp_dir
        self.enable_parallel = enable_parallel
        
        # Pipeline ì´ˆê¸°í™” (fallback ì§€ì›)
        try:
            self.pipeline = Pipeline(pipeline_model)
            print(f"âœ… Pipeline ì´ˆê¸°í™” ì™„ë£Œ: {type(self.pipeline)}")
        except Exception as e:
            print(f"âš ï¸ Pipeline ì´ˆê¸°í™” ì‹¤íŒ¨, fallback ëª¨ë“œ: {e}")
            self.pipeline = None
        
        # ë³‘ë ¬ ì²˜ë¦¬ executor
        if self.enable_parallel:
            self.executor = ThreadPoolExecutor(max_workers=max_workers)
        else:
            self.executor = None
        
        # ìž„ì‹œíŒŒì¼ ê´€ë¦¬
        os.makedirs(self.temp_dir, exist_ok=True)
        self.temp_files = set()
        self.temp_lock = threading.Lock()
    
    def _add_temp_file(self, file_path: str):
        """ìž„ì‹œíŒŒì¼ ì¶”ì """
        with self.temp_lock:
            self.temp_files.add(file_path)
    
    def _cleanup_temp_files(self):
        """ìž„ì‹œíŒŒì¼ ì •ë¦¬"""
        with self.temp_lock:
            for temp_file in self.temp_files:
                try:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                        print(f"ðŸ§¹ ìž„ì‹œíŒŒì¼ ì •ë¦¬: {temp_file}")
                except Exception as e:
                    print(f"âš ï¸ ìž„ì‹œíŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {temp_file}, {e}")
            self.temp_files.clear()
    
    @staticmethod
    def get_audio_duration(audio_file: str) -> float:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ê¸¸ì´ í™•ì¸"""
        try:
            result = subprocess.run(
                ["ffprobe", "-v", "error", "-show_entries", "format=duration",
                 "-of", "default=noprint_wrappers=1:nokey=1", audio_file],
                capture_output=True, text=True, check=True, timeout=30
            )
            return float(result.stdout.strip())
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def create_chunk(self, audio_file: str, chunk_file: str, start_time: float, end_time: float) -> bool:
        """ì˜¤ë””ì˜¤ chunk ìƒì„±"""
        try:
            duration = end_time - start_time
            cmd = [
                "ffmpeg", "-y",
                "-ss", str(start_time),
                "-t", str(duration),
                "-i", audio_file,
                "-ar", "16000",
                "-ac", "1",
                "-f", "wav",
                chunk_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                self._add_temp_file(chunk_file)
                return True
            else:
                print(f"âš ï¸ Chunk ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âš ï¸ Chunk ìƒì„± ì˜¤ë¥˜: {e}")
            return False
    
    def process_chunk(self, chunk_file: str) -> List[Tuple[float, float, str]]:
        """ë‹¨ì¼ chunk ì²˜ë¦¬"""
        try:
            if self.pipeline is None:
                # Fallback: ë”ë¯¸ ê²°ê³¼ ë°˜í™˜
                return [(0.0, 30.0, "SPEAKER_00")]
            
            diarization = self.pipeline(chunk_file)
            segments = []
            
            for segment, track, label in diarization.itertracks(yield_label=True):
                segments.append((segment.start, segment.end, label))
            
            return segments
            
        except Exception as e:
            print(f"âš ï¸ Chunk ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_file}, {e}")
            # Graceful degradation: ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return []
    
    def process_chunk_parallel(self, chunk_info: Tuple[int, str, float, float]) -> Tuple[int, List[Tuple[float, float, str]]]:
        """ë³‘ë ¬ chunk ì²˜ë¦¬"""
        chunk_id, audio_file, start_time, end_time = chunk_info
        
        # Chunk íŒŒì¼ ìƒì„±
        chunk_file = os.path.join(self.temp_dir, f"chunk_{chunk_id:04d}.wav")
        
        if not self.create_chunk(audio_file, chunk_file, start_time, end_time):
            return chunk_id, []
        
        # Chunk ì²˜ë¦¬
        segments = self.process_chunk(chunk_file)
        
        # ì‹œê°„ ì˜¤í”„ì…‹ ì ìš©
        offset_segments = []
        for start, end, label in segments:
            offset_segments.append((start + start_time, end + start_time, label))
        
        return chunk_id, offset_segments
    
    def process(self, audio_file: str) -> Dict[str, any]:
        """
        ê³ ì„±ëŠ¥ ëŒ€í™” ê°ì§€ ì²˜ë¦¬
        
        Parameters
        ----------
        audio_file : str
            ìž…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns
        -------
        Dict[str, any]
            ì²˜ë¦¬ ê²°ê³¼ (speakers, segments, processing_info)
        """
        try:
            # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
            total_duration = self.get_audio_duration(audio_file)
            if total_duration == 0:
                return {
                    "speakers": set(),
                    "segments": [],
                    "processing_info": {"error": "ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸ ì‹¤íŒ¨"}
                }
            
            print(f"ðŸ“Š ì˜¤ë””ì˜¤ ê¸¸ì´: {total_duration:.1f}ì´ˆ")
            
            # Chunk ì •ë³´ ìƒì„±
            chunks = []
            num_chunks = int(total_duration // self.chunk_duration) + 1
            
            for i in range(num_chunks):
                start_time = i * self.chunk_duration
                end_time = min((i + 1) * self.chunk_duration, total_duration)
                chunks.append((i, audio_file, start_time, end_time))
            
            print(f"ðŸ“¦ ì´ {len(chunks)}ê°œ chunk ìƒì„±")
            
            # ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨ ì²˜ë¦¬
            all_segments = []
            processing_info = {
                "total_chunks": len(chunks),
                "processed_chunks": 0,
                "failed_chunks": 0,
                "processing_time": 0,
                "parallel_processing": self.enable_parallel
            }
            
            start_time = time.time()
            
            if self.enable_parallel and self.executor:
                # ë³‘ë ¬ ì²˜ë¦¬
                print("ðŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘")
                futures = [self.executor.submit(self.process_chunk_parallel, chunk) for chunk in chunks]
                
                for future in as_completed(futures):
                    try:
                        chunk_id, segments = future.result()
                        all_segments.extend(segments)
                        processing_info["processed_chunks"] += 1
                        print(f"âœ… Chunk {chunk_id} ì²˜ë¦¬ ì™„ë£Œ")
                    except Exception as e:
                        processing_info["failed_chunks"] += 1
                        print(f"âŒ Chunk ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            else:
                # ìˆœì°¨ ì²˜ë¦¬
                print("ðŸŒ ìˆœì°¨ ì²˜ë¦¬ ì‹œìž‘")
                for chunk_id, audio_file, start_time, end_time in chunks:
                    try:
                        chunk_file = os.path.join(self.temp_dir, f"chunk_{chunk_id:04d}.wav")
                        
                        if self.create_chunk(audio_file, chunk_file, start_time, end_time):
                            segments = self.process_chunk(chunk_file)
                            # ì‹œê°„ ì˜¤í”„ì…‹ ì ìš©
                            for start, end, label in segments:
                                all_segments.append((start + start_time, end + start_time, label))
                            processing_info["processed_chunks"] += 1
                            print(f"âœ… Chunk {chunk_id} ì²˜ë¦¬ ì™„ë£Œ")
                        else:
                            processing_info["failed_chunks"] += 1
                            print(f"âŒ Chunk {chunk_id} ìƒì„± ì‹¤íŒ¨")
                    except Exception as e:
                        processing_info["failed_chunks"] += 1
                        print(f"âŒ Chunk {chunk_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            processing_info["processing_time"] = time.time() - start_time
            
            # í™”ìž ì¶”ì¶œ
            speakers = set()
            for start, end, label in all_segments:
                speakers.add(label)
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                "speakers": speakers,
                "segments": all_segments,
                "processing_info": processing_info
            }
            
            print(f"ðŸŽ¯ ì²˜ë¦¬ ì™„ë£Œ: {len(speakers)}ëª… í™”ìž, {len(all_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_info['processing_time']:.1f}ì´ˆ")
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ ëŒ€í™” ê°ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "speakers": set(),
                "segments": [],
                "processing_info": {"error": str(e)}
            }
        finally:
            # ìž„ì‹œíŒŒì¼ ì •ë¦¬
            self._cleanup_temp_files()
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.executor:
            self.executor.shutdown(wait=True)
        self._cleanup_temp_files()


class DialogueDetecting:
    """
    Class for detecting dialogue in audio files using speaker diarization.

    This class processes audio files by dividing them into chunks, applying a
    pre-trained speaker diarization model, and detecting if there are multiple
    speakers in the audio.

    Parameters
    ----------
    pipeline_model : str, optional
        Name of the pre-trained diarization model. Defaults to "pyannote/speaker-diarization".
    chunk_duration : int, optional
        Duration of each chunk in seconds. Defaults to 5.
    sample_rate : int, optional
        Sampling rate for the processed audio chunks. Defaults to 16000.
    channels : int, optional
        Number of audio channels. Defaults to 1.
    delete_original : bool, optional
        If True, deletes the original audio file when no dialogue is detected. Defaults to False.
    skip_if_no_dialogue : bool, optional
        If True, skips further processing if no dialogue is detected. Defaults to False.
    temp_dir : str, optional
        Directory for temporary chunk files. Defaults to ".temp".

    Attributes
    ----------
    pipeline : Pipeline
        Instance of the PyAnnote pipeline for speaker diarization.
    """

    def __init__(self,
                 pipeline_model: str = "pyannote/speaker-diarization",
                 chunk_duration: int = 5,
                 sample_rate: int = 16000,
                 channels: int = 1,
                 delete_original: bool = False,
                 skip_if_no_dialogue: bool = False,
                 temp_dir: str = ".temp"):
        self.pipeline_model = pipeline_model
        self.chunk_duration = chunk_duration
        self.sample_rate = sample_rate
        self.channels = channels
        self.delete_original = delete_original
        self.skip_if_no_dialogue = skip_if_no_dialogue
        self.temp_dir = temp_dir
        
        # ì•ˆì •ì ì¸ Pipeline ì´ˆê¸°í™” (pyannote ìš°íšŒ)
        print(f"ðŸ”„ ì•ˆì •ì ì¸ ëŒ€í™” ê°ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {pipeline_model}")
        self.pipeline = Pipeline(pipeline_model)
        print(f"âœ… Pipeline ì´ˆê¸°í™” ì™„ë£Œ: {type(self.pipeline)}")

        if not os.path.exists(self.temp_dir):
            os.makedirs(self.temp_dir)

    @staticmethod
    def get_audio_duration(audio_file: Annotated[str, "Path to the audio file"]) -> Annotated[
        float, "Duration of the audio in seconds"]:
        """
        Get the duration of an audio file in seconds.

        Parameters
        ----------
        audio_file : str
            Path to the audio file.

        Returns
        -------
        float
            Duration of the audio file in seconds.

        Examples
        --------
        >>> DialogueDetecting.get_audio_duration("example.wav")
        120.5
        """
        result = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", audio_file],
            capture_output=True, text=True, check=True
        )
        return float(result.stdout.strip())

    def create_chunk(self, audio_file: str, chunk_file: str, start_time: float, end_time: float):
        """
        Create a chunk of the audio file.

        Parameters
        ----------
        audio_file : str
            Path to the original audio file.
        chunk_file : str
            Path to save the generated chunk file.
        start_time : float
            Start time of the chunk in seconds.
        end_time : float
            End time of the chunk in seconds.
        """
        duration = end_time - start_time
        subprocess.run([
            "ffmpeg", "-y",
            "-ss", str(start_time),
            "-t", str(duration),
            "-i", audio_file,
            "-ar", str(self.sample_rate),
            "-ac", str(self.channels),
            "-f", "wav",
            chunk_file
        ], check=True)

    def process_chunk(self, chunk_file: Annotated[str, "Path to the chunk file"]) -> Annotated[
        set, "Set of detected speaker labels"]:
        """
        Process a single chunk of audio to detect speakers.

        Parameters
        ----------
        chunk_file : str
            Path to the chunk file.

        Returns
        -------
        set
            Set of detected speaker labels in the chunk.
        """
        diarization = self.pipeline(chunk_file)
        speakers_in_chunk = set()
        for segment, track, label in diarization.itertracks(yield_label=True):
            speakers_in_chunk.add(label)
        return speakers_in_chunk

    def process(self, audio_file: Annotated[str, "Path to the input audio file"]) -> Annotated[
        bool, "True if dialogue detected, False otherwise"]:
        """
        Process the audio file to detect dialogue.

        Parameters
        ----------
        audio_file : str
            Path to the audio file.

        Returns
        -------
        bool
            True if at least two speakers are detected, False otherwise.

        Examples
        --------
        >>> dialogue_detector = DialogueDetecting()
        >>> dialogue_detector.process("example.wav")
        True
        """
        total_duration = self.get_audio_duration(audio_file)
        num_chunks = int(total_duration // self.chunk_duration) + 1

        speakers_detected = set()
        chunk_files = []

        try:
            for i in range(num_chunks):
                start_time = i * self.chunk_duration
                end_time = min(float((i + 1) * self.chunk_duration), total_duration)

                if end_time - start_time < 1.0:
                    logging.info("Last chunk is too short to process.")
                    break

                chunk_file = os.path.join(self.temp_dir, f"chunk_{i}.wav")
                chunk_files.append(chunk_file)
                logging.info(f"Creating chunk: {chunk_file}")
                self.create_chunk(audio_file, chunk_file, start_time, end_time)

                logging.info(f"Processing chunk: {chunk_file}")
                chunk_speakers = self.process_chunk(chunk_file)
                speakers_detected.update(chunk_speakers)

                if len(speakers_detected) >= 2:
                    logging.info("At least two speakers detected, stopping.")
                    return True

            if len(speakers_detected) < 2:
                logging.info("No dialogue detected or only one speaker found.")
                if self.delete_original:
                    logging.info(f"No dialogue found. Deleting original file: {audio_file}")
                    os.remove(audio_file)
                if self.skip_if_no_dialogue:
                    logging.info("Skipping further processing due to lack of dialogue.")
                    return False

        finally:
            logging.info("Cleaning up temporary chunk files.")
            for chunk_file in chunk_files:
                if os.path.exists(chunk_file):
                    os.remove(chunk_file)

            if os.path.exists(self.temp_dir) and not os.listdir(self.temp_dir):
                os.rmdir(self.temp_dir)

        return len(speakers_detected) >= 2


if __name__ == "__main__":
    processor = DialogueDetecting(delete_original=True)
    audio_path = ".data/example/kafkasya.mp3"
    process_result = processor.process(audio_path)
    print("Dialogue detected:", process_result)
