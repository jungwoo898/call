# Standard library imports
import os
import json
import sqlite3
import threading
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Annotated, List, Dict, Any, Optional, Tuple
from pathlib import Path
from queue import Queue
import logging
import psycopg2
import psycopg2.extras
from psycopg2.extensions import connection, cursor
from datetime import datetime, date
import uuid
from src.utils.locale_config import get_current_time

# Local imports
from src.db.multi_database_manager import MultiDatabaseManager

logger = logging.getLogger(__name__)

class AdvancedDatabaseManager:
    """
    ê³ ì„±ëŠ¥ DB ê´€ë¦¬ í´ë˜ìŠ¤
    bulk insert, ë¹„ë™ê¸° ì €ì¥, ì¥ì•  ë³µêµ¬ ë¡œì§ ì§€ì›
    """
    
    def __init__(self, 
                 config_path: str = "config/config.yaml",
                 max_workers: int = 4,
                 batch_size: int = 100,
                 enable_async: bool = True,
                 enable_recovery: bool = True):
        """
        AdvancedDatabaseManager ì´ˆê¸°í™”
        
        Parameters
        ----------
        config_path : str
            ì„¤ì • íŒŒì¼ ê²½ë¡œ
        max_workers : int
            ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        batch_size : int
            ë°°ì¹˜ í¬ê¸°
        enable_async : bool
            ë¹„ë™ê¸° ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        enable_recovery : bool
            ì¥ì•  ë³µêµ¬ í™œì„±í™” ì—¬ë¶€
        """
        self.config_path = config_path
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.enable_async = enable_async
        self.enable_recovery = enable_recovery
        
        # ê¸°ë³¸ DB ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.db_manager = MultiDatabaseManager(config_path)
        
        # ë³‘ë ¬ ì²˜ë¦¬ executor
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # ë¹„ë™ê¸° ì €ì¥ í
        self.save_queue = Queue()
        self.save_thread = None
        if self.enable_async:
            self.save_thread = threading.Thread(target=self._async_save_worker, daemon=True)
            self.save_thread.start()
        
        # ì¥ì•  ë³µêµ¬ ë¡œê·¸
        self.recovery_log_file = Path("logs/db_recovery.log")
        self.recovery_log_file.parent.mkdir(exist_ok=True)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            "total_operations": 0,
            "bulk_operations": 0,
            "async_operations": 0,
            "failed_operations": 0,
            "recovery_operations": 0,
            "avg_operation_time": 0.0
        }
        
        # ë°°ì¹˜ ë²„í¼
        self.batch_buffers = {
            "audio_analysis": [],
            "quality_analysis": [],
            "llm_analysis": []
        }
        self.batch_locks = {
            "audio_analysis": threading.Lock(),
            "quality_analysis": threading.Lock(),
            "llm_analysis": threading.Lock()
        }
        
        print(f"âœ… AdvancedDatabaseManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _log_recovery(self, operation: str, error: str, recovery_action: str):
        """ì¥ì•  ë³µêµ¬ ë¡œê·¸ ê¸°ë¡"""
        if not self.enable_recovery:
            return
        
        try:
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            log_entry = f"[{timestamp}] {operation} - Error: {error} - Recovery: {recovery_action}\n"
            
            with open(self.recovery_log_file, 'a', encoding='utf-8') as f:
                f.write(log_entry)
                
        except Exception as e:
            print(f"âš ï¸ ë³µêµ¬ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    def _async_save_worker(self):
        """ë¹„ë™ê¸° ì €ì¥ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while True:
            try:
                # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                operation = self.save_queue.get(timeout=1)
                if operation is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                operation_type, data, callback = operation
                
                # ì‹¤ì œ ì €ì¥ ìˆ˜í–‰
                start_time = time.time()
                success = self._perform_save_operation(operation_type, data)
                processing_time = time.time() - start_time
                
                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                self.performance_stats["total_operations"] += 1
                self.performance_stats["async_operations"] += 1
                self.performance_stats["avg_operation_time"] = (
                    (self.performance_stats["avg_operation_time"] * (self.performance_stats["total_operations"] - 1) + processing_time) 
                    / self.performance_stats["total_operations"]
                )
                
                if not success:
                    self.performance_stats["failed_operations"] += 1
                
                # ì½œë°± í˜¸ì¶œ
                if callback:
                    try:
                        callback(success, processing_time)
                    except Exception as e:
                        print(f"âš ï¸ ì½œë°± í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                
                self.save_queue.task_done()
                
            except Exception as e:
                print(f"âš ï¸ ë¹„ë™ê¸° ì €ì¥ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                time.sleep(1)
    
    def _perform_save_operation(self, operation_type: str, data: Dict[str, Any]) -> bool:
        """ì‹¤ì œ ì €ì¥ ì‘ì—… ìˆ˜í–‰"""
        cursor = None
        try:
            if operation_type == "audio_analysis":
                return self._save_audio_analysis_bulk(data)
            elif operation_type == "quality_analysis":
                return self._save_quality_analysis_bulk(data)
            elif operation_type == "llm_analysis":
                return self._save_llm_analysis_bulk(data)
            else:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… íƒ€ì…: {operation_type}")
                return False
                
        except Exception as e:
            print(f"âš ï¸ ì €ì¥ ì‘ì—… ì‹¤íŒ¨: {operation_type}, {e}")
            self._log_recovery(operation_type, str(e), "retry_later")
            return False
    
    def _save_audio_analysis_bulk(self, data_list: List[Dict[str, Any]]) -> bool:
        """ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            if not data_list:
                return True
            
            connection = self.db_manager.get_connection("audio_analysis")
            cursor = connection.cursor()
            
            # Bulk insert ì¿¼ë¦¬ êµ¬ì„±
            query = """
                INSERT INTO audio_analysis (
                    file_path, duration, sample_rate, channels, 
                    transcription, language, confidence_score,
                    created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            # ë°ì´í„° ì¤€ë¹„
            values = []
            current_time = time.time()
            
            for data in data_list:
                values.append((
                    data.get('file_path', ''),
                    data.get('duration', 0.0),
                    data.get('sample_rate', 16000),
                    data.get('channels', 1),
                    data.get('transcription', ''),
                    data.get('language', 'ko'),
                    data.get('confidence_score', 0.0),
                    current_time,
                    current_time
                ))
            
            # Bulk insert ì‹¤í–‰
            cursor.executemany(query, values)
            connection.commit()
            
            print(f"âœ… ì˜¤ë””ì˜¤ ë¶„ì„ bulk ì €ì¥ ì™„ë£Œ: {len(data_list)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ë¶„ì„ bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            self._log_recovery("audio_analysis_bulk_save", str(e), "rollback_and_retry")
            return False
    
    def _save_quality_analysis_bulk(self, data_list: List[Dict[str, Any]]) -> bool:
        """í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            if not data_list:
                return True
            
            connection = self.db_manager.get_connection("quality_analysis")
            cursor = connection.cursor()
            
            # Bulk insert ì¿¼ë¦¬ êµ¬ì„±
            query = """
                INSERT INTO consultation_quality (
                    audio_analysis_id, clarity_score, politeness_score,
                    empathy_score, professionalism_score, response_quality_score,
                    overall_score, sentiment_analysis, profanity_detected,
                    speaker_classification, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            # ë°ì´í„° ì¤€ë¹„
            values = []
            current_time = time.time()
            
            for data in data_list:
                values.append((
                    data.get('audio_analysis_id', 0),
                    data.get('clarity_score', 0.5),
                    data.get('politeness_score', 0.5),
                    data.get('empathy_score', 0.5),
                    data.get('professionalism_score', 0.5),
                    data.get('response_quality_score', 0.5),
                    data.get('overall_score', 0.5),
                    json.dumps(data.get('sentiment_analysis', {}), ensure_ascii=False),
                    data.get('profanity_detected', False),
                    data.get('speaker_classification', 'ê³ ê°'),
                    current_time,
                    current_time
                ))
            
            # Bulk insert ì‹¤í–‰
            cursor.executemany(query, values)
            connection.commit()
            
            print(f"âœ… í’ˆì§ˆ ë¶„ì„ bulk ì €ì¥ ì™„ë£Œ: {len(data_list)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âš ï¸ í’ˆì§ˆ ë¶„ì„ bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            self._log_recovery("quality_analysis_bulk_save", str(e), "rollback_and_retry")
            return False
    
    def _save_llm_analysis_bulk(self, data_list: List[Dict[str, Any]]) -> bool:
        """LLM ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            if not data_list:
                return True
            
            connection = self.db_manager.get_connection("quality_analysis")
            cursor = connection.cursor()
            
            # Bulk insert ì¿¼ë¦¬ êµ¬ì„±
            query = """
                INSERT INTO llm_analysis (
                    audio_analysis_id, analysis_type, analysis_result,
                    confidence_score, model_used, processing_time,
                    created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """
            
            # ë°ì´í„° ì¤€ë¹„
            values = []
            current_time = time.time()
            
            for data in data_list:
                values.append((
                    data.get('audio_analysis_id', 0),
                    data.get('analysis_type', 'general'),
                    json.dumps(data.get('analysis_result', {}), ensure_ascii=False),
                    data.get('confidence_score', 0.5),
                    data.get('model_used', 'default'),
                    data.get('processing_time', 0.0),
                    current_time,
                    current_time
                ))
            
            # Bulk insert ì‹¤í–‰
            cursor.executemany(query, values)
            connection.commit()
            
            print(f"âœ… LLM ë¶„ì„ bulk ì €ì¥ ì™„ë£Œ: {len(data_list)}ê°œ")
            return True
            
        except Exception as e:
            print(f"âš ï¸ LLM ë¶„ì„ bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            self._log_recovery("llm_analysis_bulk_save", str(e), "rollback_and_retry")
            return False
    
    def db_add_to_batch(self, operation_type: str, data: Dict[str, Any]):
        """ë°°ì¹˜ ë²„í¼ì— ë°ì´í„° ì¶”ê°€"""
        if operation_type not in self.batch_buffers:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—… íƒ€ì…: {operation_type}")
            return
        
        with self.batch_locks[operation_type]:
            self.batch_buffers[operation_type].append(data)
            
            # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ìë™ ì €ì¥
            if len(self.batch_buffers[operation_type]) >= self.batch_size:
                self.db_flush_batch(operation_type)
    
    def db_flush_batch(self, operation_type: str):
        """ë°°ì¹˜ ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if operation_type not in self.batch_buffers:
            return
        
        with self.batch_locks[operation_type]:
            if not self.batch_buffers[operation_type]:
                return
            
            data_list = self.batch_buffers[operation_type].copy()
            self.batch_buffers[operation_type].clear()
        
        # ë¹„ë™ê¸° ì €ì¥ ë˜ëŠ” ì¦‰ì‹œ ì €ì¥
        if self.enable_async:
            self.save_queue.put((operation_type, data_list, None))
        else:
            self._perform_save_operation(operation_type, data_list)
    
    def db_flush_all_batches(self):
        """ëª¨ë“  ë°°ì¹˜ ë²„í¼ í”ŒëŸ¬ì‹œ"""
        for operation_type in self.batch_buffers.keys():
            self.db_flush_batch(operation_type)
    
    async def save_audio_analysis_async(self, data: Dict[str, Any], callback=None):
        """ë¹„ë™ê¸° ì˜¤ë””ì˜¤ ë¶„ì„ ì €ì¥"""
        if self.enable_async:
            self.save_queue.put(("audio_analysis", [data], callback))
        else:
            success = self._perform_save_operation("audio_analysis", [data])
            if callback:
                callback(success, 0.0)
    
    async def save_quality_analysis_async(self, data: Dict[str, Any], callback=None):
        """ë¹„ë™ê¸° í’ˆì§ˆ ë¶„ì„ ì €ì¥"""
        if self.enable_async:
            self.save_queue.put(("quality_analysis", [data], callback))
        else:
            success = self._perform_save_operation("quality_analysis", [data])
            if callback:
                callback(success, 0.0)
    
    async def save_llm_analysis_async(self, data: Dict[str, Any], callback=None):
        """ë¹„ë™ê¸° LLM ë¶„ì„ ì €ì¥"""
        if self.enable_async:
            self.save_queue.put(("llm_analysis", [data], callback))
        else:
            success = self._perform_save_operation("llm_analysis", [data])
            if callback:
                callback(success, 0.0)
    
    def db_bulk_save_audio_analysis(self, data_list: List[Dict[str, Any]]) -> bool:
        """ì˜¤ë””ì˜¤ ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            success = self._perform_save_operation("audio_analysis", data_list)
            if success:
                self.performance_stats["bulk_operations"] += 1
            return success
        except Exception as e:
            print(f"âš ï¸ Bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def db_bulk_save_quality_analysis(self, data_list: List[Dict[str, Any]]) -> bool:
        """í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            success = self._perform_save_operation("quality_analysis", data_list)
            if success:
                self.performance_stats["bulk_operations"] += 1
            return success
        except Exception as e:
            print(f"âš ï¸ Bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def db_bulk_save_llm_analysis(self, data_list: List[Dict[str, Any]]) -> bool:
        """LLM ë¶„ì„ ê²°ê³¼ bulk ì €ì¥"""
        cursor = None
        try:
            success = self._perform_save_operation("llm_analysis", data_list)
            if success:
                self.performance_stats["bulk_operations"] += 1
            return success
        except Exception as e:
            print(f"âš ï¸ Bulk ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def db_get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.performance_stats.copy()
    
    def db_get_recovery_log(self, limit: int = 100) -> List[str]:
        """ë³µêµ¬ ë¡œê·¸ ì¡°íšŒ"""
        cursor = None
        try:
            if not self.recovery_log_file.exists():
                return []
            
            with open(self.recovery_log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                return lines[-limit:] if len(lines) > limit else lines
                
        except Exception as e:
            print(f"âš ï¸ ë³µêµ¬ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def db_cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        # ëª¨ë“  ë°°ì¹˜ í”ŒëŸ¬ì‹œ
        self.db_flush_all_batches()
        
        # ë¹„ë™ê¸° ì €ì¥ ì›Œì»¤ ì¢…ë£Œ
        if self.save_queue:
            self.save_queue.put(None)  # ì¢…ë£Œ ì‹ í˜¸
        
        # Executor ì¢…ë£Œ
        if self.executor:
            self.executor.shutdown(wait=True)
        
        print("âœ… AdvancedDatabaseManager ì •ë¦¬ ì™„ë£Œ")

class SimplifiedDBManager:
    """ê°„ì†Œí™”ëœ DB ê´€ë¦¬ì"""
    
    def __init__(self, connection_params: Dict[str, str]):
        self.connection_params = connection_params
        self.conn: Optional[connection] = None
        
    def db_connect(self) -> connection:
        """DB ì—°ê²°"""
        cursor = None
        try:
            if not self.conn or self.conn.closed:
                import psycopg2
                self.conn = psycopg2.connect(**self.connection_params)
                self.conn.autocommit = False
            return self.conn
        except Exception as e:
            logger.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def db_disconnect(self):
        """DB ì—°ê²° í•´ì œ"""
        if self.conn and not self.conn.closed:
            self.conn.close()
    
    def db_save_consultation_classification(self, 
                                       audio_file_id: int,
                                       classification_result: Dict[str, Any],
                                       session_date: date = None) -> int:
        """ìƒë‹´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor()
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if session_date is None:
                session_date = date.today()
            
            # consultation_sessions í…Œì´ë¸”ì— ì €ì¥
            insert_query = """
                INSERT INTO consultation_sessions (
                    audio_file_id, session_date, duration_minutes,
                    business_area, consultation_subject, consultation_requirement,
                    consultation_content, consultation_reason, consultation_result,
                    overall_quality_score, analysis_status, analysis_completed_at,
                    summary, key_issues, resolution_status, customer_satisfaction_score
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                ) RETURNING id
            """
            
            cursor.execute(insert_query, (
                audio_file_id,
                session_date,
                classification_result.get('duration_minutes', 0.0),
                classification_result.get('business_area', 'ê¸°íƒ€'),
                classification_result.get('consultation_subject', 'ê¸°íƒ€'),
                classification_result.get('consultation_requirement', 'ë‹¨ì¼ ìš”ê±´ ë¯¼ì›'),
                classification_result.get('consultation_content', 'ì¼ë°˜ ë¬¸ì˜ ìƒë‹´'),
                classification_result.get('consultation_reason', 'ë¯¼ì›ì¸'),
                classification_result.get('consultation_result', 'ì¶”ê°€ìƒë‹´í•„ìš”'),
                classification_result.get('overall_quality_score', 0.0),
                'completed',
                get_current_time(),
                classification_result.get('summary', ''),
                json.dumps(classification_result.get('key_issues', {}), ensure_ascii=False),
                classification_result.get('resolution_status', 'unresolved'),
                classification_result.get('customer_satisfaction_score', 0.0)
            ))
            
            session_id = cursor.fetchone()[0]
            conn.commit()
            
            logger.info(f"ìƒë‹´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: session_id={session_id}")
            return session_id
            
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"ìƒë‹´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_save_quality_metrics(self, session_id: int, metrics: Dict[str, float]):
        """í’ˆì§ˆ ì§€í‘œ ì €ì¥"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor()
            
            # ê¸°ì¡´ ì§€í‘œ ì‚­ì œ
            cursor.execute("DELETE FROM quality_metrics WHERE session_id = %s", (session_id,))
            
            # ìƒˆ ì§€í‘œ ì €ì¥
            for metric_name, metric_value in metrics.items():
                insert_query = """
                    INSERT INTO quality_metrics (
                        session_id, metric_name, metric_value, metric_description, category
                    ) VALUES (%s, %s, %s, %s, %s)
                """
                
                cursor.execute(insert_query, (
                    session_id,
                    metric_name,
                    metric_value,
                    f"{metric_name} ì§€í‘œ",
                    'communication'  # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬
                ))
            
            conn.commit()
            logger.info(f"í’ˆì§ˆ ì§€í‘œ ì €ì¥ ì™„ë£Œ: session_id={session_id}")
            
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"í’ˆì§ˆ ì§€í‘œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_save_sentiment_analysis(self, session_id: int, sentiment_data: List[Dict[str, Any]]):
        """ê°ì • ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor()
            
            # ê¸°ì¡´ ê°ì • ë¶„ì„ ì‚­ì œ
            cursor.execute("DELETE FROM sentiment_analysis WHERE session_id = %s", (session_id,))
            
            # ìƒˆ ê°ì • ë¶„ì„ ì €ì¥
            for sentiment in sentiment_data:
                insert_query = """
                    INSERT INTO sentiment_analysis (
                        session_id, speaker_type, time_segment_start, time_segment_end,
                        sentiment_score, emotion_category, confidence, emotion_intensity
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """
                
                cursor.execute(insert_query, (
                    session_id,
                    sentiment.get('speaker_type', 'unknown'),
                    sentiment.get('time_segment_start', 0.0),
                    sentiment.get('time_segment_end', 0.0),
                    sentiment.get('sentiment_score', 0.0),
                    sentiment.get('emotion_category', 'neutral'),
                    sentiment.get('confidence', 0.0),
                    sentiment.get('emotion_intensity', 0.0)
                ))
            
            conn.commit()
            logger.info(f"ê°ì • ë¶„ì„ ì €ì¥ ì™„ë£Œ: session_id={session_id}")
            
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"ê°ì • ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_get_consultation_summary(self, session_id: int) -> Dict[str, Any]:
        """ìƒë‹´ ìš”ì•½ ì •ë³´ ì¡°íšŒ"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            query = """
                SELECT 
                    cs.*,
                    af.file_name,
                    af.duration_seconds,
                    COUNT(qm.id) as metrics_count,
                    COUNT(sa.id) as sentiment_count
                FROM consultation_sessions cs
                LEFT JOIN audio_files af ON cs.audio_file_id = af.id
                LEFT JOIN quality_metrics qm ON cs.id = qm.session_id
                LEFT JOIN sentiment_analysis sa ON cs.id = sa.session_id
                WHERE cs.id = %s
                GROUP BY cs.id, af.file_name, af.duration_seconds
            """
            
            cursor.execute(query, (session_id,))
            result = cursor.fetchone()
            
            if result:
                return dict(result)
            else:
                return {}
                
        except Exception as e:
            logger.error(f"ìƒë‹´ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_get_business_area_statistics(self) -> List[Dict[str, Any]]:
        """ì—…ë¬´ ë¶„ì•¼ë³„ í†µê³„ ì¡°íšŒ"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            query = """
                SELECT 
                    business_area,
                    COUNT(*) as total_sessions,
                    AVG(overall_quality_score) as avg_quality_score,
                    AVG(customer_satisfaction_score) as avg_satisfaction,
                    AVG(duration_minutes) as avg_duration,
                    COUNT(CASE WHEN resolution_status = 'resolved' THEN 1 END) as resolved_count,
                    COUNT(CASE WHEN resolution_status = 'unresolved' THEN 1 END) as unresolved_count
                FROM consultation_sessions
                WHERE business_area IS NOT NULL
                GROUP BY business_area
                ORDER BY total_sessions DESC
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"ì—…ë¬´ ë¶„ì•¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_get_classification_accuracy_report(self) -> Dict[str, Any]:
        """ë¶„ë¥˜ ì •í™•ë„ ë¦¬í¬íŠ¸"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # ìƒë‹´ ì£¼ì œë³„ ë¶„í¬
            subject_query = """
                SELECT 
                    consultation_subject,
                    COUNT(*) as count,
                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
                FROM consultation_sessions
                WHERE consultation_subject IS NOT NULL
                GROUP BY consultation_subject
                ORDER BY count DESC
            """
            
            cursor.execute(subject_query)
            subject_stats = [dict(row) for row in cursor.fetchall()]
            
            # ìƒë‹´ ê²°ê³¼ë³„ ë¶„í¬
            result_query = """
                SELECT 
                    consultation_result,
                    COUNT(*) as count,
                    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage
                FROM consultation_sessions
                WHERE consultation_result IS NOT NULL
                GROUP BY consultation_result
                ORDER BY count DESC
            """
            
            cursor.execute(result_query)
            result_stats = [dict(row) for row in cursor.fetchall()]
            
            # ì „ì²´ í†µê³„
            total_query = """
                SELECT 
                    COUNT(*) as total_sessions,
                    AVG(overall_quality_score) as avg_quality,
                    AVG(customer_satisfaction_score) as avg_satisfaction,
                    COUNT(CASE WHEN resolution_status = 'resolved' THEN 1 END) as resolved_count
                FROM consultation_sessions
            """
            
            cursor.execute(total_query)
            total_stats = dict(cursor.fetchone())
            
            return {
                'total_statistics': total_stats,
                'subject_distribution': subject_stats,
                'result_distribution': result_stats
            }
            
        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì •í™•ë„ ë¦¬í¬íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()
    
    def db_update_audio_processing_status(self, audio_file_id: int, status: str, error_message: str = None):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        cursor = None
        conn = None
        try:
            conn = self.db_connect()
            cursor = conn.cursor()
            
            if status == 'completed':
                query = """
                    UPDATE audio_files 
                    SET processing_status = %s, processing_completed_at = %s
                    WHERE id = %s
                """
                cursor.execute(query, (status, get_current_time(), audio_file_id))
            else:
                query = """
                    UPDATE audio_files 
                    SET processing_status = %s, error_message = %s
                    WHERE id = %s
                """
                cursor.execute(query, (status, error_message, audio_file_id))
            
            conn.commit()
            logger.info(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸: audio_file_id={audio_file_id}, status={status}")
            
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if cursor:
                cursor.close()

    # -----------------------------------------------------------------
    # ğŸ”„ ë ˆê±°ì‹œ ë©”ì„œë“œëª… í˜¸í™˜ (IntegratedAnalyzer ë“± ê¸°ì¡´ ì½”ë“œ ì§€ì›)
    # -----------------------------------------------------------------
    # ì €ì¥ ê³„ì—´
    def save_consultation_classification(self, audio_file_id: int, classification_result: Dict[str, Any]):
        return self.db_save_consultation_classification(audio_file_id, classification_result)

    def save_quality_metrics(self, session_id: int, metrics: Dict[str, float]):
        return self.db_save_quality_metrics(session_id, metrics)

    def save_sentiment_analysis(self, session_id: int, sentiment_data: List[Dict[str, Any]]):
        return self.db_save_sentiment_analysis(session_id, sentiment_data)

    # ì—…ë°ì´íŠ¸ ê³„ì—´
    def update_audio_processing_status(self, audio_file_id: int, status: str, error_message: str = None):
        return self.db_update_audio_processing_status(audio_file_id, status, error_message)

    # ì¡°íšŒ ê³„ì—´
    def get_business_area_statistics(self):
        return self.db_get_business_area_statistics()

    def get_classification_accuracy_report(self):
        return self.db_get_classification_accuracy_report()

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # DB ì—°ê²° ì„¤ì •
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'database': 'callytics',
        'user': 'callytics_user',
        'password': '1234'
    }
    
    db_manager = SimplifiedDBManager(db_config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    classification_result = {
        'business_area': 'ìš”ê¸ˆì œ ë³€ê²½',
        'consultation_subject': 'ì£¼ë¬¸/ê²°ì œ/í™•ì¸',
        'consultation_requirement': 'ë‹¨ì¼ ìš”ê±´ ë¯¼ì›',
        'consultation_content': 'ì—…ë¬´ ì²˜ë¦¬ ìƒë‹´',
        'consultation_reason': 'ë¯¼ì›ì¸',
        'consultation_result': 'ë§Œì¡±',
        'overall_quality_score': 0.85,
        'customer_satisfaction_score': 0.9,
        'duration_minutes': 5.5,
        'summary': 'ìš”ê¸ˆì œ ë³€ê²½ ìš”ì²­ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìƒë‹´ ì™„ë£Œ',
        'key_issues': {'main_issue': 'ìš”ê¸ˆì œ ë³€ê²½', 'resolution': 'ì„±ê³µ'}
    }
    
    try:
        # ìƒë‹´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
        session_id = db_manager.db_save_consultation_classification(1, classification_result)
        print(f"ìƒë‹´ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: session_id={session_id}")
        
        # ìš”ì•½ ì •ë³´ ì¡°íšŒ
        summary = db_manager.db_get_consultation_summary(session_id)
        print(f"ìƒë‹´ ìš”ì•½: {summary}")
        
        # ì—…ë¬´ ë¶„ì•¼ í†µê³„ ì¡°íšŒ
        stats = db_manager.db_get_business_area_statistics()
        print(f"ì—…ë¬´ ë¶„ì•¼ í†µê³„: {stats}")
        
    finally:
        db_manager.db_disconnect() 