# Standard library imports
import os
import json
import threading
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Annotated, List, Dict, Any, Optional
from pathlib import Path
import re
from dataclasses import dataclass
from collections import defaultdict
import logging

# Related third-party imports
import torch
import openai
from openai import OpenAI

# Local imports
from src.text.model import LanguageModelManager
from src.text.korean_models import KoreanModels

logger = logging.getLogger(__name__)

@dataclass
class QualityScore:
    score: float
    details: Dict[str, any]
    examples: List[str]

class KoreanPunctuationAnalyzer:
    """í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ì‚¬ìš© ê·œì¹™ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ê·œì¹™ ì‚¬ì „
        self.punctuation_rules = {
            'period': {
                'correct': ['ìš”', 'ë‹ˆë‹¤', 'ë‹¤', 'ì–´', 'ì•„', 'ë„¤', 'ì£ '],
                'incorrect': ['ìš”.', 'ë‹ˆë‹¤.', 'ë‹¤.', 'ì–´.', 'ì•„.', 'ë„¤.', 'ì£ .'],
                'score_weight': 0.3
            },
            'comma': {
                'correct_patterns': [
                    r'[ê°€-í£]+[ì€ëŠ”ì´ê°€ì„ë¥¼]?,',  # ëª…ì‚¬ + ì¡°ì‚¬ + ì‰¼í‘œ
                    r'[ê°€-í£]+[ê³ ë©°ë‚˜]',  # ì—°ê²°ì–´ë¯¸
                    r'[ê°€-í£]+[ëŠ”ë°]',  # ì¢…ê²°ì–´ë¯¸
                ],
                'incorrect_patterns': [
                    r'[ê°€-í£]+[ì€ëŠ”ì´ê°€ì„ë¥¼]?[ê³ ë©°ë‚˜],',  # ì¡°ì‚¬ + ì–´ë¯¸ + ì‰¼í‘œ
                    r'[ê°€-í£]+[ëŠ”ë°],',  # ì¢…ê²°ì–´ë¯¸ + ì‰¼í‘œ
                ],
                'score_weight': 0.2
            },
            'question': {
                'correct': ['ê¹Œ', 'ì£ ', 'ë‚˜', 'ë‹ˆ', 'ì–´', 'ì•„'],
                'incorrect': ['ê¹Œ?', 'ì£ ?', 'ë‚˜?', 'ë‹ˆ?', 'ì–´?', 'ì•„?'],
                'score_weight': 0.25
            },
            'exclamation': {
                'correct': ['ë„¤', 'ì–´', 'ì•„', 'ë‹¤'],
                'incorrect': ['ë„¤!', 'ì–´!', 'ì•„!', 'ë‹¤!'],
                'score_weight': 0.25
            }
        }
    
    def analyze_punctuation(self, text: str) -> QualityScore:
        """ë¬¸ì¥ ë¶€í˜¸ ì‚¬ìš© ê·œì¹™ ë¶„ì„"""
        total_score = 0
        total_weight = 0
        details = {}
        examples = []
        
        for rule_type, rules in self.punctuation_rules.items():
            score = 0
            rule_examples = []
            
            if rule_type == 'comma':
                score, rule_examples = self._analyze_comma_usage(text, rules)
            else:
                score, rule_examples = self._analyze_basic_punctuation(text, rules)
            
            details[rule_type] = {
                'score': score,
                'examples': rule_examples
            }
            examples.extend(rule_examples)
            total_score += score * rules['score_weight']
            total_weight += rules['score_weight']
        
        final_score = total_score / total_weight if total_weight > 0 else 0
        
        return QualityScore(
            score=final_score,
            details=details,
            examples=examples
        )
    
    def _analyze_basic_punctuation(self, text: str, rules: Dict) -> Tuple[float, List[str]]:
        """ê¸°ë³¸ ë¬¸ì¥ ë¶€í˜¸ ë¶„ì„"""
        correct_count = 0
        incorrect_count = 0
        examples = []
        
        for correct in rules['correct']:
            pattern = re.escape(correct)
            correct_count += len(re.findall(pattern, text))
        
        for incorrect in rules['incorrect']:
            pattern = re.escape(incorrect)
            matches = re.findall(pattern, text)
            incorrect_count += len(matches)
            if matches:
                examples.append(f"ì˜ëª»ëœ ì‚¬ìš©: {incorrect}")
        
        total = correct_count + incorrect_count
        score = correct_count / total if total > 0 else 1.0
        
        return score, examples
    
    def _analyze_comma_usage(self, text: str, rules: Dict) -> Tuple[float, List[str]]:
        """ì‰¼í‘œ ì‚¬ìš© ê·œì¹™ ë¶„ì„"""
        correct_count = 0
        incorrect_count = 0
        examples = []
        
        for pattern in rules['correct_patterns']:
            matches = re.findall(pattern, text)
            correct_count += len(matches)
        
        for pattern in rules['incorrect_patterns']:
            matches = re.findall(pattern, text)
            incorrect_count += len(matches)
            if matches:
                examples.append(f"ì˜ëª»ëœ ì‰¼í‘œ ì‚¬ìš©: {matches[:3]}")
        
        total = correct_count + incorrect_count
        score = correct_count / total if total > 0 else 1.0
        
        return score, examples

class KNUSentimentAnalyzer:
    """KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„ê¸°"""
    
    def __init__(self):
        # KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ë°ì´í„° (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
        self.positive_words = {
            # ê¸ì •ì  ê°ì •
            'ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ì™„ë²½í•˜ë‹¤', 'ìµœê³ ë‹¤', 'ìµœìƒì´ë‹¤', 'ì™„ë²½í•˜ë‹¤',
            'ë§Œì¡±í•˜ë‹¤', 'ê¸°ì˜ë‹¤', 'í–‰ë³µí•˜ë‹¤', 'ì¦ê²ë‹¤', 'ì‹ ë‚˜ë‹¤', 'ì„¤ë ˆë‹¤',
            'ê°ì‚¬í•˜ë‹¤', 'ê³ ë§™ë‹¤', 'ì‚¬ë‘í•˜ë‹¤', 'ì¢‹ì•„í•˜ë‹¤', 'ì¦ê¸°ë‹¤', 'ì¦ê²ë‹¤',
            'í¸í•˜ë‹¤', 'í¸ì•ˆí•˜ë‹¤', 'ì•ˆì „í•˜ë‹¤', 'ì•ˆì •ì ì´ë‹¤', 'ë¯¿ìŒì§í•˜ë‹¤',
            'ì •í™•í•˜ë‹¤', 'ì •ë°€í•˜ë‹¤', 'ì™„ë²½í•˜ë‹¤', 'ì™„ì„±ë„ê°€ ë†’ë‹¤', 'í’ˆì§ˆì´ ì¢‹ë‹¤',
            'íš¨ê³¼ì ì´ë‹¤', 'íš¨ìœ¨ì ì´ë‹¤', 'ë¹ ë¥´ë‹¤', 'ì •í™•í•˜ë‹¤', 'ì •ì‹œì—',
            'ì¹œì ˆí•˜ë‹¤', 'ë”°ëœ»í•˜ë‹¤', 'ê´€ëŒ€í•˜ë‹¤', 'ë„ˆê·¸ëŸ½ë‹¤', 'ì´í•´ì‹¬ì´ ë§ë‹¤',
            'ì „ë¬¸ì ì´ë‹¤', 'ì „ë¬¸ê°€ë‹¤', 'ëŠ¥ë ¥ì´ ìˆë‹¤', 'ì‹¤ë ¥ì´ ì¢‹ë‹¤', 'ê²½í—˜ì´ ë§ë‹¤',
            'í•´ê²°í•˜ë‹¤', 'ë„ì™€ì£¼ë‹¤', 'ì§€ì›í•˜ë‹¤', 'í˜‘ì¡°í•˜ë‹¤', 'í˜‘ë ¥í•˜ë‹¤',
            'ê°œì„ í•˜ë‹¤', 'í–¥ìƒì‹œí‚¤ë‹¤', 'ë°œì „ì‹œí‚¤ë‹¤', 'ì„±ì¥í•˜ë‹¤', 'ì§„ë³´í•˜ë‹¤',
            'í˜œíƒ', 'í• ì¸', 'í”„ë¡œëª¨ì…˜', 'ì´ë²¤íŠ¸', 'íŠ¹ê°€', 'íŠ¹ë³„ê°€', 'ë¬´ë£Œ',
            'ë³´ë„ˆìŠ¤', 'ì¶”ê°€', 'ì¦ì •', 'ì‚¬ì€í’ˆ', 'ì„ ë¬¼', 'ê²½í’ˆ', 'ë‹¹ì²¨',
            'ì„±ê³µ', 'ë‹¬ì„±', 'ì™„ë£Œ', 'ì²˜ë¦¬', 'í•´ê²°', 'í™•ì¸', 'ìŠ¹ì¸', 'í—ˆê°€',
            'ì •ìƒ', 'ì–‘í˜¸', 'ì¢‹ìŒ', 'ìš°ìˆ˜', 'ìµœê³ ', 'ìµœìƒ', 'ì™„ë²½', 'ì™„ì „'
        }
        
        self.negative_words = {
            # ë¶€ì •ì  ê°ì •
            'ë‚˜ì˜ë‹¤', 'ìµœì•…ì´ë‹¤', 'ë”ì°í•˜ë‹¤', 'ë¬´ì„­ë‹¤', 'ë‘ë µë‹¤', 'ê±±ì •ë˜ë‹¤',
            'ë¶ˆì•ˆí•˜ë‹¤', 'ê¸´ì¥í•˜ë‹¤', 'ìŠ¤íŠ¸ë ˆìŠ¤ë°›ë‹¤', 'ì§œì¦ë‚˜ë‹¤', 'í™”ë‚˜ë‹¤',
            'ë¶„ë…¸í•˜ë‹¤', 'í™”ê°€ ë‚˜ë‹¤', 'ì—´ë°›ë‹¤', 'ë¹¡ì¹˜ë‹¤', 'ì—´ë°›ë‹¤', 'í™”ê°€ ì¹˜ë°€ë‹¤',
            'ì‹¤ë§í•˜ë‹¤', 'ì ˆë§í•˜ë‹¤', 'ìš°ìš¸í•˜ë‹¤', 'ìŠ¬í”„ë‹¤', 'ìš°ìš¸í•˜ë‹¤',
            'ë‹µë‹µí•˜ë‹¤', 'ë‹µë‹µí•˜ë‹¤', 'ë‹µë‹µí•˜ë‹¤', 'ë‹µë‹µí•˜ë‹¤', 'ë‹µë‹µí•˜ë‹¤',
            'í˜ë“¤ë‹¤', 'ì–´ë µë‹¤', 'ë³µì¡í•˜ë‹¤', 'ì–´ë ¤ì›€', 'ë¬¸ì œ', 'ì¥ì• ', 'ì˜¤ë¥˜',
            'ê³ ì¥', 'ì°¨ë‹¨', 'í•´ì§€', 'íì§€', 'ì¤‘ë‹¨', 'ì§€ì—°', 'ì§€ì²´', 'ëŠ¦ë‹¤',
            'ë¶ˆí¸í•˜ë‹¤', 'ë¶ˆí¸í•˜ë‹¤', 'ë¶ˆí¸í•˜ë‹¤', 'ë¶ˆí¸í•˜ë‹¤', 'ë¶ˆí¸í•˜ë‹¤',
            'ë¶ˆë§Œ', 'ë¶ˆí‰', 'í•­ì˜', 'ë¯¼ì›', 'ë¶ˆë§Œ', 'ë¶ˆí‰', 'í•­ì˜', 'ë¯¼ì›',
            'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨', 'ì‹¤íŒ¨',
            'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜', 'ì˜¤ë¥˜',
            'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ', 'ì¥ì• ',
            'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰', 'ë¶ˆëŸ‰',
            'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨', 'ì°¨ë‹¨',
            'í•´ì§€', 'í•´ì§€', 'í•´ì§€', 'í•´ì§€', 'í•´ì§€', 'í•´ì§€', 'í•´ì§€', 'í•´ì§€',
            'íì§€', 'íì§€', 'íì§€', 'íì§€', 'íì§€', 'íì§€', 'íì§€', 'íì§€',
            'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨', 'ì¤‘ë‹¨',
            'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°', 'ì§€ì—°',
            'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´', 'ì§€ì²´',
            'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤', 'ëŠ¦ë‹¤'
        }
        
        # ê°ì • ê°•ë„ ì‚¬ì „
        self.emotion_intensity = {
            # ê¸ì • ê°•ë„
            'ì¢‹ë‹¤': 1, 'í›Œë¥­í•˜ë‹¤': 2, 'ì™„ë²½í•˜ë‹¤': 3, 'ìµœê³ ë‹¤': 3, 'ìµœìƒì´ë‹¤': 3,
            'ë§Œì¡±í•˜ë‹¤': 2, 'ê¸°ì˜ë‹¤': 2, 'í–‰ë³µí•˜ë‹¤': 3, 'ì¦ê²ë‹¤': 2, 'ì‹ ë‚˜ë‹¤': 2,
            'ê°ì‚¬í•˜ë‹¤': 2, 'ê³ ë§™ë‹¤': 2, 'ì‚¬ë‘í•˜ë‹¤': 3, 'ì¢‹ì•„í•˜ë‹¤': 1,
            'í¸í•˜ë‹¤': 1, 'í¸ì•ˆí•˜ë‹¤': 2, 'ì•ˆì „í•˜ë‹¤': 2, 'ì•ˆì •ì ì´ë‹¤': 2,
            'ì •í™•í•˜ë‹¤': 2, 'ì •ë°€í•˜ë‹¤': 2, 'íš¨ê³¼ì ì´ë‹¤': 2, 'íš¨ìœ¨ì ì´ë‹¤': 2,
            'ì¹œì ˆí•˜ë‹¤': 2, 'ë”°ëœ»í•˜ë‹¤': 2, 'ì „ë¬¸ì ì´ë‹¤': 2, 'ëŠ¥ë ¥ì´ ìˆë‹¤': 2,
            'í•´ê²°í•˜ë‹¤': 2, 'ë„ì™€ì£¼ë‹¤': 2, 'ê°œì„ í•˜ë‹¤': 2, 'í–¥ìƒì‹œí‚¤ë‹¤': 2,
            
            # ë¶€ì • ê°•ë„
            'ë‚˜ì˜ë‹¤': 1, 'ìµœì•…ì´ë‹¤': 3, 'ë”ì°í•˜ë‹¤': 3, 'ë¬´ì„­ë‹¤': 2, 'ë‘ë µë‹¤': 2,
            'ê±±ì •ë˜ë‹¤': 1, 'ë¶ˆì•ˆí•˜ë‹¤': 2, 'ìŠ¤íŠ¸ë ˆìŠ¤ë°›ë‹¤': 2, 'ì§œì¦ë‚˜ë‹¤': 2,
            'í™”ë‚˜ë‹¤': 2, 'ë¶„ë…¸í•˜ë‹¤': 3, 'ì—´ë°›ë‹¤': 2, 'ì‹¤ë§í•˜ë‹¤': 2, 'ì ˆë§í•˜ë‹¤': 3,
            'ìš°ìš¸í•˜ë‹¤': 2, 'ìŠ¬í”„ë‹¤': 2, 'ë‹µë‹µí•˜ë‹¤': 1, 'í˜ë“¤ë‹¤': 1, 'ì–´ë µë‹¤': 1,
            'ë³µì¡í•˜ë‹¤': 1, 'ë¶ˆí¸í•˜ë‹¤': 1, 'ë¶ˆë§Œ': 1, 'ì‹¤íŒ¨': 2, 'ì˜¤ë¥˜': 1,
            'ì¥ì• ': 2, 'ë¶ˆëŸ‰': 2, 'ì°¨ë‹¨': 2, 'í•´ì§€': 2, 'íì§€': 2, 'ì¤‘ë‹¨': 2,
            'ì§€ì—°': 1, 'ì§€ì²´': 1, 'ëŠ¦ë‹¤': 1
        }
    
    def analyze_sentiment(self, text: str) -> QualityScore:
        """KNU ê°ì„±ì‚¬ì „ ê¸°ë°˜ ê°ì„± ë¶„ì„"""
        positive_count = 0
        negative_count = 0
        positive_words_found = []
        negative_words_found = []
        positive_intensity = 0
        negative_intensity = 0
        
        # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
        words = re.findall(r'[ê°€-í£]+', text)
        total_words = len(words)
        
        if total_words == 0:
            return QualityScore(
                score=0.5,
                details={'positive_ratio': 0, 'negative_ratio': 0, 'sentiment_score': 0.5},
                examples=[]
            )
        
        # ê¸ì •/ë¶€ì • ë‹¨ì–´ ì¹´ìš´íŠ¸
        for word in words:
            if word in self.positive_words:
                positive_count += 1
                positive_words_found.append(word)
                positive_intensity += self.emotion_intensity.get(word, 1)
            
            if word in self.negative_words:
                negative_count += 1
                negative_words_found.append(word)
                negative_intensity += self.emotion_intensity.get(word, 1)
        
        # ë¹„ìœ¨ ê³„ì‚°
        positive_ratio = positive_count / total_words
        negative_ratio = negative_count / total_words
        
        # ê°ì„± ì ìˆ˜ ê³„ì‚° (ê¸ì • ë¹„ìœ¨ - ë¶€ì • ë¹„ìœ¨, 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”)
        sentiment_score = (positive_ratio - negative_ratio + 1) / 2
        
        # ê°•ë„ ê°€ì¤‘ì¹˜ ì ìš©
        intensity_score = 0.5
        if positive_intensity > 0 or negative_intensity > 0:
            intensity_score = positive_intensity / (positive_intensity + negative_intensity)
        
        # ìµœì¢… ì ìˆ˜ (ê°ì„± ì ìˆ˜ì™€ ê°•ë„ ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê· )
        final_score = sentiment_score * 0.7 + intensity_score * 0.3
        
        details = {
            'positive_count': positive_count,
            'negative_count': negative_count,
            'positive_ratio': positive_ratio,
            'negative_ratio': negative_ratio,
            'sentiment_score': sentiment_score,
            'positive_intensity': positive_intensity,
            'negative_intensity': negative_intensity,
            'intensity_score': intensity_score,
            'total_words': total_words
        }
        
        examples = []
        if positive_words_found:
            examples.append(f"ê¸ì • ë‹¨ì–´: {', '.join(set(positive_words_found[:5]))}")
        if negative_words_found:
            examples.append(f"ë¶€ì • ë‹¨ì–´: {', '.join(set(negative_words_found[:5]))}")
        
        return QualityScore(
            score=final_score,
            details=details,
            examples=examples
        )

class CommunicationQualityAnalyzer:
    """í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€ì˜ ì˜ì‚¬ì†Œí†µ í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # KNU ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.knu_analyzer = KNUSentimentAnalyzer()
        
        # ì¡´ëŒ“ë§ ì‚¬ìš© íŒ¨í„´
        self.polite_patterns = {
            'formal_endings': [
                r'[ê°€-í£]+ìŠµë‹ˆë‹¤', r'[ê°€-í£]+ë‹ˆë‹¤', r'[ê°€-í£]+ìš”',
                r'[ê°€-í£]+ê² ìŠµë‹ˆë‹¤', r'[ê°€-í£]+ì‹œê² ìŠµë‹ˆë‹¤'
            ],
            'honorific_verbs': [
                'ë“œë¦¬ë‹¤', 'í•´ë“œë¦¬ë‹¤', 'ë§ì”€ë“œë¦¬ë‹¤', 'ì•ˆë‚´ë“œë¦¬ë‹¤',
                'ë„ì™€ë“œë¦¬ë‹¤', 'í™•ì¸í•´ë“œë¦¬ë‹¤', 'ì—°ê²°í•´ë“œë¦¬ë‹¤'
            ],
            'honorific_nouns': [
                'ê³ ê°ë‹˜', 'ì†ë‹˜', 'ì„ ìƒë‹˜', 'ê³ ê°ë¶„', 'ê³ ê°'
            ],
            'polite_expressions': [
                'ì£„ì†¡í•©ë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ë¶€íƒë“œë¦½ë‹ˆë‹¤', 'ì•Œê² ìŠµë‹ˆë‹¤',
                'ì´í•´í–ˆìŠµë‹ˆë‹¤', 'í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤', 'ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤'
            ]
        }
        
        # ë¶€ì •ì  í‘œí˜„ íŒ¨í„´ (KNU ì‚¬ì „ê³¼ ì—°ë™)
        self.negative_patterns = {
            'direct_negative': [
                'ì•ˆ ë©ë‹ˆë‹¤', 'ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤', 'í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 'ì•ˆ ë˜ê² ìŠµë‹ˆë‹¤',
                'ëª»í•©ë‹ˆë‹¤', 'ì–´ë µìŠµë‹ˆë‹¤', 'ë¶ˆê°€í•©ë‹ˆë‹¤'
            ],
            'negative_words': [
                'ë¬¸ì œ', 'ì˜¤ë¥˜', 'ì¥ì• ', 'ë¶ˆëŸ‰', 'ê³ ì¥', 'ì°¨ë‹¨', 'í•´ì§€',
                'íì§€', 'ì¤‘ë‹¨', 'ì§€ì—°', 'ì˜¤ë¥˜', 'ì‹¤íŒ¨'
            ],
            'negative_emotions': [
                'ì§œì¦', 'í™”ë‚¨', 'ë¶ˆë§Œ', 'ë‹µë‹µ', 'í˜ë“¦', 'ì–´ë ¤ì›€', 'ë³µì¡'
            ]
        }
        
        # ê³µê° í‘œí˜„ íŒ¨í„´
        self.empathy_patterns = {
            'understanding': [
                'ì´í•´í•©ë‹ˆë‹¤', 'ì•Œê² ìŠµë‹ˆë‹¤', 'ë§ì”€í•˜ì‹  ëŒ€ë¡œ', 'ê·¸ë ‡êµ°ìš”',
                'ì¶©ë¶„íˆ ì´í•´ë©ë‹ˆë‹¤', 'ê³ ë¯¼ì´ ë˜ì‹œê² ë„¤ìš”'
            ],
            'emotional_support': [
                'í˜ë“œì…¨ê² ì–´ìš”', 'ë‹µë‹µí•˜ì…¨ê² ì–´ìš”', 'ë¶ˆí¸í•˜ì…¨ê² ì–´ìš”',
                'ê±±ì •ë˜ì…¨ê² ì–´ìš”', 'ê´´ë¡œìš°ì…¨ê² ì–´ìš”'
            ],
            'positive_reinforcement': [
                'ì˜ í•˜ì…¨ìŠµë‹ˆë‹¤', 'ì •ë§ ì¢‹ìŠµë‹ˆë‹¤', 'í›Œë¥­í•©ë‹ˆë‹¤',
                'ë§ìŠµë‹ˆë‹¤', 'ì •í™•í•©ë‹ˆë‹¤'
            ]
        }
        
        # ì „ë¬¸ì„± í‘œí˜„ íŒ¨í„´
        self.expertise_patterns = {
            'technical_terms': [
                'ë°ì´í„°', 'í†µí™”ëŸ‰', 'ìš”ê¸ˆì œ', 'í• ì¸', 'í˜œíƒ', 'í”„ë¡œëª¨ì…˜',
                'ì •ì±…', 'ê·œì •', 'ì ˆì°¨', 'ë°©ë²•', 'í•´ê²°ì±…'
            ],
            'precise_explanations': [
                'êµ¬ì²´ì ìœ¼ë¡œ', 'ì •í™•íˆ', 'ìƒì„¸íˆ', 'ìì„¸íˆ', 'ëª…í™•íˆ',
                'ë‹¨ê³„ë³„ë¡œ', 'ìˆœì„œëŒ€ë¡œ', 'ë°©ë²•ì€'
            ],
            'solution_oriented': [
                'í•´ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤', 'ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤', 'ë°©ë²•ì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤',
                'í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤', 'ì²˜ë¦¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤'
            ]
        }
        
        # êµ¬ì²´ì  ì •ë³´ ì œê³µ íŒ¨í„´
        self.specific_info_patterns = {
            'numbers': [
                r'\d+ì›', r'\d+%', r'\d+ì¼', r'\d+ì‹œê°„', r'\d+ë¶„',
                r'\d+ê°œ', r'\d+íšŒ', r'\d+ë²ˆ'
            ],
            'time_specific': [
                'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ ì£¼', 'ë‹¤ìŒ ì£¼', 'ì´ë²ˆ ë‹¬', 'ë‹¤ìŒ ë‹¬',
                '3ì¼ í›„', '1ì£¼ì¼ í›„', '1ê°œì›” í›„'
            ],
            'process_steps': [
                'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', '1ë‹¨ê³„', '2ë‹¨ê³„', '3ë‹¨ê³„',
                'ë¨¼ì €', 'ê·¸ ë‹¤ìŒ', 'ë§ˆì§€ë§‰ìœ¼ë¡œ'
            ]
        }
        
        # ë¬¸ì¥ ë¶€í˜¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.punctuation_analyzer = KoreanPunctuationAnalyzer()
    
    def analyze_communication_quality(self, text: str) -> Dict[str, QualityScore]:
        """í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€ì˜ ì˜ì‚¬ì†Œí†µ í’ˆì§ˆ ì¢…í•© ë¶„ì„"""
        results = {}
        
        # 1. ì¡´ëŒ“ë§ ì‚¬ìš© ë¶„ì„
        results['politeness'] = self._analyze_politeness(text)
        
        # 2. ë¶€ì •ì  í‘œí˜„ ë¶„ì„ (KNU ê°ì„± ë¶„ì„ê³¼ ì—°ë™)
        results['negative_expression'] = self._analyze_negative_expressions(text)
        
        # 3. ê³µê° í‘œí˜„ ë¶„ì„
        results['empathy'] = self._analyze_empathy(text)
        
        # 4. ì „ë¬¸ì„± ë¶„ì„
        results['expertise'] = self._analyze_expertise(text)
        
        # 5. êµ¬ì²´ì  ì •ë³´ ì œê³µ ë¶„ì„
        results['specific_info'] = self._analyze_specific_info(text)
        
        # 6. ë¬¸ì¥ ë¶€í˜¸ ì‚¬ìš© ë¶„ì„
        results['punctuation'] = self.punctuation_analyzer.analyze_punctuation(text)
        
        # 7. KNU ê°ì„± ë¶„ì„
        results['sentiment'] = self.knu_analyzer.analyze_sentiment(text)
        
        return results
    
    def _analyze_politeness(self, text: str) -> QualityScore:
        """ì¡´ëŒ“ë§ ì‚¬ìš© ë¶„ì„"""
        total_score = 0
        total_weight = 0
        details = {}
        examples = []
        
        # ê³µì‹ ì¢…ê²°ì–´ë¯¸ ì‚¬ìš©
        formal_count = 0
        for pattern in self.polite_patterns['formal_endings']:
            formal_count += len(re.findall(pattern, text))
        
        # ê²½ì–´ ë™ì‚¬ ì‚¬ìš©
        honorific_verb_count = 0
        for verb in self.polite_patterns['honorific_verbs']:
            honorific_verb_count += text.count(verb)
        
        # ê²½ì–´ ëª…ì‚¬ ì‚¬ìš©
        honorific_noun_count = 0
        for noun in self.polite_patterns['honorific_nouns']:
            honorific_noun_count += text.count(noun)
        
        # ê³µì†í•œ í‘œí˜„ ì‚¬ìš©
        polite_expression_count = 0
        for expression in self.polite_patterns['polite_expressions']:
            polite_expression_count += text.count(expression)
        
        # ì „ì²´ ë¬¸ì¥ ìˆ˜ ì¶”ì • (ë§ˆì¹¨í‘œ ê¸°ì¤€)
        total_sentences = len(re.findall(r'[.!?]', text)) + 1
        
        # ì ìˆ˜ ê³„ì‚°
        formal_score = min(formal_count / total_sentences * 2, 1.0) if total_sentences > 0 else 0
        honorific_score = min((honorific_verb_count + honorific_noun_count) / total_sentences, 1.0) if total_sentences > 0 else 0
        polite_score = min(polite_expression_count / total_sentences, 1.0) if total_sentences > 0 else 0
        
        # ê°€ì¤‘ í‰ê· 
        final_score = (formal_score * 0.4 + honorific_score * 0.4 + polite_score * 0.2)
        
        details = {
            'formal_endings': formal_score,
            'honorific_usage': honorific_score,
            'polite_expressions': polite_score
        }
        
        if formal_count > 0:
            examples.append(f"ê³µì‹ ì¢…ê²°ì–´ë¯¸ ì‚¬ìš©: {formal_count}íšŒ")
        if honorific_verb_count > 0:
            examples.append(f"ê²½ì–´ ë™ì‚¬ ì‚¬ìš©: {honorific_verb_count}íšŒ")
        if polite_expression_count > 0:
            examples.append(f"ê³µì†í•œ í‘œí˜„ ì‚¬ìš©: {polite_expression_count}íšŒ")
        
        return QualityScore(score=final_score, details=details, examples=examples)
    
    def _analyze_negative_expressions(self, text: str) -> QualityScore:
        """ë¶€ì •ì  í‘œí˜„ ë¶„ì„ (KNU ê°ì„± ë¶„ì„ê³¼ ì—°ë™)"""
        # KNU ê°ì„± ë¶„ì„ ê²°ê³¼ í™œìš©
        knu_result = self.knu_analyzer.analyze_sentiment(text)
        knu_negative_ratio = knu_result.details.get('negative_ratio', 0)
        knu_negative_intensity = knu_result.details.get('negative_intensity', 0)
        
        # ê¸°ì¡´ íŒ¨í„´ ê¸°ë°˜ ë¶„ì„
        pattern_negative_count = 0
        examples = []
        
        # ì§ì ‘ì  ë¶€ì • í‘œí˜„
        for expression in self.negative_patterns['direct_negative']:
            count = text.count(expression)
            pattern_negative_count += count
            if count > 0:
                examples.append(f"ì§ì ‘ì  ë¶€ì •: {expression}")
        
        # ë¶€ì •ì  ë‹¨ì–´
        for word in self.negative_patterns['negative_words']:
            count = text.count(word)
            pattern_negative_count += count * 0.5  # ê°€ì¤‘ì¹˜ ì ìš©
            if count > 0:
                examples.append(f"ë¶€ì •ì  ë‹¨ì–´: {word}")
        
        # ë¶€ì •ì  ê°ì • í‘œí˜„
        for emotion in self.negative_patterns['negative_emotions']:
            count = text.count(emotion)
            pattern_negative_count += count * 0.3
            if count > 0:
                examples.append(f"ë¶€ì •ì  ê°ì •: {emotion}")
        
        # KNU ë¶„ì„ ê²°ê³¼ì™€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ê²°í•©
        total_words = len(text.split())
        pattern_negative_ratio = pattern_negative_count / total_words if total_words > 0 else 0
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ë¶€ì • ë¹„ìœ¨ ê³„ì‚°
        final_negative_ratio = (knu_negative_ratio * 0.6 + pattern_negative_ratio * 0.4)
        
        # ì ìˆ˜ ê³„ì‚° (ë¶€ì •ì  í‘œí˜„ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        score = max(0, 1 - final_negative_ratio * 8)  # ë¶€ì •ì  í‘œí˜„ ë¹„ìœ¨ì— ë”°ë¥¸ ê°ì 
        
        # KNU ê°ì • ê°•ë„ ë°˜ì˜
        if knu_negative_intensity > 0:
            intensity_penalty = min(0.2, knu_negative_intensity * 0.05)
            score = max(0, score - intensity_penalty)
        
        details = {
            'knu_negative_ratio': knu_negative_ratio,
            'pattern_negative_ratio': pattern_negative_ratio,
            'final_negative_ratio': final_negative_ratio,
            'knu_negative_intensity': knu_negative_intensity,
            'pattern_negative_count': pattern_negative_count,
            'total_words': total_words,
            'intensity_penalty': min(0.2, knu_negative_intensity * 0.05) if knu_negative_intensity > 0 else 0
        }
        
        # KNUì—ì„œ ë°œê²¬ëœ ë¶€ì • ë‹¨ì–´ë„ ì˜ˆì‹œì— ì¶”ê°€
        knu_examples = knu_result.examples
        examples.extend([ex for ex in knu_examples if 'ë¶€ì • ë‹¨ì–´' in ex])
        
        return QualityScore(score=score, details=details, examples=examples)
    
    def _analyze_empathy(self, text: str) -> QualityScore:
        """ê³µê° í‘œí˜„ ë¶„ì„"""
        empathy_count = 0
        examples = []
        
        # ì´í•´ í‘œí˜„
        for expression in self.empathy_patterns['understanding']:
            count = text.count(expression)
            empathy_count += count
            if count > 0:
                examples.append(f"ì´í•´ í‘œí˜„: {expression}")
        
        # ê°ì •ì  ì§€ì§€
        for expression in self.empathy_patterns['emotional_support']:
            count = text.count(expression)
            empathy_count += count * 1.5  # ë” ë†’ì€ ê°€ì¤‘ì¹˜
            if count > 0:
                examples.append(f"ê°ì •ì  ì§€ì§€: {expression}")
        
        # ê¸ì •ì  ê°•í™”
        for expression in self.empathy_patterns['positive_reinforcement']:
            count = text.count(expression)
            empathy_count += count
            if count > 0:
                examples.append(f"ê¸ì •ì  ê°•í™”: {expression}")
        
        # ì ìˆ˜ ê³„ì‚°
        total_sentences = len(re.findall(r'[.!?]', text)) + 1
        empathy_ratio = empathy_count / total_sentences if total_sentences > 0 else 0
        score = min(empathy_ratio * 2, 1.0)  # ì ì ˆí•œ ê³µê° í‘œí˜„ ë¹„ìœ¨
        
        details = {
            'empathy_count': empathy_count,
            'empathy_ratio': empathy_ratio,
            'total_sentences': total_sentences
        }
        
        return QualityScore(score=score, details=details, examples=examples)
    
    def _analyze_expertise(self, text: str) -> QualityScore:
        """ì „ë¬¸ì„± ë¶„ì„"""
        expertise_count = 0
        examples = []
        
        # ì „ë¬¸ ìš©ì–´ ì‚¬ìš©
        for term in self.expertise_patterns['technical_terms']:
            count = text.count(term)
            expertise_count += count
            if count > 0:
                examples.append(f"ì „ë¬¸ ìš©ì–´: {term}")
        
        # ì •í™•í•œ ì„¤ëª…
        for expression in self.expertise_patterns['precise_explanations']:
            count = text.count(expression)
            expertise_count += count * 1.2
            if count > 0:
                examples.append(f"ì •í™•í•œ ì„¤ëª…: {expression}")
        
        # í•´ê²°ì±… ì œì‹œ
        for expression in self.expertise_patterns['solution_oriented']:
            count = text.count(expression)
            expertise_count += count * 1.5
            if count > 0:
                examples.append(f"í•´ê²°ì±… ì œì‹œ: {expression}")
        
        # ì ìˆ˜ ê³„ì‚°
        total_words = len(text.split())
        expertise_ratio = expertise_count / total_words if total_words > 0 else 0
        score = min(expertise_ratio * 5, 1.0)  # ì „ë¬¸ì„± í‘œí˜„ ë¹„ìœ¨
        
        details = {
            'expertise_count': expertise_count,
            'expertise_ratio': expertise_ratio,
            'total_words': total_words
        }
        
        return QualityScore(score=score, details=details, examples=examples)
    
    def _analyze_specific_info(self, text: str) -> QualityScore:
        """êµ¬ì²´ì  ì •ë³´ ì œê³µ ë¶„ì„"""
        specific_count = 0
        examples = []
        
        # ìˆ«ì ì •ë³´
        for pattern in self.specific_info_patterns['numbers']:
            matches = re.findall(pattern, text)
            specific_count += len(matches)
            if matches:
                examples.append(f"ìˆ«ì ì •ë³´: {matches[:3]}")
        
        # ì‹œê°„ ì •ë³´
        for time_expr in self.specific_info_patterns['time_specific']:
            count = text.count(time_expr)
            specific_count += count
            if count > 0:
                examples.append(f"ì‹œê°„ ì •ë³´: {time_expr}")
        
        # ë‹¨ê³„ë³„ ì„¤ëª…
        for step_expr in self.specific_info_patterns['process_steps']:
            count = text.count(step_expr)
            specific_count += count * 1.5
            if count > 0:
                examples.append(f"ë‹¨ê³„ë³„ ì„¤ëª…: {step_expr}")
        
        # ì ìˆ˜ ê³„ì‚°
        total_sentences = len(re.findall(r'[.!?]', text)) + 1
        specific_ratio = specific_count / total_sentences if total_sentences > 0 else 0
        score = min(specific_ratio * 1.5, 1.0)
        
        details = {
            'specific_count': specific_count,
            'specific_ratio': specific_ratio,
            'total_sentences': total_sentences
        }
        
        return QualityScore(score=score, details=details, examples=examples)

def analyze_communication_quality_advanced(text: str) -> Dict[str, any]:
    """ê³ ê¸‰ ì˜ì‚¬ì†Œí†µ í’ˆì§ˆ ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
    analyzer = CommunicationQualityAnalyzer()
    results = analyzer.analyze_communication_quality(text)
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (KNU ê°ì„± ë¶„ì„ í¬í•¨)
    weights = {
        'politeness': 0.20,
        'negative_expression': 0.15,
        'empathy': 0.15,
        'expertise': 0.15,
        'specific_info': 0.10,
        'punctuation': 0.05,
        'sentiment': 0.20  # KNU ê°ì„± ë¶„ì„ ì¶”ê°€
    }
    
    total_score = 0
    for category, weight in weights.items():
        if category in results:
            total_score += results[category].score * weight
    
    # ê²°ê³¼ ì •ë¦¬
    analysis_result = {
        'overall_score': total_score,
        'category_scores': {},
        'detailed_analysis': {},
        'recommendations': []
    }
    
    for category, result in results.items():
        analysis_result['category_scores'][category] = result.score
        analysis_result['detailed_analysis'][category] = {
            'score': result.score,
            'details': result.details,
            'examples': result.examples
        }
    
    # ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„± (KNU ê°ì„± ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
    recommendations = []
    
    if results['politeness'].score < 0.7:
        recommendations.append("ì¡´ëŒ“ë§ ì‚¬ìš©ì„ ë” ì ê·¹ì ìœ¼ë¡œ í•˜ì„¸ìš”. '-ìŠµë‹ˆë‹¤', '-ë‹ˆë‹¤', '-ìš”' ì¢…ê²°ì–´ë¯¸ë¥¼ í™œìš©í•˜ì„¸ìš”.")
    
    if results['negative_expression'].score < 0.8:
        recommendations.append("ë¶€ì •ì  í‘œí˜„ì„ ì¤„ì´ê³  ê¸ì •ì  ëŒ€ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.")
    
    if results['empathy'].score < 0.6:
        recommendations.append("ê³ ê°ì˜ ê°ì •ì— ê³µê°í•˜ëŠ” í‘œí˜„ì„ ë” ì‚¬ìš©í•˜ì„¸ìš”.")
    
    if results['expertise'].score < 0.6:
        recommendations.append("ì „ë¬¸ ìš©ì–´ì™€ ì •í™•í•œ ì„¤ëª…ì„ ë” í™œìš©í•˜ì„¸ìš”.")
    
    if results['specific_info'].score < 0.5:
        recommendations.append("êµ¬ì²´ì ì¸ ìˆ«ì, ì‹œê°„, ë‹¨ê³„ë³„ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.")
    
    if results['punctuation'].score < 0.8:
        recommendations.append("í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.")
    
    # KNU ê°ì„± ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
    sentiment_result = results.get('sentiment', QualityScore(0.5, {}, []))
    sentiment_details = sentiment_result.details
    
    if sentiment_details.get('negative_ratio', 0) > 0.1:
        recommendations.append("ë¶€ì •ì  ë‹¨ì–´ ì‚¬ìš©ì„ ì¤„ì´ê³  ê¸ì •ì  í‘œí˜„ì„ ëŠ˜ë¦¬ì„¸ìš”.")
    
    if sentiment_details.get('positive_ratio', 0) < 0.05:
        recommendations.append("ê¸ì •ì  ë‹¨ì–´ì™€ í‘œí˜„ì„ ë” ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    if sentiment_details.get('negative_intensity', 0) > 5:
        recommendations.append("ê°•í•œ ë¶€ì •ì  ê°ì • í‘œí˜„ì„ ì¤„ì´ê³  ì¤‘ë¦½ì  í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    analysis_result['recommendations'] = recommendations
    
    return analysis_result

class AdvancedAnalysisManager:
    """
    ê³ ì„±ëŠ¥ ë¶„ì„ ê´€ë¦¬ í´ë˜ìŠ¤
    ìºì‹±, LLM í˜¸ì¶œ ë³‘ë ¬í™”, í’ˆì§ˆì§€í‘œ ì„¸ë¶„í™” ì§€ì›
    """
    
    def __init__(self, 
                 config_path: str = "config/config.yaml",
                 cache_dir: str = "/app/.cache/analysis",
                 max_workers: int = 4,
                 enable_cache: bool = True,
                 enable_parallel: bool = True):
        """
        AdvancedAnalysisManager ì´ˆê¸°í™”
        
        Parameters
        ----------
        config_path : str
            ì„¤ì • íŒŒì¼ ê²½ë¡œ
        cache_dir : str
            ìºì‹œ ë””ë ‰í† ë¦¬
        max_workers : int
            ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
        enable_cache : bool
            ìºì‹œ í™œì„±í™” ì—¬ë¶€
        enable_parallel : bool
            ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        """
        self.config_path = config_path
        self.cache_dir = Path(cache_dir)
        self.max_workers = max_workers
        self.enable_cache = enable_cache
        self.enable_parallel = enable_parallel
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        if self.enable_cache:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ìºì‹œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
        self.cache_metadata_file = self.cache_dir / "metadata.json"
        self.cache_metadata = self._load_cache_metadata()
        self.cache_lock = threading.Lock()
        
        # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.llm_manager = LanguageModelManager(config_path)
        self.korean_models = KoreanModels()
        
        # ë³‘ë ¬ ì²˜ë¦¬ executor
        if self.enable_parallel:
            self.executor = ThreadPoolExecutor(max_workers=max_workers)
        else:
            self.executor = None
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            "total_analyses": 0,
            "cache_hits": 0,
            "parallel_analyses": 0,
            "avg_processing_time": 0.0
        }
        
        print(f"âœ… AdvancedAnalysisManager ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cache_metadata(self) -> Dict[str, Any]:
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            if self.cache_metadata_file.exists():
                with open(self.cache_metadata_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}
    
    def _save_cache_metadata(self):
        """ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            with open(self.cache_metadata_file, 'w') as f:
                json.dump(self.cache_metadata, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _get_cache_key(self, analysis_type: str, content: str, params: Dict[str, Any] = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        import hashlib
        
        # ë¶„ì„ íƒ€ì…ê³¼ ë‚´ìš©ìœ¼ë¡œ í‚¤ ìƒì„±
        key_content = f"{analysis_type}_{content}"
        if params:
            key_content += f"_{json.dumps(params, sort_keys=True)}"
        
        content_hash = hashlib.md5(key_content.encode('utf-8')).hexdigest()
        return f"{content_hash}_{analysis_type}"
    
    def _is_cached(self, analysis_type: str, content: str, params: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """ìºì‹œëœ ê²°ê³¼ í™•ì¸"""
        if not self.enable_cache:
            return None
        
        cache_key = self._get_cache_key(analysis_type, content, params)
        
        with self.cache_lock:
            if cache_key in self.cache_metadata:
                cache_info = self.cache_metadata[cache_key]
                cache_path = self.cache_dir / cache_info["filename"]
                
                if cache_path.exists() and os.path.getsize(cache_path) > 0:
                    return cache_info
        
        return None
    
    def _save_to_cache(self, analysis_type: str, content: str, result: Dict[str, Any], params: Dict[str, Any] = None):
        """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.enable_cache:
            return
        
        try:
            cache_key = self._get_cache_key(analysis_type, content, params)
            cache_filename = f"{cache_key}.json"
            cache_path = self.cache_dir / cache_filename
            
            # ê²°ê³¼ ì €ì¥
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            with self.cache_lock:
                self.cache_metadata[cache_key] = {
                    "filename": cache_filename,
                    "analysis_type": analysis_type,
                    "created_at": time.time(),
                    "file_size": os.path.getsize(cache_path)
                }
                self._save_cache_metadata()
            
            print(f"ğŸ’¾ ë¶„ì„ ìºì‹œ ì €ì¥: {cache_filename}")
            
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _load_from_cache(self, cache_info: Dict[str, Any]) -> Dict[str, Any]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë“œ"""
        try:
            cache_path = self.cache_dir / cache_info["filename"]
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _analyze_sentiment_parallel(self, text: str) -> Dict[str, Any]:
        """ë³‘ë ¬ ê°ì • ë¶„ì„"""
        try:
            result = await self.korean_models.analyze_sentiment_with_api(text)
            return {
                "sentiment": result,
                "confidence": 0.8,
                "method": "api"
            }
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "sentiment": "ì¤‘ë¦½",
                "confidence": 0.5,
                "method": "fallback"
            }
    
    async def _analyze_profanity_parallel(self, text: str) -> Dict[str, Any]:
        """ë³‘ë ¬ ë¹„ì†ì–´ ê°ì§€"""
        try:
            result = await self.korean_models.detect_profanity_with_api(text)
            return {
                "has_profanity": result,
                "confidence": 0.8,
                "method": "api"
            }
        except Exception as e:
            print(f"âš ï¸ ë¹„ì†ì–´ ê°ì§€ ì‹¤íŒ¨: {e}")
            return {
                "has_profanity": False,
                "confidence": 0.5,
                "method": "fallback"
            }
    
    async def _analyze_speaker_classification_parallel(self, text: str) -> Dict[str, Any]:
        """ë³‘ë ¬ í™”ì ë¶„ë¥˜"""
        try:
            result = await self.korean_models.classify_speaker_with_api(text)
            return {
                "speaker_type": result,
                "confidence": 0.8,
                "method": "api"
            }
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {
                "speaker_type": "ê³ ê°",
                "confidence": 0.5,
                "method": "fallback"
            }
    
    async def _analyze_communication_quality_parallel(self, text: str) -> Dict[str, Any]:
        """ë³‘ë ¬ ì˜ì‚¬ì†Œí†µ í’ˆì§ˆ ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        try:
            # ìºì‹œ í™•ì¸
            cached_info = self._is_cached("communication_quality", text)
            if cached_info:
                return self._load_from_cache(cached_info)
            
            # ìƒˆë¡œìš´ ê³ ê¸‰ ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš©
            analysis_result = analyze_communication_quality_advanced(text)
            analysis_result["method"] = "parallel_advanced"
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache("communication_quality", text, analysis_result)
            
            return analysis_result
            
        except Exception as e:
            print(f"âš ï¸ ë³‘ë ¬ ì˜ì‚¬ì†Œí†µ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "method": "fallback"
            }
    
    def _analyze_clarity(self, text: str) -> float:
        """ëª…í™•ì„± ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        analyzer = CommunicationQualityAnalyzer()
        results = analyzer.analyze_communication_quality(text)
        
        # ëª…í™•ì„±ì€ ì „ë¬¸ì„±ê³¼ êµ¬ì²´ì  ì •ë³´ ì œê³µì˜ ì¡°í•©
        expertise_score = results.get('expertise', QualityScore(0, {}, [])).score
        specific_info_score = results.get('specific_info', QualityScore(0, {}, [])).score
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°
        clarity_score = (expertise_score * 0.6 + specific_info_score * 0.4)
        
        return clarity_score
    
    def _analyze_politeness(self, text: str) -> float:
        """ì˜ˆì˜ì„± ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        analyzer = CommunicationQualityAnalyzer()
        results = analyzer.analyze_communication_quality(text)
        
        # ì˜ˆì˜ì„±ì€ ì¡´ëŒ“ë§ ì‚¬ìš©ê³¼ ë¶€ì •ì  í‘œí˜„ íšŒí”¼ì˜ ì¡°í•©
        politeness_score = results.get('politeness', QualityScore(0, {}, [])).score
        negative_score = results.get('negative_expression', QualityScore(0, {}, [])).score
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì˜ˆì˜ì„± ì ìˆ˜ ê³„ì‚°
        final_score = (politeness_score * 0.7 + negative_score * 0.3)
        
        return final_score
    
    def _analyze_empathy(self, text: str) -> float:
        """ê³µê°ì„± ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        analyzer = CommunicationQualityAnalyzer()
        results = analyzer.analyze_communication_quality(text)
        
        # ê³µê°ì„± ì ìˆ˜ ë°˜í™˜
        empathy_score = results.get('empathy', QualityScore(0, {}, [])).score
        
        return empathy_score
    
    def _analyze_professionalism(self, text: str) -> float:
        """ì „ë¬¸ì„± ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        analyzer = CommunicationQualityAnalyzer()
        results = analyzer.analyze_communication_quality(text)
        
        # ì „ë¬¸ì„± ì ìˆ˜ ë°˜í™˜
        expertise_score = results.get('expertise', QualityScore(0, {}, [])).score
        
        return expertise_score
    
    def _analyze_response_quality(self, text: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ ë¶„ì„ (í†µì‹ ì‚¬ ìƒë‹´ì‚¬ ìˆ˜ì¤€)"""
        analyzer = CommunicationQualityAnalyzer()
        results = analyzer.analyze_communication_quality(text)
        
        # ì‘ë‹µ í’ˆì§ˆì€ ëª¨ë“  ì§€í‘œì˜ ì¢…í•©
        weights = {
            'politeness': 0.25,
            'negative_expression': 0.15,
            'empathy': 0.20,
            'expertise': 0.25,
            'specific_info': 0.10,
            'punctuation': 0.05
        }
        
        total_score = 0
        for category, weight in weights.items():
            if category in results:
                total_score += results[category].score * weight
        
        return total_score
    
    async def analyze_text_comprehensive(self, text: str) -> Dict[str, Any]:
        """
        ì¢…í•© í…ìŠ¤íŠ¸ ë¶„ì„
        
        Parameters
        ----------
        text : str
            ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns
        -------
        Dict[str, Any]
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        try:
            print(f"ğŸ” ì¢…í•© í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘: {len(text)}ì")
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸
            cached_info = self._is_cached("comprehensive", text)
            if cached_info:
                result = self._load_from_cache(cached_info)
                self.performance_stats["cache_hits"] += 1
                print(f"ğŸ’¾ ìºì‹œì—ì„œ ë¡œë“œ: ì¢…í•© ë¶„ì„")
                return result
            
            # ë³‘ë ¬ ë¶„ì„ íƒœìŠ¤í¬ ìƒì„±
            analysis_tasks = [
                self._analyze_sentiment_parallel(text),
                self._analyze_profanity_parallel(text),
                self._analyze_speaker_classification_parallel(text),
                self._analyze_communication_quality_parallel(text)
            ]
            
            # ë³‘ë ¬ ì‹¤í–‰
            if self.enable_parallel:
                print("ğŸš€ ë³‘ë ¬ ë¶„ì„ ì‹œì‘")
                results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
                self.performance_stats["parallel_analyses"] += 1
            else:
                print("ğŸŒ ìˆœì°¨ ë¶„ì„ ì‹œì‘")
                results = []
                for task in analysis_tasks:
                    try:
                        result = await task
                        results.append(result)
                    except Exception as e:
                        print(f"âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
                        results.append({"error": str(e)})
            
            # ê²°ê³¼ ì •ë¦¬
            analysis_result = {
                "sentiment_analysis": results[0] if len(results) > 0 else {},
                "profanity_detection": results[1] if len(results) > 1 else {},
                "speaker_classification": results[2] if len(results) > 2 else {},
                "communication_quality": results[3] if len(results) > 3 else {},
                "analysis_metadata": {
                    "text_length": len(text),
                    "processing_time": time.time() - start_time,
                    "analysis_method": "parallel" if self.enable_parallel else "sequential"
                }
            }
            
            # ìºì‹œì— ì €ì¥
            self._save_to_cache("comprehensive", text, analysis_result)
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self.performance_stats["total_analyses"] += 1
            self.performance_stats["avg_processing_time"] = (
                (self.performance_stats["avg_processing_time"] * (self.performance_stats["total_analyses"] - 1) + processing_time) 
                / self.performance_stats["total_analyses"]
            )
            
            print(f"âœ… ì¢…í•© ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            return analysis_result
            
        except Exception as e:
            print(f"âš ï¸ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "analysis_metadata": {
                    "text_length": len(text),
                    "processing_time": 0,
                    "analysis_method": "error"
                }
            }
    
    async def analyze_batch_parallel(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ë³‘ë ¬ ë¶„ì„
        
        Parameters
        ----------
        texts : List[str]
            ë¶„ì„í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns
        -------
        List[Dict[str, Any]]
            ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸš€ ë°°ì¹˜ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            start_time = time.time()
            
            # ë³‘ë ¬ íƒœìŠ¤í¬ ìƒì„±
            tasks = [self.analyze_text_comprehensive(text) for text in texts]
            
            # ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì •ë¦¬
            final_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"âš ï¸ í…ìŠ¤íŠ¸ {i} ë¶„ì„ ì‹¤íŒ¨: {result}")
                    final_results.append({
                        "error": str(result),
                        "text_index": i
                    })
                else:
                    final_results.append(result)
            
            processing_time = time.time() - start_time
            print(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            
            return final_results
            
        except Exception as e:
            print(f"âš ï¸ ë°°ì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return [{"error": str(e)} for _ in texts]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return self.performance_stats.copy()
    
    def cleanup_cache(self, max_age_hours: int = 24):
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_hours * 3600
            
            with self.cache_lock:
                keys_to_remove = []
                
                for cache_key, cache_info in self.cache_metadata.items():
                    if current_time - cache_info["created_at"] > max_age_seconds:
                        keys_to_remove.append(cache_key)
                
                for cache_key in keys_to_remove:
                    cache_info = self.cache_metadata[cache_key]
                    cache_path = self.cache_dir / cache_info["filename"]
                    
                    try:
                        if cache_path.exists():
                            os.remove(cache_path)
                            print(f"ğŸ§¹ ë¶„ì„ ìºì‹œ ì •ë¦¬: {cache_info['filename']}")
                    except Exception as e:
                        print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {cache_path}, {e}")
                    
                    del self.cache_metadata[cache_key]
                
                if keys_to_remove:
                    self._save_cache_metadata()
                    print(f"ğŸ§¹ {len(keys_to_remove)}ê°œ ë¶„ì„ ìºì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
                    
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.executor:
            self.executor.shutdown(wait=True) 