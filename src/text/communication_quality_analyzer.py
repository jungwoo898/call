#!/usr/bin/env python3
"""
ìƒë‹´ì‚¬ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ ëª¨ë“ˆ
8ê°€ì§€ í’ˆì§ˆ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ìƒë‹´ì‚¬ì˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬ì„ ì •ëŸ‰ í‰ê°€
"""

import re
import json
import requests
import os
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import pandas as pd

# LLM ê¸°ëŠ¥ import
try:
    from src.text.llm import LLMHandler
except ImportError:
    print("âš ï¸ LLM ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ê¸°ë°˜ ì§€í‘œëŠ” Noneìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.")
    LLMHandler = None


@dataclass
class CommunicationQualityResult:
    """ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼"""
    honorific_ratio: float
    positive_word_ratio: float
    negative_word_ratio: float
    euphonious_word_ratio: float
    empathy_ratio: float
    apology_ratio: float
    total_sentences: int
    analysis_details: Dict[str, Any]
    
    # ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢…
    customer_sentiment_early: Optional[float] = None  # ê³ ê° ê°ì • ì´ˆë°˜ë¶€ í‰ê· 
    customer_sentiment_late: Optional[float] = None   # ê³ ê° ê°ì • í›„ë°˜ë¶€ í‰ê·   
    customer_sentiment_trend: Optional[float] = None  # ê³ ê° ê°ì • ë³€í™” ì¶”ì„¸
    avg_response_latency: Optional[float] = None      # í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„
    task_ratio: Optional[float] = None                # ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨
    
    # ìƒˆë¡œìš´ LLM ê¸°ë°˜ ì •ì„± í‰ê°€ ì§€í‘œ 2ì¢…
    suggestions: Optional[float] = None               # ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ (0.0, 0.2, 0.6, 1.0)
    interruption_count: Optional[int] = None          # ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜


class CommunicationQualityAnalyzer:
    """
    ìƒë‹´ì‚¬ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ê¸°
    
    í™”ì ë¶„ë¦¬ëœ JSON ë°ì´í„°ì—ì„œ ìƒë‹´ì‚¬ ë°œí™”ë¥¼ ì¶”ì¶œí•˜ê³ 
    6ê°€ì§€ í’ˆì§ˆ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.sentiment_dict = None
        self._load_sentiment_dictionary()
        
        # íŒ¨í„´ ì •ì˜
        self._define_patterns()
        
        # LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        self.llm_handler = None
        if LLMHandler is not None:
            try:
                self.llm_handler = LLMHandler()
                print("âœ… LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ LLM í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_handler = None
        else:
            print("âš ï¸ LLM í•¸ë“¤ëŸ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _load_sentiment_dictionary(self):
        """KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ë¡œë“œ"""
        try:
            # ê°ì„±ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            dict_path = "data/knu_sentiment_dict.json"
            
            if os.path.exists(dict_path):
                print("âœ… ë¡œì»¬ ê°ì„±ì‚¬ì „ ë¡œë“œ")
                with open(dict_path, 'r', encoding='utf-8') as f:
                    self.sentiment_dict = json.load(f)
            else:
                print("ğŸ”„ KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                self._download_sentiment_dictionary(dict_path)
                
        except Exception as e:
            print(f"âš ï¸ ê°ì„±ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ ê°ì„±ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self._create_fallback_sentiment_dict()
    
    def _download_sentiment_dictionary(self, save_path: str):
        """KNU í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ë‹¤ìš´ë¡œë“œ"""
        try:
            # KNU ê°ì„±ì‚¬ì „ URL (GitHub ë˜ëŠ” ê³µì‹ ì €ì¥ì†Œ)
            url = "https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/KnuSentiLex.json"
            
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # JSON íŒŒì‹± ë° ì €ì¥
            sentiment_data = response.json()
            
            # ì‚¬ì „ í˜•íƒœë¡œ ë³€í™˜ (ë‹¨ì–´: polarity)
            self.sentiment_dict = {}
            for word, info in sentiment_data.items():
                if isinstance(info, dict) and 'polarity' in info:
                    self.sentiment_dict[word] = info['polarity']
                elif isinstance(info, (int, float)):
                    self.sentiment_dict[word] = info
            
            # íŒŒì¼ ì €ì¥
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(self.sentiment_dict, f, ensure_ascii=False, indent=2)
                
            print(f"âœ… ê°ì„±ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(self.sentiment_dict)}ê°œ ë‹¨ì–´")
            
        except Exception as e:
            print(f"âŒ ê°ì„±ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._create_fallback_sentiment_dict()
    
    def _create_fallback_sentiment_dict(self):
        """ê¸°ë³¸ ê°ì„±ì‚¬ì „ ìƒì„± (ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ)"""
        self.sentiment_dict = {
            # ê¸ì • ë‹¨ì–´ (Positive Words) - 30ê°œ
            "ì¢‹ë‹¤": 1, "ê°ì‚¬í•˜ë‹¤": 2, "ê¸°ì˜ë‹¤": 2, "ë‹¤í–‰ì´ë‹¤": 2, "ë§Œì¡±í•˜ë‹¤": 2, 
            "ì•ˆì‹¬ì´ë‹¤": 2, "ì¹œì ˆí•˜ë‹¤": 2, "í›Œë¥­í•˜ë‹¤": 2, "í–‰ë³µí•˜ë‹¤": 2, "ê³ ë§™ë‹¤": 2,
            "ë„ì›€": 1, "ì„±ê³µ": 1, "í•´ê²°": 1, "íš¨ê³¼": 1, "í¸ë¦¬í•˜ë‹¤": 1, 
            "ë¹ ë¥´ë‹¤": 1, "ì‰½ë‹¤": 1, "ì •í™•í•˜ë‹¤": 1, "ê°„í¸í•˜ë‹¤": 1, "ë›°ì–´ë‚˜ë‹¤": 1,
            "ì™„ë²½í•˜ë‹¤": 2, "ìµœê³ ë‹¤": 2, "ìš°ìˆ˜í•˜ë‹¤": 1, "ê¸°ëŒ€ë˜ë‹¤": 1, "ëŒ€ë‹¨í•˜ë‹¤": 1,
            "ë©‹ì§€ë‹¤": 1, "ìƒì„¸í•˜ë‹¤": 1, "ì‹ ì†í•˜ë‹¤": 1, "ì•ˆì •ì ": 1,

            # ë¶€ì • ë‹¨ì–´ (Negative Words) - 30ê°œ
            "ë‚˜ì˜ë‹¤": -1, "ì‹«ë‹¤": -1, "ë¬¸ì œ": -1, "ì˜¤ë¥˜": -1, "ì–´ë µë‹¤": -1,
            "ëŠë¦¬ë‹¤": -1, "ë³µì¡í•˜ë‹¤": -1, "í˜ë“¤ë‹¤": -1, "ì•„ì‰½ë‹¤": -1, "ìœ ê°": -1,
            "ì‹¤ë§í•˜ë‹¤": -2, "í™”ë‚˜ë‹¤": -2, "ì§œì¦ë‚˜ë‹¤": -2, "ë¶ˆí¸í•˜ë‹¤": -2, "ë¶ˆë§Œ": -2, 
            "ì‹¤íŒ¨": -2, "ë‹µë‹µí•˜ë‹¤": -2, "ì†ìƒí•˜ë‹¤": -2, "ê±±ì •ë˜ë‹¤": -1, "ë¶ˆì•ˆí•˜ë‹¤": -1,
            "ìœ„í—˜í•˜ë‹¤": -2, "ê·€ì°®ë‹¤": -1, "í”¼ê³¤í•˜ë‹¤": -1, "ìµœì•…": -2, "ì—‰ë§": -2,
            "ë¶€ì¡±í•˜ë‹¤": -1, "ë¶ˆê°€ëŠ¥í•˜ë‹¤": -2, "ë¶ˆì¹œì ˆí•˜ë‹¤": -2, "ì§€ì—°": -1, "ëˆ„ë½": -1, "ì˜¤ì‘ë™":-2
        }
        print(f"âœ… ê°•í™”ëœ ê¸°ë³¸ ê°ì„±ì‚¬ì „ ìƒì„±: {len(self.sentiment_dict)}ê°œ ë‹¨ì–´")

    def _define_patterns(self):
        """ë¶„ì„ìš© íŒ¨í„´ ì •ì˜ (ê°•í™” ë²„ì „)"""
        
        # 1. ì¡´ëŒ“ë§ íŒ¨í„´ (Honorific Patterns)
        self.honorific_patterns = [
            # ê³µì‹ì  ì¢…ê²°ì–´ë¯¸ (í•˜ì‹­ì‹œì˜¤ì²´)
            r'ìŠµë‹ˆë‹¤$', r'ã…‚ë‹ˆë‹¤$', r'ã…‚ë‹ˆê¹Œ\?$', r'ì‹œì£ $', r'í•˜ì‹­ì‹œì˜¤$', r'í•´ì£¼ì‹­ì‹œì˜¤$',
            # ë³´í¸ì  ì¢…ê²°ì–´ë¯¸ (í•´ìš”ì²´)
            r'í•´ìš”$', r'ì„¸ìš”$', r'ì…”ìš”$', r'ë„¤ìš”$', r'ê±¸ìš”$', r'ì§€ìš”\?$', r'ê¹Œìš”\?$',
            # ì„œë¹„ìŠ¤ ì œê³µ í‘œí˜„
            r'ë“œë¦½ë‹ˆë‹¤$', r'ë“œë ¤ìš”$', r'í•´ë“œë¦´ê²Œìš”$', r'ë„ì™€ë“œë¦´ê¹Œìš”\?$',
            # ì£¼ì²´ ë†’ì„ ì„ ì–´ë§ ì–´ë¯¸ '-(ìœ¼)ì‹œ-'
            r'(ìœ¼)ì‹œê² ìŠµë‹ˆë‹¤$', r'(ìœ¼)ì…¨ìŠµë‹ˆë‹¤$', r'(ìœ¼)ì‹­ë‹ˆë‹¤$', r'(ìœ¼)ì‹œì£ $', r'(ìœ¼)ì‹œë„¤ìš”$', 
            r'(ìœ¼)ì‹œëŠ”êµ°ìš”$', r'ì´ì‹­ë‹ˆë‹¤$',
            # ì—°ê²°í˜•
            r'í•˜ì‹œë©´$', r'í•˜ì‹œê³ $', r'ì´ì‹œê³ $'
        ]
        
        # 4. ì¿ ì…˜ì–´/ì™„ê³¡ í‘œí˜„ (Euphonious Patterns)
        self.euphonious_patterns = [
            # [ì¿ ì…˜ì–´] ì–‘í•´/ìš”ì²­
            r'ì‹¤ë¡€ì§€ë§Œ', r'ì£„ì†¡í•˜ì§€ë§Œ', r'ê´œì°®ìœ¼ì‹œë‹¤ë©´', r'í˜¹ì‹œë¼ë„', r'ë°”ì˜ì‹œê² ì§€ë§Œ', 
            r'ë²ˆê±°ë¡œìš°ì‹œê² ì§€ë§Œ', r'ì—¼ë ¤ìŠ¤ëŸ¬ìš°ì‹œê² ì§€ë§Œ', r'ë‹¤ë¦„ì´ ì•„ë‹ˆì˜¤ë¼',
            # [ì¿ ì…˜ì–´] ì œì•ˆ/ì˜ê²¬
            r'ë§Œì•½', r'ì˜ˆë¥¼ ë“¤ì–´', r'ì•„ì‰½ì§€ë§Œ', r'ìœ ê°ì´ì§€ë§Œ',
            
            # [ì™„ê³¡ í‘œí˜„] ë‹¨ì • íšŒí”¼
            r'ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤$', r'ã„¹ ê²ƒ ê°™ìŠµë‹ˆë‹¤$', r'ë“¯í•©ë‹ˆë‹¤$', r'ã„¹ ë“¯í•©ë‹ˆë‹¤$', r'ë¡œ ë³´ì…ë‹ˆë‹¤$',
            # [ì™„ê³¡ í‘œí˜„] ë¶€ë“œëŸ¬ìš´ ê±°ì ˆ/ë¶€ì •
            r'ë„ì›€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤$', r'ì²˜ë¦¬ê°€ ê³¤ë€í•©ë‹ˆë‹¤$', r'í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤$', 
            r'ê²€í†  í›„ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤$', r'ê·œì •ìƒ ì–´ë µìŠµë‹ˆë‹¤$',
            # [ì™„ê³¡ í‘œí˜„] ë¶€ë“œëŸ¬ìš´ ìš”ì²­
            r'ã„¹ ìˆ˜ ìˆì„ê¹Œìš”\?$', r'í•´ ì£¼ì‹œê² ì–´ìš”\?$', r'í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”\?$', 
            r'ë¶€íƒë“œë ¤ë„ ë ê¹Œìš”\?$', r'í™•ì¸í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤$'
        ]
        
        # 5. ê³µê° í‘œí˜„ (Empathy Patterns)
        self.empathy_patterns = [
            # [ê°ì • ì½ê¸°] ê³ ê°ì˜ ê°ì • ìƒíƒœë¥¼ ì§ì‘í•˜ê³  ì–¸ê¸‰
            r'ì†ìƒí•˜ì…¨ê² ì–´ìš”', r'ë‹µë‹µí•˜ì…¨ê² ë„¤ìš”', r'ë§ì´ ë†€ë¼ì…¨ê² ì–´ìš”', r'ë¶ˆí¸í•˜ì…¨ê² ì–´ìš”',
            r'ê±±ì • ë§ì´ í•˜ì…¨ê² ë„¤ìš”', r'ë§ì´ í˜ë“œì…¨ì£ ', r'ì‹ ê²½ ì“°ì´ì…¨ê² ì–´ìš”',
            # [ê°ì • ì¸ì •] ê³ ê°ì˜ ê°ì •ì— íƒ€ë‹¹ì„± ë¶€ì—¬
            r'ì–´ë–¤ ë§ˆìŒì¸ì§€ ì•Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤', r'ì¶©ë¶„íˆ ì´í•´ë©ë‹ˆë‹¤', r'ê·¸ë ‡ê²Œ ìƒê°í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤',
            r'ì˜¤ì£½í•˜ë©´ ê·¸ëŸ¬ì…¨ê² ì–´ìš”',
            # [ê´€ì  ìˆ˜ìš©] ê³ ê°ì˜ ì…ì¥ì—ì„œ ìƒê°
            r'ì œê°€ ê³ ê°ë‹˜ ì…ì¥ì´ë¼ë„', r'ì œê°€ ê°™ì€ ìƒí™©ì´ì—ˆë”ë¼ë„',
            # [ê³µê°ì  ì¶”ì„ìƒˆ] ëŒ€í™”ì— ì§‘ì¤‘í•˜ê³  ìˆìŒì„ í‘œí˜„
            r'ì•„ì´ê³ (,)? ì €ëŸ°', r'ì–´ë¨¸ë‚˜', r'ê·¸ë¬êµ°ìš”'
        ]
        
        # 6. ì‚¬ê³¼ í‘œí˜„ (Apology Patterns)
        self.apology_patterns = [
            # [ì§ì ‘ ì‚¬ê³¼] ì§ì ‘ì ì¸ ì‚¬ê³¼ í‘œí˜„
            r'ì£„ì†¡í•©ë‹ˆë‹¤', r'ì •ë§ ì£„ì†¡í•©ë‹ˆë‹¤', r'ëŒ€ë‹¨íˆ ì£„ì†¡í•©ë‹ˆë‹¤', r'ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤', r'ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤',
            # [ìƒí™© ìœ ê° í‘œí˜„] ë¶ˆí¸ì„ ë¼ì¹œ ìƒí™©ì— ëŒ€í•œ ìœ ê°
            r'ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤', r'ì‹¬ë ¤ë¥¼ ë¼ì³ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤', r'í˜¼ë€ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤',
            r'ì˜¤ë˜ ê¸°ë‹¤ë¦¬ê²Œ í•´ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤',
            # [ì±…ì„ ì¸ì •] ìì‹ ì˜ ê³¼ì‹¤ì„ ì¸ì •
            r'ì €ì˜ ë¶ˆì°°ì…ë‹ˆë‹¤', r'ì €í¬ì˜ ì˜ëª»ì…ë‹ˆë‹¤',
            # [ì´í•´ ìš”ì²­] ê³ ê°ì˜ ì´í•´ë¥¼ êµ¬í•¨
            r'ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤', r'ë„ˆê·¸ëŸ¬ì´ ì´í•´í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤'
        ]
    
    def analyze_communication_quality(self, utterances_data: List[Dict[str, Any]]) -> CommunicationQualityResult:
        """
        ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ ìˆ˜í–‰
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            í™”ì ë¶„ë¦¬ëœ ë°œí™” ë°ì´í„°
            ê° í•­ëª©ì€ {'speaker': str, 'text': str, ...} í˜•íƒœ
            
        Returns
        -------
        CommunicationQualityResult
            6ê°€ì§€ í’ˆì§ˆ ì§€í‘œ ë¶„ì„ ê²°ê³¼
        """
        
        # ìƒë‹´ì‚¬ ë°œí™”ë§Œ ì¶”ì¶œ
        counselor_sentences = self._extract_counselor_sentences(utterances_data)
        
        if not counselor_sentences:
            return CommunicationQualityResult(
                honorific_ratio=0.0,
                positive_word_ratio=0.0,
                negative_word_ratio=0.0,
                euphonious_word_ratio=0.0,
                empathy_ratio=0.0,
                apology_ratio=0.0,
                total_sentences=0,
                analysis_details={"error": "ìƒë‹´ì‚¬ ë°œí™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            )
        
        total_sentences = len(counselor_sentences)
        
        # ê° ì§€í‘œ ê³„ì‚°
        honorific_count = self._count_honorific_sentences(counselor_sentences)
        positive_count = self._count_positive_word_sentences(counselor_sentences)
        negative_count = self._count_negative_word_sentences(counselor_sentences)
        euphonious_count = self._count_euphonious_sentences(counselor_sentences)
        empathy_count = self._count_empathy_sentences(counselor_sentences)
        apology_count = self._count_apology_sentences(counselor_sentences)
        
        # ë¹„ìœ¨ ê³„ì‚° (%)
        honorific_ratio = (honorific_count / total_sentences) * 100
        positive_word_ratio = (positive_count / total_sentences) * 100
        negative_word_ratio = (negative_count / total_sentences) * 100
        euphonious_word_ratio = (euphonious_count / total_sentences) * 100
        empathy_ratio = (empathy_count / total_sentences) * 100
        apology_ratio = (apology_count / total_sentences) * 100
        
        # ìƒì„¸ ë¶„ì„ ì •ë³´
        analysis_details = {
            "honorific_sentences": honorific_count,
            "positive_word_sentences": positive_count,
            "negative_word_sentences": negative_count,
            "euphonious_sentences": euphonious_count,
            "empathy_sentences": empathy_count,
            "apology_sentences": apology_count,
            "sample_sentences": {
                "honorific": self._get_sample_sentences(counselor_sentences, self.honorific_patterns, "ì¡´ëŒ“ë§"),
                "positive": self._get_sample_sentences_by_sentiment(counselor_sentences, "positive"),
                "negative": self._get_sample_sentences_by_sentiment(counselor_sentences, "negative"),
                "euphonious": self._get_sample_sentences(counselor_sentences, self.euphonious_patterns, "ì¿ ì…˜ì–´"),
                "empathy": self._get_sample_sentences(counselor_sentences, self.empathy_patterns, "ê³µê°"),
                "apology": self._get_sample_sentences(counselor_sentences, self.apology_patterns, "ì‚¬ê³¼")
            }
        }
        
        # ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢… ê³„ì‚°
        customer_sentiment_early, customer_sentiment_late, customer_sentiment_trend = self._calculate_customer_sentiment_trend(utterances_data)
        avg_response_latency = self._calculate_avg_response_latency(utterances_data)
        task_ratio = self._calculate_task_ratio(utterances_data)
        
        # ìƒˆë¡œìš´ LLM ê¸°ë°˜ ì •ì„± í‰ê°€ ì§€í‘œ 2ì¢… ê³„ì‚°
        suggestions = asyncio.run(self._calculate_suggestions_score(utterances_data))
        interruption_count = self._calculate_interruption_count(utterances_data)
        
        return CommunicationQualityResult(
            honorific_ratio=round(honorific_ratio, 2),
            positive_word_ratio=round(positive_word_ratio, 2),
            negative_word_ratio=round(negative_word_ratio, 2),
            euphonious_word_ratio=round(euphonious_word_ratio, 2),
            empathy_ratio=round(empathy_ratio, 2),
            apology_ratio=round(apology_ratio, 2),
            total_sentences=total_sentences,
            analysis_details=analysis_details,
            # ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ
            customer_sentiment_early=customer_sentiment_early,
            customer_sentiment_late=customer_sentiment_late,
            customer_sentiment_trend=customer_sentiment_trend,
            avg_response_latency=avg_response_latency,
            task_ratio=task_ratio,
            # ìƒˆë¡œìš´ LLM ê¸°ë°˜ ì •ì„± í‰ê°€ ì§€í‘œ
            suggestions=suggestions,
            interruption_count=interruption_count
        )
    
    def _extract_counselor_sentences(self, utterances_data: List[Dict[str, Any]]) -> List[str]:
        """ìƒë‹´ì‚¬ ë°œí™”ë§Œ ì¶”ì¶œí•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
        counselor_sentences = []
        
        for utterance in utterances_data:
            speaker = utterance.get('speaker', '').lower()
            text = utterance.get('text', '').strip()
            
            # ìƒë‹´ì‚¬ ë°œí™” ì‹ë³„
            if any(keyword in speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff']):
                if text:
                    # ë¬¸ì¥ ë¶„ë¦¬
                    sentences = self._split_sentences(text)
                    counselor_sentences.extend(sentences)
        
        return counselor_sentences
    
    def _split_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
        # í•œêµ­ì–´ ë¬¸ì¥ ë¶€í˜¸ ê¸°ì¤€ ë¶„ë¦¬
        sentences = re.split(r'[.!?ã€‚ï¼Ÿï¼]+', text)
        
        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _count_honorific_sentences(self, sentences: List[str]) -> int:
        """ì¡´ëŒ“ë§ ì‚¬ìš© ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        count = 0
        for sentence in sentences:
            for pattern in self.honorific_patterns:
                if re.search(pattern, sentence):
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ì—¬ëŸ¬ íŒ¨í„´ ë§¤ì¹˜ë˜ì–´ë„ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _count_positive_word_sentences(self, sentences: List[str]) -> int:
        """ê¸ì • ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        if not self.sentiment_dict:
            return 0
            
        count = 0
        for sentence in sentences:
            words = sentence.split()
            for word in words:
                # í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ë‹¨ìˆœ ë§¤ì¹­ (ì¶”í›„ ê°œì„  ê°€ëŠ¥)
                clean_word = re.sub(r'[^\wê°€-í£]', '', word)
                if clean_word in self.sentiment_dict and self.sentiment_dict[clean_word] > 0:
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ê¸ì • ë‹¨ì–´ ë°œê²¬ ì‹œ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _count_negative_word_sentences(self, sentences: List[str]) -> int:
        """ë¶€ì • ë‹¨ì–´ í¬í•¨ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        if not self.sentiment_dict:
            return 0
            
        count = 0
        for sentence in sentences:
            words = sentence.split()
            for word in words:
                # í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ ë‹¨ìˆœ ë§¤ì¹­ (ì¶”í›„ ê°œì„  ê°€ëŠ¥)
                clean_word = re.sub(r'[^\wê°€-í£]', '', word)
                if clean_word in self.sentiment_dict and self.sentiment_dict[clean_word] < 0:
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ë¶€ì • ë‹¨ì–´ ë°œê²¬ ì‹œ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _count_euphonious_sentences(self, sentences: List[str]) -> int:
        """ì¿ ì…˜ì–´/ì™„ê³¡ í‘œí˜„ í¬í•¨ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        count = 0
        for sentence in sentences:
            for pattern in self.euphonious_patterns:
                if re.search(pattern, sentence):
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ì—¬ëŸ¬ íŒ¨í„´ ë§¤ì¹˜ë˜ì–´ë„ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _count_empathy_sentences(self, sentences: List[str]) -> int:
        """ê³µê° í‘œí˜„ í¬í•¨ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        count = 0
        for sentence in sentences:
            for pattern in self.empathy_patterns:
                if re.search(pattern, sentence):
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ì—¬ëŸ¬ íŒ¨í„´ ë§¤ì¹˜ë˜ì–´ë„ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _count_apology_sentences(self, sentences: List[str]) -> int:
        """ì‚¬ê³¼ í‘œí˜„ í¬í•¨ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        count = 0
        for sentence in sentences:
            for pattern in self.apology_patterns:
                if re.search(pattern, sentence):
                    count += 1
                    break  # í•˜ë‚˜ì˜ ë¬¸ì¥ì—ì„œ ì—¬ëŸ¬ íŒ¨í„´ ë§¤ì¹˜ë˜ì–´ë„ 1ë²ˆë§Œ ì¹´ìš´íŠ¸
        return count
    
    def _get_sample_sentences(self, sentences: List[str], patterns: List[str], category: str) -> List[str]:
        """í•´ë‹¹ íŒ¨í„´ì— ë§¤ì¹˜ë˜ëŠ” ìƒ˜í”Œ ë¬¸ì¥ë“¤ ë°˜í™˜ (ìµœëŒ€ 3ê°œ)"""
        samples = []
        for sentence in sentences:
            if len(samples) >= 3:
                break
            for pattern in patterns:
                if re.search(pattern, sentence):
                    samples.append(sentence)
                    break
        return samples
    
    def _get_sample_sentences_by_sentiment(self, sentences: List[str], sentiment_type: str) -> List[str]:
        """ê°ì •ë³„ ìƒ˜í”Œ ë¬¸ì¥ë“¤ ë°˜í™˜ (ìµœëŒ€ 3ê°œ)"""
        if not self.sentiment_dict:
            return []
            
        samples = []
        target_polarity = 1 if sentiment_type == "positive" else -1
        
        for sentence in sentences:
            if len(samples) >= 3:
                break
            words = sentence.split()
            for word in words:
                clean_word = re.sub(r'[^\wê°€-í£]', '', word)
                if (clean_word in self.sentiment_dict and 
                    ((sentiment_type == "positive" and self.sentiment_dict[clean_word] > 0) or
                     (sentiment_type == "negative" and self.sentiment_dict[clean_word] < 0))):
                    samples.append(sentence)
                    break
        return samples
    
    def _calculate_customer_sentiment_trend(self, utterances_data: List[Dict[str, Any]]) -> tuple:
        """
        ê³ ê° ê°ì • ì¶”ì„¸ ë¶„ì„ (ì§€í‘œ 1, 2, 3)
        
        Returns
        -------
        tuple: (customer_sentiment_early, customer_sentiment_late, customer_sentiment_trend)
        """
        try:
            # 1. ê³ ê° ë°œí™”ë§Œ í•„í„°ë§
            customer_utterances = []
            for utterance in utterances_data:
                speaker = utterance.get('speaker', '').lower()
                if any(keyword in speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']):
                    customer_utterances.append(utterance)
            
            if len(customer_utterances) < 3:  # ìµœì†Œ 3ê°œ ë°œí™” í•„ìš”
                return None, None, None
            
            # 2. sentiment í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë§¤í•‘
            sentiment_scores = []
            for utterance in customer_utterances:
                sentiment_text = utterance.get('sentiment', '').lower()
                score = self._map_sentiment_to_score(sentiment_text)
                if score is not None:
                    sentiment_scores.append(score)
            
            if len(sentiment_scores) < 3:
                return None, None, None
            
            # 3. ì´ˆë°˜ë¶€(ì²˜ìŒ 33%)ì™€ í›„ë°˜ë¶€(ë 33%) êµ¬ë¶„
            total_count = len(sentiment_scores)
            early_count = max(1, int(total_count * 0.33))
            late_count = max(1, int(total_count * 0.33))
            
            early_scores = sentiment_scores[:early_count]
            late_scores = sentiment_scores[-late_count:]
            
            # 4. ê° êµ¬ê°„ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
            customer_sentiment_early = round(sum(early_scores) / len(early_scores), 3)
            customer_sentiment_late = round(sum(late_scores) / len(late_scores), 3)
            
            # 5. ê°ì • ë³€í™” ì¶”ì„¸ ê³„ì‚° (í›„ë°˜ë¶€ - ì´ˆë°˜ë¶€)
            customer_sentiment_trend = round(customer_sentiment_late - customer_sentiment_early, 3)
            
            return customer_sentiment_early, customer_sentiment_late, customer_sentiment_trend
            
        except Exception as e:
            print(f"âš ï¸ ê³ ê° ê°ì • ì¶”ì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None, None, None
    
    def _map_sentiment_to_score(self, sentiment_text: str) -> Optional[float]:
        """
        sentiment í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ì ìˆ˜ë¡œ ë§¤í•‘
        
        Parameters
        ----------
        sentiment_text : str
            ê°ì • í…ìŠ¤íŠ¸ (ì˜ˆ: 'positive', 'negative', 'neutral')
            
        Returns
        -------
        Optional[float]
            ê°ì • ì ìˆ˜ ë˜ëŠ” None
        """
        sentiment_mapping = {
            # ê¸°ë³¸ ë§¤í•‘
            'positive': 1.0,
            'neutral': 0.0,
            'negative': -1.0,
            
            # í™•ì¥ ë§¤í•‘ (5ì  ì²™ë„)
            'very positive': 2.0,
            'very_positive': 2.0,
            'very negative': -2.0,
            'very_negative': -2.0,
            
            # í•œêµ­ì–´ ë§¤í•‘
            'ê¸ì •': 1.0,
            'ë¶€ì •': -1.0,
            'ì¤‘ë¦½': 0.0,
            'ë§¤ìš°ê¸ì •': 2.0,
            'ë§¤ìš°ë¶€ì •': -2.0,
            
            # ìˆ«ì ë¬¸ìì—´ ì§ì ‘ ë§¤í•‘
            '1': 1.0,
            '0': 0.0,
            '-1': -1.0,
            '2': 2.0,
            '-2': -2.0
        }
        
        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¡œ ë§¤í•‘ ì‹œë„
        normalized_text = sentiment_text.strip().lower().replace(' ', '_')
        
        if normalized_text in sentiment_mapping:
            return sentiment_mapping[normalized_text]
        
        # ìˆ«ìë¡œ ì§ì ‘ ë³€í™˜ ì‹œë„
        try:
            score = float(sentiment_text)
            return max(-2.0, min(2.0, score))  # -2.0 ~ 2.0 ë²”ìœ„ë¡œ ì œí•œ
        except (ValueError, TypeError):
            pass
        
        return None
    
    def _calculate_avg_response_latency(self, utterances_data: List[Dict[str, Any]]) -> Optional[float]:
        """
        í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„ ê³„ì‚° (ì§€í‘œ 4)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            ë°œí™” ë°ì´í„° (start_time, end_time, speaker í¬í•¨)
            
        Returns
        -------
        Optional[float]
            í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„(ì´ˆ) ë˜ëŠ” None
        """
        try:
            # 1. ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_utterances = sorted(utterances_data, key=lambda x: x.get('start_time', 0))
            
            latencies = []
            prev_utterance = None
            
            # 2. ê³ ê° -> ìƒë‹´ì‚¬ ì „í™˜ ì§€ì  ì°¾ê¸°
            for utterance in sorted_utterances:
                current_speaker = utterance.get('speaker', '').lower()
                current_start_time = utterance.get('start_time')
                
                if prev_utterance is not None:
                    prev_speaker = prev_utterance.get('speaker', '').lower()
                    prev_end_time = prev_utterance.get('end_time')
                    
                    # ê³ ê° -> ìƒë‹´ì‚¬ ì „í™˜ í™•ì¸
                    is_customer_to_counselor = (
                        any(keyword in prev_speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']) and
                        any(keyword in current_speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff'])
                    )
                    
                    if is_customer_to_counselor and prev_end_time is not None and current_start_time is not None:
                        # 3. ì§€ì—°ì‹œê°„ = ìƒë‹´ì‚¬ ë°œí™” ì‹œì‘ - ê³ ê° ë°œí™” ì¢…ë£Œ
                        latency = current_start_time - prev_end_time
                        if latency >= 0:  # ìŒìˆ˜ ì§€ì—°ì‹œê°„ ì œì™¸
                            latencies.append(latency)
                
                prev_utterance = utterance
            
            # 4. í‰ê·  ê³„ì‚°
            if latencies:
                avg_latency = sum(latencies) / len(latencies)
                return round(avg_latency, 3)
            else:
                return None
                
        except Exception as e:
            print(f"âš ï¸ í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_task_ratio(self, utterances_data: List[Dict[str, Any]]) -> Optional[float]:
        """
        ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ ê³„ì‚° (ì§€í‘œ 5)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            ë°œí™” ë°ì´í„° (start_time, end_time, speaker í¬í•¨)
            
        Returns
        -------
        Optional[float]
            ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ (ê³ ê° ë°œí™” ì‹œê°„ / ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„) ë˜ëŠ” None
        """
        try:
            customer_total_time = 0.0
            counselor_total_time = 0.0
            
            # 1. ê° í™”ìë³„ ì´ ë°œí™” ì‹œê°„ ê³„ì‚°
            for utterance in utterances_data:
                speaker = utterance.get('speaker', '').lower()
                start_time = utterance.get('start_time')
                end_time = utterance.get('end_time')
                
                if start_time is not None and end_time is not None and end_time > start_time:
                    duration = end_time - start_time
                    
                    # ê³ ê° ë°œí™” ì‹œê°„
                    if any(keyword in speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']):
                        customer_total_time += duration
                    
                    # ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„
                    elif any(keyword in speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff']):
                        counselor_total_time += duration
            
            # 2. ë¹„ìœ¨ ê³„ì‚°
            if counselor_total_time > 0:
                task_ratio = customer_total_time / counselor_total_time
                return round(task_ratio, 3)
            else:
                return None  # ìƒë‹´ì‚¬ ë°œí™” ì‹œê°„ì´ 0ì¸ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                
        except Exception as e:
            print(f"âš ï¸ ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    async def _calculate_suggestions_score(self, utterances_data: List[Dict[str, Any]]) -> Optional[float]:
        """
        LLM ê¸°ë°˜ ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ ê³„ì‚° (ì§€í‘œ 6)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            í™”ì ë¶„ë¦¬ëœ ë°œí™” ë°ì´í„°
            
        Returns
        -------
        Optional[float]
            ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ (1.0, 0.6, 0.2, 0.0) ë˜ëŠ” None
        """
        if self.llm_handler is None:
            print("âš ï¸ LLM í•¸ë“¤ëŸ¬ê°€ ì—†ì–´ ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        try:
            # 1. ì „ì²´ ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            conversation_text = ""
            for utterance in utterances_data:
                speaker = utterance.get('speaker', 'Unknown')
                text = utterance.get('text', '').strip()
                if text:
                    conversation_text += f"{speaker}: {text}\n"
            
            if not conversation_text.strip():
                return None
            
            # 2. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = """ë‹¹ì‹ ì€ ìƒë‹´ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ìƒë‹´ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³ , ë¬¸ì œ í•´ê²° ê³¼ì •ì— ë”°ë¼ ì•„ë˜ ê·œì¹™ì— ë§ì¶° ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”.

ì ìˆ˜ ê¸°ì¤€:
- 1.0ì : ìµœì´ˆë¡œ ì œì‹œí•œ ì•„ì´ë””ì–´ë¡œ ë¬¸ì œê°€ í•´ê²°ë¨
- 0.6ì : ì²« ë²ˆì§¸ ì•„ì´ë””ì–´ëŠ” ì‹¤íŒ¨í–ˆì§€ë§Œ, ë‘ ë²ˆì§¸ë¡œ ì œì‹œí•œ ì•„ì´ë””ì–´ë¡œ í•´ê²°ë¨  
- 0.2ì : ì„¸ ë²ˆ ì´ìƒì˜ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•¨
- 0.0ì : ëŒ€í™”ê°€ ëë‚  ë•Œê¹Œì§€ ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ëª»í•¨

ë°˜ë“œì‹œ '1.0', '0.6', '0.2', '0.0' ì¤‘ í•˜ë‚˜ì˜ ìˆ«ìë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            user_prompt = f"ë‹¤ìŒ ìƒë‹´ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”:\n\n{conversation_text}"
            
            # 3. LLM API í˜¸ì¶œ
            response = await self.llm_handler.client.chat.completions.create(
                model="gpt-4.1-nano",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=50
            )
            
            # 4. ì‘ë‹µ íŒŒì‹±
            response_text = response.choices[0].message.content.strip()
            
            # 5. ì ìˆ˜ ì¶”ì¶œ
            if "1.0" in response_text:
                return 1.0
            elif "0.6" in response_text:
                return 0.6
            elif "0.2" in response_text:
                return 0.2
            elif "0.0" in response_text:
                return 0.0
            else:
                print(f"âš ï¸ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {response_text}")
                return None
                
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_interruption_count(self, utterances_data: List[Dict[str, Any]]) -> Optional[int]:
        """
        ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜ ê³„ì‚° (ì§€í‘œ 7)
        
        Parameters
        ----------
        utterances_data : List[Dict[str, Any]]
            í™”ì ë¶„ë¦¬ëœ ë°œí™” ë°ì´í„° (start_time, end_time í•„ìˆ˜)
            
        Returns
        -------
        Optional[int]
            ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜ ë˜ëŠ” None
        """
        try:
            if len(utterances_data) < 2:
                return 0
            
            interruption_count = 0
            
            # 1. ë°œí™” ìˆœì„œëŒ€ë¡œ ì •ë ¬ (start_time ê¸°ì¤€)
            sorted_utterances = sorted(utterances_data, key=lambda x: x.get('start_time', 0))
            
            # 2. ì—°ì†ëœ ë°œí™” ìŒì„ ê²€ì‚¬
            for i in range(1, len(sorted_utterances)):
                prev_utterance = sorted_utterances[i-1]
                curr_utterance = sorted_utterances[i]
                
                prev_speaker = prev_utterance.get('speaker', '').lower()
                curr_speaker = curr_utterance.get('speaker', '').lower()
                
                prev_end_time = prev_utterance.get('end_time')
                curr_start_time = curr_utterance.get('start_time')
                
                # 3. ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ê²€ì‚¬
                if prev_end_time is not None and curr_start_time is not None:
                    # 4. ì´ì „ ë°œí™”ê°€ ê³ ê°, í˜„ì¬ ë°œí™”ê°€ ìƒë‹´ì‚¬ì¸ ê²½ìš°
                    if (any(keyword in prev_speaker for keyword in ['ê³ ê°', 'customer', 'client', 'user']) and
                        any(keyword in curr_speaker for keyword in ['ìƒë‹´ì‚¬', 'counselor', 'agent', 'csr', 'staff'])):
                        
                        # 5. ìƒë‹´ì‚¬ ë°œí™” ì‹œì‘ ì‹œê°„ < ê³ ê° ë°œí™” ì¢…ë£Œ ì‹œê°„ â†’ ê°€ë¡œì±„ê¸°
                        if curr_start_time < prev_end_time:
                            interruption_count += 1
                            print(f"ğŸ” ê°€ë¡œì±„ê¸° ê°ì§€: ìƒë‹´ì‚¬ê°€ {prev_end_time - curr_start_time:.2f}ì´ˆ ì¼ì° ë°œí™” ì‹œì‘")
            
            return interruption_count
            
        except Exception as e:
            print(f"âš ï¸ ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def export_results_to_dataframe(self, result: CommunicationQualityResult) -> pd.DataFrame:
        """ë¶„ì„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        data = {
            'honorific_ratio': [result.honorific_ratio],
            'positive_word_ratio': [result.positive_word_ratio],
            'negative_word_ratio': [result.negative_word_ratio],
            'euphonious_word_ratio': [result.euphonious_word_ratio],
            'empathy_ratio': [result.empathy_ratio],
            'apology_ratio': [result.apology_ratio],
            'total_sentences': [result.total_sentences],
            # ìƒˆë¡œìš´ ì •ëŸ‰ ë¶„ì„ ì§€í‘œ 5ì¢…
            'customer_sentiment_early': [result.customer_sentiment_early],
            'customer_sentiment_late': [result.customer_sentiment_late],
            'customer_sentiment_trend': [result.customer_sentiment_trend],
            'avg_response_latency': [result.avg_response_latency],
            'task_ratio': [result.task_ratio],
            # ìƒˆë¡œìš´ LLM ê¸°ë°˜ ì •ì„± í‰ê°€ ì§€í‘œ 2ì¢…
            'suggestions': [result.suggestions],
            'interruption_count': [result.interruption_count]
        }
        
        return pd.DataFrame(data)
    
    def print_analysis_report(self, result: CommunicationQualityResult):
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š ìƒë‹´ì‚¬ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í’ˆì§ˆ ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        print(f"ğŸ“ ì´ ìƒë‹´ì‚¬ ë°œí™” ë¬¸ì¥ ìˆ˜: {result.total_sentences}ê°œ")
        print()
        
        print("ğŸ“ˆ í’ˆì§ˆ ì§€í‘œ (%):")
        print(f"  1. ì¡´ëŒ“ë§ ì‚¬ìš© ë¹„ìœ¨:     {result.honorific_ratio:6.2f}%")
        print(f"  2. ê¸ì • ë‹¨ì–´ ë¹„ìœ¨:       {result.positive_word_ratio:6.2f}%")
        print(f"  3. ë¶€ì • ë‹¨ì–´ ë¹„ìœ¨:       {result.negative_word_ratio:6.2f}%")
        print(f"  4. ì¿ ì…˜ì–´/ì™„ê³¡ í‘œí˜„:     {result.euphonious_word_ratio:6.2f}%")
        print(f"  5. ê³µê° í‘œí˜„ ë¹„ìœ¨:       {result.empathy_ratio:6.2f}%")
        print(f"  6. ì‚¬ê³¼ í‘œí˜„ ë¹„ìœ¨:       {result.apology_ratio:6.2f}%")
        
        print("\nğŸ“Š ì •ëŸ‰ ë¶„ì„ ì§€í‘œ:")
        if result.customer_sentiment_early is not None:
            print(f"  1. ê³ ê° ê°ì • ì´ˆë°˜ë¶€:     {result.customer_sentiment_early:6.3f}")
        else:
            print(f"  1. ê³ ê° ê°ì • ì´ˆë°˜ë¶€:     {'N/A':>6}")
            
        if result.customer_sentiment_late is not None:
            print(f"  2. ê³ ê° ê°ì • í›„ë°˜ë¶€:     {result.customer_sentiment_late:6.3f}")
        else:
            print(f"  2. ê³ ê° ê°ì • í›„ë°˜ë¶€:     {'N/A':>6}")
            
        if result.customer_sentiment_trend is not None:
            print(f"  3. ê³ ê° ê°ì • ë³€í™” ì¶”ì„¸:  {result.customer_sentiment_trend:6.3f}")
            trend_desc = "ê°œì„ ë¨" if result.customer_sentiment_trend > 0 else "ì•…í™”ë¨" if result.customer_sentiment_trend < 0 else "ë³€í™”ì—†ìŒ"
            print(f"     â†’ {trend_desc}")
        else:
            print(f"  3. ê³ ê° ê°ì • ë³€í™” ì¶”ì„¸:  {'N/A':>6}")
            
        if result.avg_response_latency is not None:
            print(f"  4. í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„:  {result.avg_response_latency:6.3f}ì´ˆ")
        else:
            print(f"  4. í‰ê·  ì‘ë‹µ ì§€ì—° ì‹œê°„:  {'N/A':>6}")
            
        if result.task_ratio is not None:
            print(f"  5. ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨:       {result.task_ratio:6.3f}")
            ratio_desc = "ê³ ê° ë°œí™” > ìƒë‹´ì‚¬ ë°œí™”" if result.task_ratio > 1 else "ìƒë‹´ì‚¬ ë°œí™” > ê³ ê° ë°œí™”" if result.task_ratio < 1 else "ê· í˜•ì "
            print(f"     â†’ {ratio_desc}")
        else:
            print(f"  5. ì—…ë¬´ ì²˜ë¦¬ ë¹„ìœ¨:       {'N/A':>6}")
        
        print("\nğŸ¤– LLM ê¸°ë°˜ ì •ì„± í‰ê°€ ì§€í‘œ:")
        if result.suggestions is not None:
            print(f"  6. ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜:  {result.suggestions:6.1f}")
            if result.suggestions == 1.0:
                print(f"     â†’ ìµœì´ˆ ì œì•ˆìœ¼ë¡œ ë¬¸ì œ í•´ê²°")
            elif result.suggestions == 0.6:
                print(f"     â†’ ë‘ ë²ˆì§¸ ì œì•ˆìœ¼ë¡œ ë¬¸ì œ í•´ê²°")
            elif result.suggestions == 0.2:
                print(f"     â†’ ì„¸ ë²ˆ ì´ìƒ ì œì•ˆìœ¼ë¡œ ë¬¸ì œ í•´ê²°")
            else:
                print(f"     â†’ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨")
        else:
            print(f"  6. ë¬¸ì œ í•´ê²° ì œì•ˆ ì ìˆ˜:  {'N/A':>6}")
            
        if result.interruption_count is not None:
            print(f"  7. ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜:   {result.interruption_count:6d}íšŒ")
            if result.interruption_count == 0:
                print(f"     â†’ ê°€ë¡œì±„ê¸° ì—†ìŒ (ì–‘í˜¸)")
            elif result.interruption_count <= 2:
                print(f"     â†’ ê°€ë¡œì±„ê¸° ì ìŒ (ë³´í†µ)")
            else:
                print(f"     â†’ ê°€ë¡œì±„ê¸° ë§ìŒ (ê°œì„  í•„ìš”)")
        else:
            print(f"  7. ëŒ€í™” ê°€ë¡œì±„ê¸° íšŸìˆ˜:   {'N/A':>6}")
        
        print("\nğŸ’¡ í’ˆì§ˆ í‰ê°€:")
        self._print_quality_assessment(result)
        
        print("\nğŸ“‹ ìƒ˜í”Œ ë¬¸ì¥ë“¤:")
        details = result.analysis_details.get('sample_sentences', {})
        for category, samples in details.items():
            if samples:
                print(f"\n  {category.upper()}:")
                for i, sample in enumerate(samples[:2], 1):  # ìµœëŒ€ 2ê°œë§Œ ì¶œë ¥
                    print(f"    {i}. {sample}")
    
    def _print_quality_assessment(self, result: CommunicationQualityResult):
        """í’ˆì§ˆ í‰ê°€ ì½”ë©˜íŠ¸ ì¶œë ¥"""
        assessments = []
        
        if result.honorific_ratio >= 80:
            assessments.append("âœ… ì¡´ëŒ“ë§ ì‚¬ìš©ì´ ìš°ìˆ˜í•¨")
        elif result.honorific_ratio >= 60:
            assessments.append("âš ï¸ ì¡´ëŒ“ë§ ì‚¬ìš© ê°œì„  í•„ìš”")
        else:
            assessments.append("âŒ ì¡´ëŒ“ë§ ì‚¬ìš©ì´ ë¶€ì¡±í•¨")
        
        if result.positive_word_ratio >= 30:
            assessments.append("âœ… ê¸ì •ì  ì–¸ì–´ ì‚¬ìš©ì´ ìš°ìˆ˜í•¨")
        elif result.positive_word_ratio >= 15:
            assessments.append("âš ï¸ ê¸ì •ì  ì–¸ì–´ ì‚¬ìš© ê°œì„  í•„ìš”")
        else:
            assessments.append("âŒ ê¸ì •ì  ì–¸ì–´ ì‚¬ìš©ì´ ë¶€ì¡±í•¨")
        
        if result.negative_word_ratio <= 10:
            assessments.append("âœ… ë¶€ì •ì  ì–¸ì–´ ì‚¬ìš©ì´ ì ì ˆí•¨")
        elif result.negative_word_ratio <= 20:
            assessments.append("âš ï¸ ë¶€ì •ì  ì–¸ì–´ ì‚¬ìš© ì£¼ì˜ í•„ìš”")
        else:
            assessments.append("âŒ ë¶€ì •ì  ì–¸ì–´ ì‚¬ìš©ì´ ê³¼ë‹¤í•¨")
        
        if result.euphonious_word_ratio >= 20:
            assessments.append("âœ… ì¿ ì…˜ì–´ ì‚¬ìš©ì´ ìš°ìˆ˜í•¨")
        elif result.euphonious_word_ratio >= 10:
            assessments.append("âš ï¸ ì¿ ì…˜ì–´ ì‚¬ìš© ê°œì„  í•„ìš”")
        else:
            assessments.append("âŒ ì¿ ì…˜ì–´ ì‚¬ìš©ì´ ë¶€ì¡±í•¨")
        
        for assessment in assessments:
            print(f"    {assessment}")


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_communication_quality_analyzer():
    """ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    
    # ìƒ˜í”Œ ë°ì´í„° (ì‹œê°„ ì •ë³´ì™€ ê°ì • ì •ë³´ í¬í•¨)
    sample_utterances = [
        {"speaker": "ê³ ê°", "text": "ì•ˆë…•í•˜ì„¸ìš”. ìš”ê¸ˆ ê´€ë ¨í•´ì„œ ë¬¸ì˜ë“œë¦¬ê³  ì‹¶ìŠµë‹ˆë‹¤.", 
         "start_time": 0.0, "end_time": 3.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ì•ˆë…•í•˜ì„¸ìš”. ê³ ê°ë‹˜. ìš”ê¸ˆ ê´€ë ¨ ë¬¸ì˜ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ë¶€ë¶„ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?", 
         "start_time": 4.0, "end_time": 8.0, "sentiment": "positive"},
        {"speaker": "ê³ ê°", "text": "ì´ë²ˆ ë‹¬ ìš”ê¸ˆì´ ë„ˆë¬´ ë§ì´ ë‚˜ì™”ì–´ìš”. ì™œ ì´ë ‡ê²Œ ë§ì´ ë‚˜ì˜¨ ê±´ì§€ ëª¨ë¥´ê² ì–´ìš”.", 
         "start_time": 9.0, "end_time": 13.0, "sentiment": "negative"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ê±±ì •ë˜ì…¨ê² ì–´ìš”. ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ê¸ˆ ë‚´ì—­ì„ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í˜¹ì‹œ í•´ì™¸ ë¡œë°ì„ ì‚¬ìš©í•˜ì…¨ë‚˜ìš”?", 
         "start_time": 14.5, "end_time": 19.0, "sentiment": "positive"},
        {"speaker": "ê³ ê°", "text": "ì•„ë‹ˆìš”. í•´ì™¸ì— ì•ˆ ê°”ëŠ”ë°ìš”.", 
         "start_time": 20.0, "end_time": 22.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ê·¸ë ‡ë‹¤ë©´ ë°ì´í„° ì‚¬ìš©ëŸ‰ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì‹¤ë¡€ì§€ë§Œ íœ´ëŒ€í° ë²ˆí˜¸ë¥¼ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?", 
         "start_time": 23.0, "end_time": 27.0, "sentiment": "positive"},
        {"speaker": "ê³ ê°", "text": "010-1234-5678ì…ë‹ˆë‹¤.", 
         "start_time": 28.0, "end_time": 30.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ê°ì‚¬í•©ë‹ˆë‹¤. í™•ì¸í•´ë³´ë‹ˆ ì´ë²ˆ ë‹¬ì— ë°ì´í„°ë¥¼ ë§ì´ ì‚¬ìš©í•˜ì…¨ë„¤ìš”. ë™ì˜ìƒ ì‹œì²­ì´ë‚˜ ê²Œì„ì„ í•˜ì…¨ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.", 
         "start_time": 31.5, "end_time": 36.0, "sentiment": "positive"},
        {"speaker": "ê³ ê°", "text": "ì•„ ê·¸ëŸ°ê°€ìš”? ê·¸ëŸ¼ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", 
         "start_time": 37.0, "end_time": 39.0, "sentiment": "neutral"},
        {"speaker": "ìƒë‹´ì‚¬", "text": "ë‹¤ìŒ ë‹¬ë¶€í„°ëŠ” ë°ì´í„° ì‚¬ìš©ëŸ‰ì„ ì²´í¬í•´ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”?", 
         "start_time": 40.0, "end_time": 44.0, "sentiment": "positive"}
    ]
    
    # ë¶„ì„ ìˆ˜í–‰
    analyzer = CommunicationQualityAnalyzer()
    result = analyzer.analyze_communication_quality(sample_utterances)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_analysis_report(result)
    
    # DataFrame ë³€í™˜
    df = analyzer.export_results_to_dataframe(result)
    print("\nğŸ“Š DataFrame ê²°ê³¼:")
    print(df.to_string(index=False))
    
    return result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_communication_quality_analyzer() 